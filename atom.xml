<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>小江的博客</title>
  
  <subtitle>想成为一个温暖而有趣的人</subtitle>
  <link href="https://rozen12123.github.io/atom.xml" rel="self"/>
  
  <link href="https://rozen12123.github.io/"/>
  <updated>2025-03-19T08:23:42.672Z</updated>
  <id>https://rozen12123.github.io/</id>
  
  <author>
    <name>小江</name>
    
  </author>
  
  <generator uri="https://hexo.io/">Hexo</generator>
  
  <entry>
    <title></title>
    <link href="https://rozen12123.github.io/posts/0.html"/>
    <id>https://rozen12123.github.io/posts/0.html</id>
    <published>2025-03-19T08:23:42.671Z</published>
    <updated>2025-03-19T08:23:42.672Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Hugging-Face预训练GPT微调ChatGPT（微调入门！新手友好！）"><a href="#Hugging-Face预训练GPT微调ChatGPT（微调入门！新手友好！）" class="headerlink" title="Hugging Face预训练GPT微调ChatGPT（微调入门！新手友好！）"></a>Hugging Face预训练GPT微调ChatGPT（微调入门！新手友好！）</h1><p>在实战中，⼤多数情况下都不需要从0开始训练模型，⽽是使⽤“⼤⼚”或者其他研究者开源的已经训练好的⼤模型。</p><p>在各种⼤模型开源库中，最具代表性的就是<code>Hugging Face</code>。<code>Hugging Face</code>是⼀家专注于NLP领域的AI公司，开发了⼀个名为<code>Transformers</code>的开源库，该开源库拥有许多预训练后的深度学习模型，如BERT、GPT-2、T5等。<code>Hugging Face</code>的<code>Transformers</code>开源库使研究⼈员和开发⼈员能够更轻松地使⽤这些模型进⾏各种NLP任务，例如⽂本分类、问答、⽂本⽣成等。这个库也提供了简洁、⾼效的API，有助于快速实现⾃然语⾔处理应⽤。</p><p>从Hugging Face下载⼀个GPT-2并微调成ChatGPT，需要遵循的步骤如下。</p><p><img src="https://rozen-picture.oss-cn-beijing.aliyuncs.com/img/202503191444425.png" alt="image-20250319144401279" style="zoom:67%;"></p><h2 id="1-安装Hugging-Face-Transformers库"><a href="#1-安装Hugging-Face-Transformers库" class="headerlink" title="1.安装Hugging Face Transformers库"></a>1.安装Hugging Face Transformers库</h2><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pip install transformers</span><br></pre></td></tr></table></figure><h2 id="2-载入预训练GPT-2模型和分词器"><a href="#2-载入预训练GPT-2模型和分词器" class="headerlink" title="2.载入预训练GPT-2模型和分词器"></a>2.载入预训练GPT-2模型和分词器</h2><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch <span class="comment"># 导⼊torch</span></span><br><span class="line"><span class="keyword">from</span> transformers <span class="keyword">import</span> GPT2Tokenizer <span class="comment"># 导⼊GPT-2分词器</span></span><br><span class="line"><span class="keyword">from</span> transformers <span class="keyword">import</span> GPT2LMHeadModel <span class="comment"># 导⼊GPT-2语⾔模型</span></span><br><span class="line">model_name = <span class="string">&quot;gpt2&quot;</span> <span class="comment"># 也可以选择其他模型，如&quot;gpt2-medium&quot; &quot;gpt2-large&quot;等</span></span><br><span class="line">tokenizer = GPT2Tokenizer.from_pretrained(model_name) <span class="comment"># 加载分词器</span></span><br><span class="line">tokenizer.pad_token = <span class="string">&#x27;&#x27;</span> <span class="comment"># 为分词器添加pad token</span></span><br><span class="line">tokenizer.pad_token_id = tokenizer.convert_tokens_to_ids(<span class="string">&#x27;&#x27;</span>)</span><br><span class="line">device = <span class="string">&quot;cuda&quot;</span> <span class="keyword">if</span> torch.cuda.is_available() <span class="keyword">else</span> <span class="string">&quot;cpu&quot;</span> <span class="comment"># 判断是否有可⽤的GPU</span></span><br><span class="line">model = GPT2LMHeadModel.from_pretrained(model_name).to(device) <span class="comment"># 将模型加载到设备上（CPU或GPU）</span></span><br><span class="line">vocab = tokenizer.get_vocab() <span class="comment"># 获取词汇表</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;模型信息：&quot;</span>, model)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;分词器信息：&quot;</span>,tokenizer)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;词汇表⼤⼩：&quot;</span>, <span class="built_in">len</span>(vocab))</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;部分词汇示例：&quot;</span>, (<span class="built_in">list</span>(vocab.keys())[<span class="number">8000</span>:<span class="number">8005</span>]))</span><br></pre></td></tr></table></figure><h2 id="3-准备微调数据集"><a href="#3-准备微调数据集" class="headerlink" title="3.准备微调数据集"></a>3.准备微调数据集</h2><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> torch.utils.data <span class="keyword">import</span> Dataset  <span class="comment"># 导入PyTorch的Dataset</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 自定义ChatDataset类，继承自PyTorch的Dataset类</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">ChatDataset</span>(<span class="title class_ inherited__">Dataset</span>):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, file_path, tokenizer, vocab</span>):</span><br><span class="line">        self.tokenizer = tokenizer  <span class="comment"># 分词器</span></span><br><span class="line">        self.vocab = vocab  <span class="comment"># 词汇表</span></span><br><span class="line">        <span class="comment"># 加载数据并处理，将处理后的输入数据和目标数据赋值给input_data和target_data</span></span><br><span class="line">        self.input_data, self.target_data = self.load_and_process_data(file_path)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 定义加载和处理数据的方法</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">load_and_process_data</span>(<span class="params">self, file_path</span>):</span><br><span class="line">        <span class="keyword">with</span> <span class="built_in">open</span>(file_path, <span class="string">&quot;r&quot;</span>) <span class="keyword">as</span> f:  <span class="comment"># 读取文件内容</span></span><br><span class="line">            lines = f.readlines()</span><br><span class="line">        input_data, target_data = [], []</span><br><span class="line">        <span class="keyword">for</span> i, line <span class="keyword">in</span> <span class="built_in">enumerate</span>(lines):  <span class="comment"># 遍历文件的每一行</span></span><br><span class="line">            <span class="keyword">if</span> line.startswith(<span class="string">&quot;User:&quot;</span>):  <span class="comment"># 如以&quot;User:&quot;开头，移除&quot;User: &quot;前缀，并将张量转换为列表</span></span><br><span class="line">                tokens = self.tokenizer(line.strip()[<span class="number">6</span>:], return_tensors=<span class="string">&quot;pt&quot;</span>)[<span class="string">&quot;input_ids&quot;</span>].tolist()[<span class="number">0</span>]</span><br><span class="line">                tokens = tokens + [self.tokenizer.eos_token_id]  <span class="comment"># 添加结束符</span></span><br><span class="line">                input_data.append(torch.tensor(tokens, dtype=torch.long))  <span class="comment"># 添加到input_data</span></span><br><span class="line">            <span class="keyword">elif</span> line.startswith(<span class="string">&quot;AI:&quot;</span>):  <span class="comment"># 如以&quot;AI:&quot;开头，移除&quot;AI: &quot;前缀，并将张量转换为列表</span></span><br><span class="line">                tokens = self.tokenizer(line.strip()[<span class="number">4</span>:], return_tensors=<span class="string">&quot;pt&quot;</span>)[<span class="string">&quot;input_ids&quot;</span>].tolist()[<span class="number">0</span>]</span><br><span class="line">                tokens = tokens + [self.tokenizer.eos_token_id]  <span class="comment"># 添加结束符</span></span><br><span class="line">                target_data.append(torch.tensor(tokens, dtype=torch.long))  <span class="comment"># 添加到target_data</span></span><br><span class="line">        <span class="keyword">return</span> input_data, target_data</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 定义数据集的长度，即input_data的长度</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__len__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="keyword">return</span> <span class="built_in">len</span>(self.input_data)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 定义获取数据集中指定索引的数据的方法</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__getitem__</span>(<span class="params">self, idx</span>):</span><br><span class="line">        <span class="keyword">return</span> self.input_data[idx], self.target_data[idx]</span><br><span class="line"></span><br><span class="line">file_path = <span class="string">&quot;/kaggle/input/hugging-face-chatgpt-chat-data/chat.txt&quot;</span>  <span class="comment"># 加载chat.txt数据集</span></span><br><span class="line">chat_dataset = ChatDataset(file_path, tokenizer, vocab)  <span class="comment"># 创建ChatDataset对象，传入文件、分词器和词汇表</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 打印数据集中前2个数据示例</span></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">2</span>):</span><br><span class="line">    input_example, target_example = chat_dataset[i]</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&quot;示例 <span class="subst">&#123;i + <span class="number">1</span>&#125;</span>:&quot;</span>)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;输入：&quot;</span>, tokenizer.decode(input_example))</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;输出：&quot;</span>, tokenizer.decode(target_example))</span><br><span class="line"></span><br></pre></td></tr></table></figure><h2 id="4-准备微调数据加载器"><a href="#4-准备微调数据加载器" class="headerlink" title="4.准备微调数据加载器"></a>4.准备微调数据加载器</h2><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> torch.utils.data <span class="keyword">import</span> DataLoader  <span class="comment"># 导入DataLoader</span></span><br><span class="line"></span><br><span class="line">tokenizer.pad_token = <span class="string">&#x27;&#x27;</span>  <span class="comment"># 为分词器添加pad token</span></span><br><span class="line">tokenizer.pad_token_id = tokenizer.convert_tokens_to_ids(<span class="string">&#x27;&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义pad_sequence函数，用于将一批序列补齐到相同长度</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">pad_sequence</span>(<span class="params">sequences, padding_value=<span class="number">0</span>, length=<span class="literal">None</span></span>):</span><br><span class="line">    <span class="comment"># 计算最大序列长度，如果length参数未提供，则使用输入序列中的最大长度</span></span><br><span class="line">    max_length = <span class="built_in">max</span>(<span class="built_in">len</span>(seq) <span class="keyword">for</span> seq <span class="keyword">in</span> sequences) <span class="keyword">if</span> length <span class="keyword">is</span> <span class="literal">None</span> <span class="keyword">else</span> length</span><br><span class="line">    <span class="comment"># 创建一个具有适当形状的全零张量，用于存储补齐后的序列</span></span><br><span class="line">    result = torch.full((<span class="built_in">len</span>(sequences), max_length), padding_value, dtype=torch.long)</span><br><span class="line">    <span class="comment"># 遍历序列，将每个序列的内容复制到张量result中</span></span><br><span class="line">    <span class="keyword">for</span> i, seq <span class="keyword">in</span> <span class="built_in">enumerate</span>(sequences):</span><br><span class="line">        end = <span class="built_in">len</span>(seq)</span><br><span class="line">        result[i, :end] = seq[:end]</span><br><span class="line">    <span class="keyword">return</span> result</span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义collate_fn函数，用于将一个批次的数据整理成适当的形状</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">collate_fn</span>(<span class="params">batch</span>):</span><br><span class="line">    <span class="comment"># 从批次中分离源序列和目标序列</span></span><br><span class="line">    sources, targets = <span class="built_in">zip</span>(*batch)</span><br><span class="line">    <span class="comment"># 计算批次中的最大序列长度</span></span><br><span class="line">    max_length = <span class="built_in">max</span>(<span class="built_in">max</span>(<span class="built_in">len</span>(s) <span class="keyword">for</span> s <span class="keyword">in</span> sources), <span class="built_in">max</span>(<span class="built_in">len</span>(t) <span class="keyword">for</span> t <span class="keyword">in</span> targets))</span><br><span class="line">    <span class="comment"># 使用pad_sequence函数补齐源序列和目标序列</span></span><br><span class="line">    sources = pad_sequence(sources, padding_value=tokenizer.pad_token_id, length=max_length)</span><br><span class="line">    targets = pad_sequence(targets, padding_value=tokenizer.pad_token_id, length=max_length)</span><br><span class="line">    <span class="comment"># 返回补齐后的源序列和目标序列</span></span><br><span class="line">    <span class="keyword">return</span> sources, targets</span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建DataLoader</span></span><br><span class="line">chat_dataloader = DataLoader(chat_dataset, batch_size=<span class="number">2</span>, shuffle=<span class="literal">True</span>, collate_fn=collate_fn)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 检查Dataloader输出</span></span><br><span class="line"><span class="keyword">for</span> input_batch, target_batch <span class="keyword">in</span> chat_dataloader:</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;Input batch tensor size:&quot;</span>, input_batch.size())</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;Target batch tensor size:&quot;</span>, target_batch.size())</span><br><span class="line">    <span class="keyword">break</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> input_batch, target_batch <span class="keyword">in</span> chat_dataloader:</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;Input batch tensor:&quot;</span>)</span><br><span class="line">    <span class="built_in">print</span>(input_batch)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;Target batch tensor:&quot;</span>)</span><br><span class="line">    <span class="built_in">print</span>(target_batch)</span><br><span class="line">    <span class="keyword">break</span></span><br><span class="line"></span><br></pre></td></tr></table></figure><h2 id="5-对GPT-2进行微调"><a href="#5-对GPT-2进行微调" class="headerlink" title="5.对GPT-2进行微调"></a>5.对GPT-2进行微调</h2><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> torch.optim <span class="keyword">as</span> optim</span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义损失函数，忽略pad_token_id对应的损失值</span></span><br><span class="line">criterion = nn.CrossEntropyLoss(ignore_index=tokenizer.pad_token_id)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义优化器</span></span><br><span class="line">optimizer = optim.Adam(model.parameters(), lr=<span class="number">0.0001</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 进行500个epoch的训练</span></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">500</span>):</span><br><span class="line">    <span class="keyword">for</span> batch_idx, (input_batch, target_batch) <span class="keyword">in</span> <span class="built_in">enumerate</span>(chat_dataloader):  <span class="comment"># 遍历数据加载器中的批次</span></span><br><span class="line">        optimizer.zero_grad()  <span class="comment"># 梯度清零</span></span><br><span class="line">        input_batch, target_batch = input_batch.to(device), target_batch.to(device)  <span class="comment"># 输入和目标批次移至设备</span></span><br><span class="line">        </span><br><span class="line">        outputs = model(input_batch)  <span class="comment"># 前向传播</span></span><br><span class="line">        logits = outputs.logits  <span class="comment"># 获取logits</span></span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 计算损失</span></span><br><span class="line">        loss = criterion(logits.view(-<span class="number">1</span>, <span class="built_in">len</span>(vocab)), target_batch.view(-<span class="number">1</span>))</span><br><span class="line">        </span><br><span class="line">        loss.backward()  <span class="comment"># 反向传播</span></span><br><span class="line">        optimizer.step()  <span class="comment"># 更新参数</span></span><br><span class="line">        </span><br><span class="line">    <span class="keyword">if</span> (epoch + <span class="number">1</span>) % <span class="number">100</span> == <span class="number">0</span>:  <span class="comment"># 每100个epoch打印一次损失值</span></span><br><span class="line">        <span class="built_in">print</span>(<span class="string">f&#x27;Epoch: <span class="subst">&#123;epoch + <span class="number">1</span>:04d&#125;</span>, cost = <span class="subst">&#123;loss:<span class="number">.6</span>f&#125;</span>&#x27;</span>)</span><br><span class="line"></span><br></pre></td></tr></table></figure><h2 id="6-用约束解码函数生成回答"><a href="#6-用约束解码函数生成回答" class="headerlink" title="6.用约束解码函数生成回答"></a>6.用约束解码函数生成回答</h2><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 定义集束解码函数</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">generate_text_beam_search</span>(<span class="params">model, input_str, max_len=<span class="number">50</span>, beam_width=<span class="number">5</span></span>):</span><br><span class="line">    model.<span class="built_in">eval</span>()  <span class="comment"># 将模型设置为评估模式（不计算梯度）</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 对输入字符串进行编码，并将其转换为张量，然后将其移动到相应的设备上</span></span><br><span class="line">    input_tokens = tokenizer.encode(input_str, return_tensors=<span class="string">&quot;pt&quot;</span>).to(device)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 初始化候选序列列表，包含当前输入序列和其对数概率得分（我们从0开始）</span></span><br><span class="line">    candidates = [(input_tokens, <span class="number">0.0</span>)]</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 禁用梯度计算，以加速预测过程</span></span><br><span class="line">    <span class="keyword">with</span> torch.no_grad():</span><br><span class="line">        <span class="comment"># 迭代生成最大长度的序列</span></span><br><span class="line">        <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(max_len):</span><br><span class="line">            new_candidates = []</span><br><span class="line">            </span><br><span class="line">            <span class="comment"># 对于每个候选序列</span></span><br><span class="line">            <span class="keyword">for</span> candidate, candidate_score <span class="keyword">in</span> candidates:</span><br><span class="line">                <span class="comment"># 使用模型进行预测</span></span><br><span class="line">                outputs = model(candidate)</span><br><span class="line">                <span class="comment"># 获取输出logits</span></span><br><span class="line">                logits = outputs.logits[:, -<span class="number">1</span>, :]</span><br><span class="line">                <span class="comment"># 获取对数概率得分的top-k值（即beam_width）及其对应的token</span></span><br><span class="line">                scores, next_tokens = torch.topk(logits, beam_width, dim=-<span class="number">1</span>)</span><br><span class="line">                </span><br><span class="line">                final_results = []</span><br><span class="line">                <span class="comment"># 遍历top-k token及其对应的得分</span></span><br><span class="line">                <span class="keyword">for</span> score, next_token <span class="keyword">in</span> <span class="built_in">zip</span>(scores.squeeze(), next_tokens.squeeze()):</span><br><span class="line">                    <span class="comment"># 在当前候选序列中添加新的token</span></span><br><span class="line">                    new_candidate = torch.cat((candidate, next_token.unsqueeze(<span class="number">0</span>).unsqueeze(<span class="number">0</span>)), dim=-<span class="number">1</span>)</span><br><span class="line">                    <span class="comment"># 更新候选序列的得分</span></span><br><span class="line">                    new_score = candidate_score - score.item()</span><br><span class="line">                    <span class="comment"># 如果新的token是结束符（eos_token），则将该候选序列添加到最终结果中</span></span><br><span class="line">                    <span class="keyword">if</span> next_token.item() == tokenizer.eos_token_id:</span><br><span class="line">                        final_results.append((new_candidate, new_score))</span><br><span class="line">                    <span class="comment"># 否则，将新的候选序列添加到新候选序列列表中</span></span><br><span class="line">                    <span class="keyword">else</span>:</span><br><span class="line">                        new_candidates.append((new_candidate, new_score))</span><br><span class="line">                </span><br><span class="line">            <span class="comment"># 从新候选序列列表中选择得分最⾼的top-k个序列</span></span><br><span class="line">            candidates = <span class="built_in">sorted</span>(new_candidates, key=<span class="keyword">lambda</span> x: x[<span class="number">1</span>])[:beam_width]</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 选择得分最⾼的候选序列</span></span><br><span class="line">        best_candidate, _ = <span class="built_in">sorted</span>(candidates, key=<span class="keyword">lambda</span> x: x[<span class="number">1</span>])[<span class="number">0</span>]</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 将输出token转换回文本字符串</span></span><br><span class="line">        output_str = tokenizer.decode(best_candidate[<span class="number">0</span>])</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 移除输入字符串并修复空格问题</span></span><br><span class="line">        input_len = <span class="built_in">len</span>(tokenizer.encode(input_str))</span><br><span class="line">        output_str = tokenizer.decode(best_candidate.squeeze()[input_len:])</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> output_str</span><br><span class="line"></span><br><span class="line"><span class="comment"># 测试模型</span></span><br><span class="line">test_inputs = [</span><br><span class="line">    <span class="string">&quot;what is the weather like today?&quot;</span>,</span><br><span class="line">    <span class="string">&quot;can you recommend a good book?&quot;</span></span><br><span class="line">]</span><br><span class="line"></span><br><span class="line"><span class="comment"># 输出测试结果</span></span><br><span class="line"><span class="keyword">for</span> i, input_str <span class="keyword">in</span> <span class="built_in">enumerate</span>(test_inputs, start=<span class="number">1</span>):</span><br><span class="line">    generated_text = generate_text_beam_search(model, input_str)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&quot;测试 <span class="subst">&#123;i&#125;</span>:&quot;</span>)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&quot;User: <span class="subst">&#123;input_str&#125;</span>&quot;</span>)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&quot;AI: <span class="subst">&#123;generated_text&#125;</span>&quot;</span>)</span><br><span class="line"></span><br></pre></td></tr></table></figure><figure class="highlight markdown"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">测试1:</span><br><span class="line">User: what is the weather like today?&lt;|endoftext|&gt;</span><br><span class="line">AI: you need an current time for now app with app app app app</span><br><span class="line">测试2:</span><br><span class="line">User: Can you recommend a good book?&lt;|endoftext|&gt;</span><br><span class="line">AI: ockingbird Lee Harper Harper Taylor</span><br></pre></td></tr></table></figure><p>模型的回答虽然称不上完美，但是，我们⾄少能够看出，微调数据集中的信息起到了⼀定的作⽤。第⼀个问题问及天⽓，模型敏锐地指向“app”（应⽤）这个存在于训练语料库中的信息，⽽查看“应⽤”确实是我们希望模型给出的答案。回答第⼆个问题时，模型给出了语料库中所推荐图书的作者的名字“Lee Harper”，⽽书名“To kill a Mockingbird”中的mockingbird是⼀个未知token，模型把它拆解成了三个token。具体信息如下。</p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">tokenizer.encode(<span class="string">&#x27;Mockingbird&#x27;</span>)：[<span class="number">44</span>/<span class="number">76</span>, <span class="number">8629</span>, <span class="number">16944</span>]</span><br><span class="line">tokenizer.decode(<span class="number">44</span>)：<span class="string">&#x27;M&#x27;</span></span><br><span class="line">tokenizer.decode(<span class="number">8629</span>)：<span class="string">&#x27;ocking&#x27;</span></span><br><span class="line">tokenizer.decode(<span class="number">16944</span>)：<span class="string">&#x27;bird&#x27;</span></span><br></pre></td></tr></table></figure><p>因此，在解码时，出现了ockingbird这样的不完整信息，但是其中也的确包含了⼀定的语料库内部的知识。</p><p>⽽微调则针对特定任务进⾏优化。这⼀模式的优势在于，微调过程通常需要较少的训练数据和计算资源，同时仍能获得良好的性能。</p>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;Hugging-Face预训练GPT微调ChatGPT（微调入门！新手友好！）&quot;&gt;&lt;a href=&quot;#Hugging-Face预训练GPT微调ChatGPT（微调入门！新手友好！）&quot; class=&quot;headerlink&quot; title=&quot;Hugging Face预训</summary>
      
    
    
    
    
  </entry>
  
  <entry>
    <title></title>
    <link href="https://rozen12123.github.io/posts/0.html"/>
    <id>https://rozen12123.github.io/posts/0.html</id>
    <published>2025-03-17T13:26:52.110Z</published>
    <updated>2025-03-19T06:30:21.038Z</updated>
    
    <content type="html"><![CDATA[<h1 id="从零到一：如何训练简版生成式GPT模型，快速实现创意写作"><a href="#从零到一：如何训练简版生成式GPT模型，快速实现创意写作" class="headerlink" title="从零到一：如何训练简版生成式GPT模型，快速实现创意写作"></a>从零到一：如何训练简版生成式GPT模型，快速实现创意写作</h1><h2 id="一、从零到一：你的第一个GPT诞生记"><a href="#一、从零到一：你的第一个GPT诞生记" class="headerlink" title="一、从零到一：你的第一个GPT诞生记"></a>一、从零到一：你的第一个GPT诞生记</h2><p><strong>震撼对比</strong>：使用相同训练数据（全唐诗30万首）  </p><ul><li>传统RNN生成：”春风吹又生，花落知多少”（模板化）  </li><li>简版GPT生成：”墨染江南烟雨楼，一蓑孤舟任水流。青山不语斜阳暮，白鹭惊飞入画轴”（创意性）  </li></ul><p>&nbsp;</p><h2 id="二、自回归机制"><a href="#二、自回归机制" class="headerlink" title="二、自回归机制"></a>二、自回归机制</h2><p>⾃回归模型（Autoregressive Model）是⽣成式模型的⼀种特例，它预测的新⽬标值是基于前⾯若⼲个已⽣成值的。⾃回归模型在时间序列分析、语⾳信号处理、⾃然语⾔处理等领域有⼴泛应⽤。在序列⽣成问题中，⾃回归模型特别重要，⽐如在机器翻译、⽂本⽣成、语⾳合成等任务中，Transformer的解码器、GPT等模型就是基于⾃回归原理的。</p><p><img src="https://rozen-picture.oss-cn-beijing.aliyuncs.com/img/202503172107844.png" alt="image-20250317210639269" style="zoom:50%;"></p><p>⽤⾃回归机制来逐词⽣成翻译结果。</p><p>还是使⽤同样的中英翻译数据集，还是使⽤Transformer模型，这⾥我们只是加⼀个⽤贪婪搜索进⾏⽣成式解码的函数，然后在测试过程中调⽤这个函数重新测试。</p><p>之前的数据见：<a href="https://blog.csdn.net/u011146203/article/details/146324014?spm=1001.2014.3001.5502">Transformer：GPT背后的造脑工程全解析（含手搓过程）-CSDN博客</a></p><h3 id="2-1定义贪婪解码器函数"><a href="#2-1定义贪婪解码器函数" class="headerlink" title="2.1定义贪婪解码器函数"></a>2.1定义贪婪解码器函数</h3><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 定义贪婪解码器函数</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">greedy_decoder</span>(<span class="params">model, enc_input, start_symbol</span>):</span><br><span class="line">    <span class="comment"># 对输入数据进行编码，并获得编码器输出以及自注意力权重</span></span><br><span class="line">    enc_outputs, enc_self_attns = model.encoder(enc_input)    </span><br><span class="line">    <span class="comment"># 初始化解码器输入为全零张量，大小为 (1, 5)，数据类型与 enc_input 一致</span></span><br><span class="line">    dec_input = torch.zeros(<span class="number">1</span>, <span class="number">5</span>).type_as(enc_input.data)    </span><br><span class="line">    <span class="comment"># 设置下一个要解码的符号为开始符号</span></span><br><span class="line">    next_symbol = start_symbol    </span><br><span class="line">    <span class="comment"># 循环 5 次，为解码器输入中的每一个位置填充一个符号</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">0</span>, <span class="number">5</span>):</span><br><span class="line">        <span class="comment"># 将下一个符号放入解码器输入的当前位置</span></span><br><span class="line">        dec_input[<span class="number">0</span>][i] = next_symbol        </span><br><span class="line">        <span class="comment"># 运行解码器，获得解码器输出、解码器自注意力权重和编码器 - 解码器注意力权重</span></span><br><span class="line">        dec_output, _, _ = model.decoder(dec_input, enc_input, enc_outputs)        </span><br><span class="line">        <span class="comment"># 将解码器输出投影到目标词汇空间</span></span><br><span class="line">        projected = model.projection(dec_output)        </span><br><span class="line">        <span class="comment"># 找到具有最高概率的下一个单词</span></span><br><span class="line">        prob = projected.squeeze(<span class="number">0</span>).<span class="built_in">max</span>(dim=-<span class="number">1</span>, keepdim=<span class="literal">False</span>)[<span class="number">1</span>]</span><br><span class="line">        next_word = prob.data[i]        </span><br><span class="line">        <span class="comment"># 将找到的下一个单词作为新的符号</span></span><br><span class="line">        next_symbol = next_word.item()        </span><br><span class="line">    <span class="comment"># 返回解码器输入，它包含了生成的符号序列</span></span><br><span class="line">    dec_outputs = dec_input</span><br><span class="line">    <span class="keyword">return</span> dec_outputs</span><br></pre></td></tr></table></figure><p><code>greedy_decoder</code> 实现了贪婪解码算法的过程，其中：</p><ul><li>每次解码时，模型选择概率最大的单词作为下一个符号。</li><li>这个过程在目标序列的每个位置进行，直到生成一个完整的目标序列（此处假设目标序列长度为 5）。</li></ul><p><strong>贪婪解码</strong>是一种简单的解码策略，它总是选择当前时刻最可能的单词，而不考虑全局最优解。虽然这种方法快速，但可能会陷入局部最优解，导致生成的序列不尽如人意。在更复杂的情况下，可能需要使用如 <strong>束搜索</strong>（Beam Search）等策略来改进生成的质量。</p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 用贪婪解码器生成翻译文本</span></span><br><span class="line">enc_inputs, dec_inputs, target_batch = corpus.make_batch(batch_size=<span class="number">1</span>, test_batch=<span class="literal">True</span>) </span><br><span class="line"><span class="comment"># 使用贪婪解码器生成解码器输入</span></span><br><span class="line">greedy_dec_input = greedy_decoder(model, enc_inputs, start_symbol=corpus.tgt_vocab[<span class="string">&#x27;&lt;sos&gt;&#x27;</span>])</span><br><span class="line"><span class="comment"># 将解码器输入转换为单词序列</span></span><br><span class="line">greedy_dec_output_words = [corpus.tgt_idx2word[n.item()] <span class="keyword">for</span> n <span class="keyword">in</span> greedy_dec_input.squeeze()]</span><br><span class="line"><span class="comment"># 打印编码器输入和贪婪解码器生成的文本</span></span><br><span class="line">enc_inputs_words = [corpus.src_idx2word[code.item()] <span class="keyword">for</span> code <span class="keyword">in</span> enc_inputs[<span class="number">0</span>]]</span><br><span class="line"><span class="built_in">print</span>(enc_inputs_words, <span class="string">&#x27;-&gt;&#x27;</span>, greedy_dec_output_words)</span><br></pre></td></tr></table></figure><figure class="highlight markdown"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[&#x27;我&#x27;, &#x27;爱&#x27;, &#x27;学习&#x27;, &#x27;人工智能&#x27;, &#x27;<span class="language-xml"><span class="tag">&lt;<span class="name">pad</span>&gt;</span></span>&#x27;] -&gt; [&#x27;<span class="language-xml"><span class="tag">&lt;<span class="name">sos</span>&gt;</span></span>&#x27;, &#x27;I&#x27;, &#x27;love&#x27;, &#x27;studying&#x27;, &#x27;AI&#x27;]</span><br></pre></td></tr></table></figure><p>&nbsp;</p><h2 id="三、构建GPT模型"><a href="#三、构建GPT模型" class="headerlink" title="三、构建GPT模型"></a>三、<strong>构建</strong>GPT模型</h2><h3 id="3-1-搭建GPT模型（解码器）"><a href="#3-1-搭建GPT模型（解码器）" class="headerlink" title="3.1 搭建GPT模型（解码器）"></a>3.1 搭建GPT模型（解码器）</h3><p>GPT只使⽤了Transformer的解码器部分，其关键组件如下图所示。</p><p><img src="https://rozen-picture.oss-cn-beijing.aliyuncs.com/img/202503172121029.png" alt="image-20250317212154916" style="zoom:67%;"></p><p>搭建GPT模型的代码的关键组件如下。</p><p>组件1 多头⾃注意⼒：通过<code>ScaledDotProductAttention</code>类实现缩放点积注意⼒机制，然后通过<code>MultiHeadAttention</code>类实现多头⾃注意⼒机制。</p><p>组件2 逐位置前馈⽹络：通过<code>PoswiseFeedForwardNet</code>类实现逐位置前馈⽹络。</p><p>组件3 正弦位置编码表：通过<code>get_sin_code_table</code>函数⽣成正弦位置编码表。</p><p>组件4 填充掩码：通过<code>get_attn_pad_mask</code>函数为填充<code>token</code>⽣成注意⼒掩码，避免注意⼒机制关注⽆⽤的信息。</p><p>组件5 后续掩码：通过<code>get_attn_subsequent_mask</code>函数为后续<code>token</code>（当前位置后⾯的信息）⽣成注意⼒掩码，避免解码器中的注意⼒机制“偷窥”未来的⽬标数据。</p><p>组件6 解码器层：通过<code>DecoderLayer</code>类定义解码器的单层。</p><p>组件7 解码器：通过<code>Decoder</code>类定义<code>Transformer</code>模型的完整解码器部分。</p><p>组件8 GPT：在解码器的基础上添加⼀个投影层，将解码器输出的特征向量转换为预测结果，实现⽂本⽣成。</p><p>之前在手搓transfomer中已经讲过前五个组件，这次从第六个组件开始讲解。</p><p>&nbsp;</p><h3 id="3-2-解码器层类"><a href="#3-2-解码器层类" class="headerlink" title="3.2 解码器层类"></a>3.2 解码器层类</h3><p>因为GPT模型没有编码器组件，也不需要来⾃编码器的输出，因此GPT解码器的实现更简洁。GPT模型也省略了编码器-解码器注意⼒机制，因此模型的训练速度更快。</p><p><img src="https://rozen-picture.oss-cn-beijing.aliyuncs.com/img/202503172125290.png" alt="image-20250317212502224"></p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 定义解码器层类</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">DecoderLayer</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="built_in">super</span>(DecoderLayer, self).__init__()</span><br><span class="line">        self.self_attn = MultiHeadAttention()  <span class="comment"># 多头自注意力层</span></span><br><span class="line">        self.feed_forward = PoswiseFeedForwardNet()  <span class="comment"># 逐位置前馈网络层</span></span><br><span class="line">        self.norm1 = nn.LayerNorm(d_embedding)  <span class="comment"># 第一个层归一化</span></span><br><span class="line">        self.norm2 = nn.LayerNorm(d_embedding)  <span class="comment"># 第二个层归一化</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, dec_inputs, attn_mask=<span class="literal">None</span></span>):</span><br><span class="line">        <span class="comment"># 使用多头自注意力处理输入</span></span><br><span class="line">        attn_output, _ = self.self_attn(dec_inputs, dec_inputs, dec_inputs, attn_mask)</span><br><span class="line">        <span class="comment"># 将注意力输出与输入相加并进行第一个层归一化</span></span><br><span class="line">        norm1_outputs = self.norm1(dec_inputs + attn_output)</span><br><span class="line">        <span class="comment"># 将归一化后的输出输入到位置前馈神经网络</span></span><br><span class="line">        ff_outputs = self.feed_forward(norm1_outputs)</span><br><span class="line">        <span class="comment"># 将前馈神经网络输出与第一次归一化后的输出相加并进行第二个层归一化</span></span><br><span class="line">        dec_outputs = self.norm2(norm1_outputs + ff_outputs)</span><br><span class="line">        <span class="keyword">return</span> dec_outputs <span class="comment"># 返回解码器层输出</span></span><br></pre></td></tr></table></figure><p>GPT解码器层的构造⽐<code>Transformer</code>的解码器层简单，仅包含⼀个多头⾃注意⼒层<code>MultiHeadAttention</code>和⼀个逐位置前馈⽹络层<code>PosFeedForwardNet</code>，后⾯接了两个层归⼀化<code>nn.LayerNorm</code>。</p><p>主要是两个归一化层</p><p>解码器层中，两个层归⼀化的作⽤如下。</p><p>■ 第⼀个层归⼀化norm1：在多头⾃注意⼒<code>self_attn</code>处理后，将注意⼒输出<code>attn_output</code>与原始输⼊<code>dec_inputs</code>相加。这种加和操作实现了残差连接，可以加速梯度反向传播，有助于训练深层⽹络。将相加后的结果进⾏层归⼀化。层归⼀化对输⼊进⾏标准化处理，使其具有相同的均值和⽅差。这有助于减少梯度消失或梯度爆炸问题，从⽽提⾼模型训练的稳定性。</p><p>■ 第⼆个层归⼀化norm2：在逐位置前馈⽹络<code>feed_forward</code>处理后，将前馈神经⽹络输出<code>ff_outputs</code>与第⼀个层归⼀化输出<code>norm1_outputs</code>相加。这⾥同样实现了残差连接。将相加后的结果进⾏层归⼀化。这⼀步骤的⽬的与第⼀个层归⼀化相同，即标准化输⼊数据，以提⾼训练稳定性。</p><p>&nbsp;</p><h3 id="3-3-解码器类"><a href="#3-3-解码器类" class="headerlink" title="3.3 解码器类"></a>3.3 解码器类</h3><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#  定义解码器类</span></span><br><span class="line">n_layers = <span class="number">6</span>  <span class="comment"># 设置 Decoder 的层数</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Decoder</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, vocab_size, max_seq_len</span>):</span><br><span class="line">        <span class="built_in">super</span>(Decoder, self).__init__()</span><br><span class="line">        <span class="comment"># 词嵌入层（参数为词典维度）</span></span><br><span class="line">        self.src_emb = nn.Embedding(vocab_size, d_embedding)  </span><br><span class="line">        <span class="comment"># 位置编码层（参数为序列长度）</span></span><br><span class="line">        self.pos_emb = nn.Embedding(max_seq_len, d_embedding)</span><br><span class="line">        <span class="comment"># 初始化 N 个解码器层       </span></span><br><span class="line">        self.layers = nn.ModuleList([DecoderLayer() <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(n_layers)]) </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, dec_inputs</span>):        </span><br><span class="line">        <span class="comment"># 创建位置信息</span></span><br><span class="line">        positions = torch.arange(<span class="built_in">len</span>(dec_inputs), device=dec_inputs.device).unsqueeze(-<span class="number">1</span>)</span><br><span class="line">        <span class="comment"># 将词嵌入与位置编码相加</span></span><br><span class="line">        inputs_embedding = self.src_emb(dec_inputs) + self.pos_emb(positions)</span><br><span class="line">        <span class="comment"># 生成自注意力掩码</span></span><br><span class="line">        attn_mask = get_attn_subsequent_mask(inputs_embedding).to(device)</span><br><span class="line">        <span class="comment"># 初始化解码器输入，这是第一层解码器层的输入 </span></span><br><span class="line">        dec_outputs =  inputs_embedding </span><br><span class="line">        <span class="keyword">for</span> layer <span class="keyword">in</span> self.layers:</span><br><span class="line">            <span class="comment"># 将输入数据传递给解码器层，并返回解码器层的输出，作为下一层的输入</span></span><br><span class="line">            dec_outputs = layer(dec_outputs, attn_mask) </span><br><span class="line">        <span class="keyword">return</span> dec_outputs <span class="comment"># 返回解码器输出</span></span><br></pre></td></tr></table></figure><p>GPT解码器的结构⽐Transformer解码器的结构简单，因为GPT是⼀个单向⽣成式模型，只关注⽣成⽂本⽽不关注源⽂本。GPT不需要实现编码器-解码器注意⼒的部分，仅接收解码器的输⼊，然后进⾏词嵌⼊和位置编码，并将⼆者相加，继⽽⽣成后续⾃注意⼒掩码，来保证每个位置只能看到当前位置之前的信息，以保持⽣成⽂本的⾃回归特性。最后把嵌⼊向量和掩码信息传递给解码器层，并⾏处理，并接收结果向量==dec_outputs==，然后把它返回给GPT模型。</p><p>一句话说的话就是，解码器只管生成文本。</p><p>&nbsp;</p><h3 id="3-4-GPT组件"><a href="#3-4-GPT组件" class="headerlink" title="3.4 GPT组件"></a>3.4 GPT组件</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"># 定义 GPT 模型</span><br><span class="line">class GPT(nn.Module):</span><br><span class="line">    def __init__(self, vocab_size, max_seq_len):</span><br><span class="line">        super(GPT, self).__init__()</span><br><span class="line">        self.decoder = Decoder(vocab_size, max_seq_len) # 解码器，用于学习文本生成能力</span><br><span class="line">        self.projection = nn.Linear(d_embedding, vocab_size)  # 全连接层，输出预测结果</span><br><span class="line">    def forward(self, dec_inputs):        </span><br><span class="line">        dec_outputs = self.decoder(dec_inputs) # 将输入数据传递给解码器</span><br><span class="line">        logits = self.projection(dec_outputs) # 传递给全连接层以生成预测</span><br><span class="line">        return logits # 返回预测结py</span><br><span class="line">        果</span><br></pre></td></tr></table></figure><p>在这个简化版的GPT模型中：解码器类负责学习⽂本⽣成能⼒；⼀个全连接层将解码器输出的特征向量映射到⼀个概率分布，表示⽣成每个单词的概率logits，⽤于将解码器的输出转换为与词汇表⼤⼩相匹配的预测结果。</p><p>GPT模型仅包含解码器部分，没有编码器部分。因此，它更适⽤于==⽆条件⽂本⽣成==任务，⽽不是类似机器翻译或问答等需要编码器-解码器结构的任务。</p><p>&nbsp;</p><h2 id="四、完成文本生成任务"><a href="#四、完成文本生成任务" class="headerlink" title="四、完成文本生成任务"></a>四、完成文本生成任务</h2><h3 id="4-1-构建文本生成任务的数据集"><a href="#4-1-构建文本生成任务的数据集" class="headerlink" title="4.1 构建文本生成任务的数据集"></a>4.1 构建文本生成任务的数据集</h3><p>我们要给GPT准备⼀个训练语料库。这个语料库是由现实中存在的⽂字组成的。当然，⽐起维基百科等⼤型语料库，我们的语料库中的数据⽐较少，你可以把它看成⼈类语料库的⼀个缩影。</p><p><img src="https://rozen-picture.oss-cn-beijing.aliyuncs.com/img/202503181410753.png" alt="image-20250318141004609" style="zoom:67%;"></p><p>把这个语料库存储在⽂件lang.txt中，等待程序读取。</p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 构建语料库</span></span><br><span class="line"><span class="keyword">from</span> collections <span class="keyword">import</span> Counter</span><br><span class="line"><span class="keyword">class</span> <span class="title class_">LanguageCorpus</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, sentences</span>):</span><br><span class="line">        self.sentences = sentences</span><br><span class="line">        <span class="comment"># 计算语言的最大句子长度，并加 2 以容纳特殊符号 &lt;sos&gt; 和 &lt;eos&gt;</span></span><br><span class="line">        self.seq_len = <span class="built_in">max</span>([<span class="built_in">len</span>(sentence.split()) <span class="keyword">for</span> sentence <span class="keyword">in</span> sentences]) + <span class="number">2</span></span><br><span class="line">        self.vocab = self.create_vocabulary() <span class="comment"># 创建源语言和目标语言的词汇表</span></span><br><span class="line">        self.idx2word = &#123;v: k <span class="keyword">for</span> k, v <span class="keyword">in</span> self.vocab.items()&#125; <span class="comment"># 创建索引到单词的映射</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">create_vocabulary</span>(<span class="params">self</span>):</span><br><span class="line">        vocab = &#123;<span class="string">&#x27;&lt;pad&gt;&#x27;</span>: <span class="number">0</span>, <span class="string">&#x27;&lt;sos&gt;&#x27;</span>: <span class="number">1</span>, <span class="string">&#x27;&lt;eos&gt;&#x27;</span>: <span class="number">2</span>&#125;</span><br><span class="line">        counter = Counter()</span><br><span class="line">        <span class="comment"># 统计语料库的单词频率</span></span><br><span class="line">        <span class="keyword">for</span> sentence <span class="keyword">in</span> self.sentences:</span><br><span class="line">            words = sentence.split()</span><br><span class="line">            counter.update(words)</span><br><span class="line">        <span class="comment"># 创建词汇表，并为每个单词分配一个唯一的索引</span></span><br><span class="line">        <span class="keyword">for</span> word <span class="keyword">in</span> counter:</span><br><span class="line">            <span class="keyword">if</span> word <span class="keyword">not</span> <span class="keyword">in</span> vocab:</span><br><span class="line">                vocab[word] = <span class="built_in">len</span>(vocab)</span><br><span class="line">        <span class="keyword">return</span> vocab</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">make_batch</span>(<span class="params">self, batch_size, test_batch=<span class="literal">False</span></span>):</span><br><span class="line">        input_batch, output_batch = [], [] <span class="comment"># 初始化批数据</span></span><br><span class="line">        sentence_indices = torch.randperm(<span class="built_in">len</span>(self.sentences))[:batch_size] <span class="comment"># 随机选择句子索引</span></span><br><span class="line">        <span class="keyword">for</span> index <span class="keyword">in</span> sentence_indices:</span><br><span class="line">            sentence = self.sentences[index]</span><br><span class="line">            <span class="comment"># 将句子转换为索引序列</span></span><br><span class="line">            seq = [self.vocab[<span class="string">&#x27;&lt;sos&gt;&#x27;</span>]] + [self.vocab[word] <span class="keyword">for</span> word <span class="keyword">in</span> sentence.split()] + [self.vocab[<span class="string">&#x27;&lt;eos&gt;&#x27;</span>]]</span><br><span class="line">            seq += [self.vocab[<span class="string">&#x27;&lt;pad&gt;&#x27;</span>]] * (self.seq_len - <span class="built_in">len</span>(seq)) <span class="comment"># 对序列进行填充</span></span><br><span class="line">            <span class="comment"># 将处理好的序列添加到批次中</span></span><br><span class="line">            input_batch.append(seq[:-<span class="number">1</span>])</span><br><span class="line">            output_batch.append(seq[<span class="number">1</span>:])</span><br><span class="line">        <span class="keyword">return</span> torch.LongTensor(input_batch), torch.LongTensor(output_batch)</span><br></pre></td></tr></table></figure><p>这个类的主要功能是创建词汇表、将句⼦转换为索引序列、⽣成批次数据等，其中最重要的是<code>make_batch</code>⽅法中⽣成批次数据时的“向右位移”操作，这是训练⽣成式语⾔模型的关键所在。</p><p>查看一些语料库的数据：</p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">with</span> <span class="built_in">open</span>(<span class="string">&quot;lang.txt&quot;</span>, <span class="string">&quot;r&quot;</span>) <span class="keyword">as</span> file: <span class="comment"># 从文件中读入语料</span></span><br><span class="line">    sentences = [line.strip() <span class="keyword">for</span> line <span class="keyword">in</span> file.readlines()]</span><br><span class="line">corpus = LanguageCorpus(sentences) <span class="comment"># 创建语料库</span></span><br><span class="line">vocab_size = <span class="built_in">len</span>(corpus.vocab) <span class="comment"># 词汇表大小</span></span><br><span class="line">max_seq_len = corpus.seq_len <span class="comment"># 最大句子长度（用于设置位置编码）</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot; 语料库词汇表大小 : <span class="subst">&#123;vocab_size&#125;</span>&quot;</span>) <span class="comment"># 打印词汇表大小</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot; 最长句子长度 : <span class="subst">&#123;max_seq_len&#125;</span>&quot;</span>) <span class="comment"># 打印最大序列长</span></span><br></pre></td></tr></table></figure><figure class="highlight markdown"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">语料库词汇表大小 : 133</span><br><span class="line"></span><br><span class="line">最长句子长度 : 17</span><br></pre></td></tr></table></figure><p>&nbsp;</p><h3 id="4-2-训练过程中的自回归"><a href="#4-2-训练过程中的自回归" class="headerlink" title="4.2 训练过程中的自回归"></a>4.2 训练过程中的自回归</h3><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch.optim <span class="keyword">as</span> optim <span class="comment"># 导入优化器</span></span><br><span class="line">device = <span class="string">&quot;cuda&quot;</span> <span class="keyword">if</span> torch.cuda.is_available() <span class="keyword">else</span> <span class="string">&quot;cpu&quot;</span> <span class="comment"># 设置设备</span></span><br><span class="line">model = GPT(vocab_size, max_seq_len).to(device) <span class="comment"># 创建 GPT 模型实例</span></span><br><span class="line">criterion = nn.CrossEntropyLoss() <span class="comment"># 损失函数</span></span><br><span class="line">optimizer = optim.Adam(model.parameters(), lr=<span class="number">0.0001</span>) <span class="comment"># 优化器</span></span><br><span class="line">epochs = <span class="number">500</span> <span class="comment"># 训练轮次</span></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(epochs):  <span class="comment"># 训练 epochs 轮</span></span><br><span class="line">    optimizer.zero_grad() <span class="comment"># 梯度清零</span></span><br><span class="line">    inputs, targets = corpus.make_batch(batch_size) <span class="comment"># 创建训练数据</span></span><br><span class="line">    inputs, targets = inputs.to(device), targets.to(device)</span><br><span class="line">    outputs = model(inputs) <span class="comment"># 获取模型输出 </span></span><br><span class="line">    loss = criterion(outputs.view(-<span class="number">1</span>, vocab_size), targets.view(-<span class="number">1</span>)) <span class="comment"># 计算损失</span></span><br><span class="line">    <span class="keyword">if</span> (epoch + <span class="number">1</span>) % <span class="number">100</span> == <span class="number">0</span>: <span class="comment"># 打印损失</span></span><br><span class="line">        <span class="built_in">print</span>(<span class="string">f&quot;Epoch: <span class="subst">&#123;epoch + <span class="number">1</span>:04d&#125;</span> cost = <span class="subst">&#123;loss:<span class="number">.6</span>f&#125;</span>&quot;</span>)</span><br><span class="line">    loss.backward() <span class="comment"># 反向传播</span></span><br><span class="line">    optimizer.step() <span class="comment"># 更新参数</span></span><br></pre></td></tr></table></figure><p>&nbsp;</p><p>4.3 文本生成的自回归(贪婪搜索)</p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 测试文本生成</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">generate_text</span>(<span class="params">model, input_str, max_len=<span class="number">50</span></span>):</span><br><span class="line">    model.<span class="built_in">eval</span>()  <span class="comment"># 将模型设置为评估（测试）模式，关闭 dropout 和 batch normalization 等训练相关的层</span></span><br><span class="line">    <span class="comment"># 将输入字符串中的每个 token 转换为其在词汇表中的索引</span></span><br><span class="line">    input_tokens = [corpus.vocab[token] <span class="keyword">for</span> token <span class="keyword">in</span> input_str]</span><br><span class="line">    <span class="comment"># 创建一个新列表，将输入的 tokens 复制到输出 tokens 中 , 目前只有输入的词</span></span><br><span class="line">    output_tokens = input_tokens.copy()</span><br><span class="line">    <span class="keyword">with</span> torch.no_grad():  <span class="comment"># 禁用梯度计算，以节省内存并加速测试过程</span></span><br><span class="line">        <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(max_len):  <span class="comment"># 生成最多 max_len 个 tokens</span></span><br><span class="line">            <span class="comment"># 将输出的 token 转换为 PyTorch 张量，并增加一个代表批次的维度 [1, len(output_tokens)]</span></span><br><span class="line">            inputs = torch.LongTensor(output_tokens).unsqueeze(<span class="number">0</span>).to(device)</span><br><span class="line">            outputs = model(inputs) <span class="comment"># 输出 logits 形状为 [1, len(output_tokens), vocab_size]</span></span><br><span class="line">            <span class="comment"># 在最后一个维度上获取 logits 中的最大值，并返回其索引（即下一个 token）</span></span><br><span class="line">            _, next_token = torch.<span class="built_in">max</span>(outputs[:, -<span class="number">1</span>, :], dim=-<span class="number">1</span>)            </span><br><span class="line">            next_token = next_token.item() <span class="comment"># 将张量转换为 Python 整数            </span></span><br><span class="line">            <span class="keyword">if</span> next_token == corpus.vocab[<span class="string">&quot;&lt;eos&gt;&quot;</span>]:</span><br><span class="line">                <span class="keyword">break</span> <span class="comment"># 如果生成的 token 是 EOS（结束符），则停止生成过程           </span></span><br><span class="line">            output_tokens.append(next_token) <span class="comment"># 将生成的 tokens 添加到 output_tokens 列表</span></span><br><span class="line">    <span class="comment"># 将输出 tokens 转换回文本字符串</span></span><br><span class="line">    output_str = <span class="string">&quot; &quot;</span>.join([corpus.idx2word[token] <span class="keyword">for</span> token <span class="keyword">in</span> output_tokens])</span><br><span class="line">    <span class="keyword">return</span> output_str</span><br><span class="line">input_str = [<span class="string">&quot;Python&quot;</span>] <span class="comment"># 输入一个词：Python</span></span><br><span class="line">generated_text = generate_text(model, input_str) <span class="comment"># 模型跟着这个词生成后续文本</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot; 生成的文本 :&quot;</span>, generated_text) <span class="comment"># 打印预测文本</span></span><br></pre></td></tr></table></figure><figure class="highlight markdown"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">生成的文本 : Python is a popular programming language.</span><br></pre></td></tr></table></figure><p>&nbsp;</p><h2 id="五、使用WikiText2数据集训练Wiki-GPT模型"><a href="#五、使用WikiText2数据集训练Wiki-GPT模型" class="headerlink" title="五、使用WikiText2数据集训练Wiki-GPT模型"></a>五、使用WikiText2数据集训练Wiki-GPT模型</h2><p>使⽤⼀个从互联⽹中收集真实语料的数据集WikiText 。</p><p>WikiText数据集是从维基百科上经过验证的精选⽂章集中提取的超过1亿个标记的数据集合。让我们⽤这个更真实的语料数据集，来看⼀看现实世界的模型是如何训练出来的。</p><p>■ ⽬前NLP领域中常⽤数据集的导⼊（通过Torchtext库）、设计（创建PyTorch Dataset）、加载（使⽤PyTorch Data Loader），以及如何将数据集中的数据转换为</p><p>我们的GPT模型可以读取的格式（通过Torchtext的分词⼯具Tokenizer）。</p><p>■ 如何对模型的效能进⾏测试。</p><p>之前的数据集都很⼩，没有拆分成训练数据集和测试数据集的必要，⽽现在，⽤这个真实的、包含上万条语料的数据集，就可以⽤其中的⼀部分数据来测试模型的效能。这样，在多轮训练的过程中，我们就可以选择测试集上得分最⾼的模型。</p><p>&nbsp;</p><h3 id="5-1-用WikiText2构建Dataset和DataLoader"><a href="#5-1-用WikiText2构建Dataset和DataLoader" class="headerlink" title="5.1 用WikiText2构建Dataset和DataLoader"></a>5.1 用WikiText2构建Dataset和DataLoader</h3><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> torchtext.datasets <span class="keyword">import</span> WikiText2 <span class="comment"># 导入WikiText2</span></span><br><span class="line"><span class="keyword">from</span> torchtext.data.utils <span class="keyword">import</span> get_tokenizer <span class="comment"># 导入Tokenizer分词工具</span></span><br><span class="line"><span class="keyword">from</span> torchtext.vocab <span class="keyword">import</span> build_vocab_from_iterator <span class="comment"># 导入Vocabulary工具</span></span><br><span class="line"><span class="keyword">from</span> torch.utils.data <span class="keyword">import</span> DataLoader, Dataset <span class="comment"># 导入Pytorch的DataLoader和Dataset</span></span><br><span class="line"></span><br><span class="line">tokenizer = get_tokenizer(<span class="string">&quot;basic_english&quot;</span>) <span class="comment"># 定义数据预处理所需的tokenizer</span></span><br><span class="line"></span><br><span class="line">train_iter = WikiText2(split=<span class="string">&#x27;train&#x27;</span>) <span class="comment"># 加载WikiText2数据集的训练部分</span></span><br><span class="line">valid_iter = WikiText2(split=<span class="string">&#x27;valid&#x27;</span>) <span class="comment"># 加载WikiText2数据集的验证部分</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义一个生成器函数，用于将数据集中的文本转换为tokens</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">yield_tokens</span>(<span class="params">data_iter</span>):</span><br><span class="line">    <span class="keyword">for</span> item <span class="keyword">in</span> data_iter:</span><br><span class="line">        <span class="keyword">yield</span> tokenizer(item)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建词汇表，包括特殊tokens：&quot;&lt;pad&gt;&quot;, &quot;&lt;sos&gt;&quot;, &quot;&lt;eos&gt;&quot;</span></span><br><span class="line">vocab = build_vocab_from_iterator(yield_tokens(train_iter), </span><br><span class="line">                                  specials=[<span class="string">&quot;&lt;pad&gt;&quot;</span>, <span class="string">&quot;&lt;sos&gt;&quot;</span>, <span class="string">&quot;&lt;eos&gt;&quot;</span>])</span><br><span class="line">vocab.set_default_index(vocab[<span class="string">&quot;&lt;pad&gt;&quot;</span>])</span><br><span class="line"></span><br><span class="line"><span class="comment"># 打印词汇表信息</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;词汇表大小:&quot;</span>, <span class="built_in">len</span>(vocab))</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;词汇示例(word to index):&quot;</span>, </span><br><span class="line">      &#123;word: vocab[word] <span class="keyword">for</span> word <span class="keyword">in</span> [<span class="string">&quot;&lt;pad&gt;&quot;</span>, <span class="string">&quot;&lt;sos&gt;&quot;</span>, <span class="string">&quot;&lt;eos&gt;&quot;</span>, <span class="string">&quot;the&quot;</span>, <span class="string">&quot;apple&quot;</span>]&#125;)</span><br></pre></td></tr></table></figure><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> torch.utils.data <span class="keyword">import</span> Dataset <span class="comment"># 导入Dataset</span></span><br><span class="line">max_seq_len = <span class="number">256</span> <span class="comment"># 设置序列的最大长度</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义一个处理WikiText2数据集的自定义数据集类</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">WikiDataset</span>(<span class="title class_ inherited__">Dataset</span>):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, data_iter, vocab, max_len=max_seq_len</span>):</span><br><span class="line">        self.data = []        </span><br><span class="line">        <span class="keyword">for</span> sentence <span class="keyword">in</span> data_iter: <span class="comment"># 遍历数据集，将文本转换为tokens</span></span><br><span class="line">            <span class="comment"># 对每个句子进行tokenization，并截取长度为max_len-2，为&lt;sos&gt;和&lt;eos&gt;留出空间</span></span><br><span class="line">            tokens = tokenizer(sentence)[:max_len - <span class="number">2</span>]</span><br><span class="line">            tokens = [vocab[<span class="string">&quot;&lt;sos&gt;&quot;</span>]] + vocab(tokens) + [vocab[<span class="string">&quot;&lt;eos&gt;&quot;</span>]] <span class="comment"># 添加&lt;sos&gt;和&lt;eos&gt;            </span></span><br><span class="line">            self.data.append(tokens) <span class="comment"># 将处理好的tokens添加到数据集中</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__len__</span>(<span class="params">self</span>): <span class="comment"># 定义数据集的长度</span></span><br><span class="line">        <span class="keyword">return</span> <span class="built_in">len</span>(self.data)    </span><br><span class="line">    </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__getitem__</span>(<span class="params">self, idx</span>): <span class="comment"># 定义数据集的索引方法 (即抽取数据条目)        </span></span><br><span class="line">        source = self.data[idx][:-<span class="number">1</span>] <span class="comment"># 获取当前数据，并将&lt;eos&gt;移除，作为source        </span></span><br><span class="line">        target = self.data[idx][<span class="number">1</span>:] <span class="comment"># 获取当前数据，并将&lt;sos&gt;移除，作为target（右移1位）       </span></span><br><span class="line">        <span class="keyword">return</span> torch.tensor(source), torch.tensor(target) <span class="comment"># 转换为tensor并返回</span></span><br><span class="line"></span><br><span class="line">train_dataset = WikiDataset(train_iter, vocab) <span class="comment"># 创建训练数据集</span></span><br><span class="line">valid_dataset = WikiDataset(valid_iter, vocab) <span class="comment"># 创建验证数据集</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;Dataset数据条目: <span class="subst">&#123;<span class="built_in">len</span>(train_dataset)&#125;</span>&quot;</span>)</span><br><span class="line">sample_source, sample_target = train_dataset[<span class="number">100</span>]</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;输入序列张量样例: <span class="subst">&#123;sample_source&#125;</span>&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;目标序列张量样例: <span class="subst">&#123;sample_target&#125;</span>&quot;</span>)</span><br><span class="line">decoded_source = <span class="string">&#x27; &#x27;</span>.join(vocab.lookup_tokens(sample_source.tolist()))</span><br><span class="line">decoded_target = <span class="string">&#x27; &#x27;</span>.join(vocab.lookup_tokens(sample_target.tolist()))</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;输入序列样例文本: <span class="subst">&#123;decoded_source&#125;</span>&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;目标序列样例文本: <span class="subst">&#123;decoded_target&#125;</span>&quot;</span>)</span><br></pre></td></tr></table></figure><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> torch.utils.data <span class="keyword">import</span> DataLoader <span class="comment"># 导入Dataloader</span></span><br><span class="line"><span class="comment"># 定义pad_sequence函数，用于将一批序列补齐到相同长度</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">pad_sequence</span>(<span class="params">sequences, padding_value=<span class="number">0</span>, length=<span class="literal">None</span></span>):</span><br><span class="line">    <span class="comment"># 计算最大序列长度，如果length参数未提供，则使用输入序列中的最大长度</span></span><br><span class="line">    max_length = <span class="built_in">max</span>(<span class="built_in">len</span>(seq) <span class="keyword">for</span> seq <span class="keyword">in</span> sequences) <span class="keyword">if</span> length <span class="keyword">is</span> <span class="literal">None</span> <span class="keyword">else</span> length    </span><br><span class="line">    <span class="comment"># 创建一个具有适当形状的全零张量，用于存储补齐后的序列</span></span><br><span class="line">    result = torch.full((<span class="built_in">len</span>(sequences), max_length), padding_value, dtype=torch.long)    </span><br><span class="line">    <span class="comment"># 遍历序列，将每个序列的内容复制到结果张量中</span></span><br><span class="line">    <span class="keyword">for</span> i, seq <span class="keyword">in</span> <span class="built_in">enumerate</span>(sequences):</span><br><span class="line">        end = <span class="built_in">len</span>(seq)</span><br><span class="line">        result[i, :end] = seq[:end]</span><br><span class="line">    <span class="keyword">return</span> result</span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义collate_fn函数，用于将一个批次的数据整理成适当的形状</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">collate_fn</span>(<span class="params">batch</span>):</span><br><span class="line">    <span class="comment"># 从批次中分离源序列和目标序列</span></span><br><span class="line">    sources, targets = <span class="built_in">zip</span>(*batch)    </span><br><span class="line">    <span class="comment"># 计算批次中的最大序列长度</span></span><br><span class="line">    max_length = <span class="built_in">max</span>(<span class="built_in">max</span>(<span class="built_in">len</span>(s) <span class="keyword">for</span> s <span class="keyword">in</span> sources), <span class="built_in">max</span>(<span class="built_in">len</span>(t) <span class="keyword">for</span> t <span class="keyword">in</span> targets))    </span><br><span class="line">    <span class="comment"># 使用pad_sequence函数补齐源序列和目标序列</span></span><br><span class="line">    sources = pad_sequence(sources, padding_value=vocab[<span class="string">&quot;&lt;pad&gt;&quot;</span>], length=max_length)</span><br><span class="line">    targets = pad_sequence(targets, padding_value=vocab[<span class="string">&quot;&lt;pad&gt;&quot;</span>], length=max_length)    </span><br><span class="line">    <span class="comment"># 返回补齐后的源序列和目标序列</span></span><br><span class="line">    <span class="keyword">return</span> sources, targets</span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建一个训练数据加载器，使用自定义的collate_fn函数</span></span><br><span class="line">train_dataloader = DataLoader(train_dataset, batch_size=batch_size, </span><br><span class="line">                              shuffle=<span class="literal">True</span>, collate_fn=collate_fn)</span><br><span class="line"><span class="comment"># 创建一个验证数据加载器，使用自定义的collate_fn函数</span></span><br><span class="line">valid_dataloader = DataLoader(valid_dataset, batch_size=batch_size,</span><br><span class="line">                              shuffle=<span class="literal">False</span>, collate_fn=collate_fn)</span><br></pre></td></tr></table></figure><h3 id="5-2-微调Wik-GPT"><a href="#5-2-微调Wik-GPT" class="headerlink" title="5.2 微调Wik-GPT"></a>5.2 微调Wik-GPT</h3><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch.optim <span class="keyword">as</span> optim  <span class="comment"># 导入优化器</span></span><br><span class="line">device = <span class="string">&quot;cuda&quot;</span> <span class="keyword">if</span> torch.cuda.is_available() <span class="keyword">else</span> <span class="string">&quot;cpu&quot;</span>  <span class="comment"># 设置设备</span></span><br><span class="line">model = GPT(<span class="built_in">len</span>(vocab), max_seq_len).to(device)  <span class="comment"># 创建GPT模型实例</span></span><br><span class="line">criterion = nn.CrossEntropyLoss(ignore_index=vocab[<span class="string">&quot;&lt;pad&gt;&quot;</span>])</span><br><span class="line">optimizer = optim.Adam(model.parameters(), lr=<span class="number">0.0001</span>)  <span class="comment"># 优化器</span></span><br><span class="line">epochs = <span class="number">2</span>  <span class="comment"># 训练轮次</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(epochs):</span><br><span class="line">    epoch_loss = <span class="number">0</span></span><br><span class="line">    <span class="keyword">for</span> batch_idx, (source, target) <span class="keyword">in</span> <span class="built_in">enumerate</span>(train_dataloader): <span class="comment"># 用Dataloader加载数据</span></span><br><span class="line">        inputs, targets = source.to(device), target.to(device)</span><br><span class="line">        optimizer.zero_grad()  <span class="comment"># 梯度清零</span></span><br><span class="line">        outputs = model(inputs)  <span class="comment"># 获取模型输出</span></span><br><span class="line">        loss = criterion(outputs.view(-<span class="number">1</span>, <span class="built_in">len</span>(vocab)), targets.view(-<span class="number">1</span>))  <span class="comment"># 计算损失</span></span><br><span class="line">        loss.backward()  <span class="comment"># 反向传播</span></span><br><span class="line">        optimizer.step()  <span class="comment"># 更新参数</span></span><br><span class="line">        epoch_loss += loss.item()        </span><br><span class="line">        <span class="keyword">if</span> (batch_idx + <span class="number">1</span>) % <span class="number">500</span> == <span class="number">0</span>: <span class="comment"># 每500个批次打印一次损失</span></span><br><span class="line">            <span class="built_in">print</span>(<span class="string">f&quot;Batch <span class="subst">&#123;batch_idx + <span class="number">1</span>&#125;</span>/<span class="subst">&#123;<span class="built_in">len</span>(train_dataloader)&#125;</span>, Loss: <span class="subst">&#123;loss.item()&#125;</span>&quot;</span>)    </span><br><span class="line">    epoch_loss /= <span class="built_in">len</span>(train_dataloader) <span class="comment"># 每轮打印一次损失</span></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&quot;Epoch <span class="subst">&#123;epoch + <span class="number">1</span>&#125;</span>/<span class="subst">&#123;epochs&#125;</span>, Average Loss: <span class="subst">&#123;epoch_loss&#125;</span>&quot;</span>)</span><br></pre></td></tr></table></figure><h3 id="5-3-生成文本"><a href="#5-3-生成文本" class="headerlink" title="5.3 生成文本"></a>5.3 生成文本</h3><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Replace &#x27;model_timestamp.pt&#x27; with your saved model&#x27;s filename</span></span><br><span class="line">model.load_state_dict(torch.load(<span class="string">&#x27;trained_model_2023-05-05_14-08-24.pt&#x27;</span>))</span><br><span class="line"><span class="comment"># 测试文本生成</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">generate_text_greedy_search</span>(<span class="params">model, input_str, max_len=<span class="number">50</span></span>):</span><br><span class="line">    model.<span class="built_in">eval</span>()  <span class="comment"># 将模型设置为评估（测试）模式，关闭dropout和batch normalization等训练相关的层</span></span><br><span class="line">    <span class="comment"># 将输入字符串中的每个Token 转换为其在词汇表中的索引</span></span><br><span class="line">    input_tokens = [vocab[token] <span class="keyword">for</span> token <span class="keyword">in</span> input_str.split()]</span><br><span class="line">    <span class="comment"># 创建一个新列表，将输入的Token复制到输出Token中,目前只有输入的词</span></span><br><span class="line">    output_tokens = input_tokens.copy()</span><br><span class="line">    <span class="keyword">with</span> torch.no_grad():  <span class="comment"># 禁用梯度计算，以节省内存并加速测试过程</span></span><br><span class="line">        <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(max_len):  <span class="comment"># 生成最多max_len个Token</span></span><br><span class="line">            <span class="comment"># 将输出token转换为 PyTorch张量，并增加一个代表批次的维度[1, len(output_tokens)]</span></span><br><span class="line">            inputs = torch.LongTensor(output_tokens).unsqueeze(<span class="number">0</span>).to(device)</span><br><span class="line">            outputs = model(inputs) <span class="comment"># 输出 logits形状为[1, len(output_tokens), vocab_size]</span></span><br><span class="line">            logits = outputs[:, -<span class="number">1</span>, :] <span class="comment"># 只关心最后一个时间步（即最新生成的token）的logits</span></span><br><span class="line">            <span class="comment"># 在最后一个维度上获取logits中的最大值，并返回其索引（即下一个Token）</span></span><br><span class="line">            _, next_token = torch.<span class="built_in">max</span>(logits, dim=-<span class="number">1</span>)            </span><br><span class="line">            next_token = next_token.item() <span class="comment"># 将张量转换为Python整数            </span></span><br><span class="line">            <span class="keyword">if</span> next_token == vocab[<span class="string">&quot;&lt;eos&gt;&quot;</span>]:</span><br><span class="line">                <span class="keyword">break</span> <span class="comment"># 如果生成的Token是 EOS（结束符），则停止生成过程           </span></span><br><span class="line">            output_tokens.append(next_token) <span class="comment"># 将生成的Token添加到output_tokens列表</span></span><br><span class="line">    <span class="comment"># 将输出Token转换回文本字符串</span></span><br><span class="line">    output_str = <span class="string">&quot; &quot;</span>.join([vocab.get_itos()[token] <span class="keyword">for</span> token <span class="keyword">in</span> output_tokens</span><br><span class="line">                           <span class="keyword">if</span> vocab.get_itos()[token] != <span class="string">&quot;&lt;pad&gt;&quot;</span> <span class="keyword">and</span> vocab.get_itos()[token] != <span class="string">&quot;&lt;unk&gt;&quot;</span> ])</span><br><span class="line">    <span class="keyword">return</span> output_str</span><br><span class="line"></span><br><span class="line">input_str = <span class="string">&quot;how are you&quot;</span> <span class="comment"># 输入一个词：Python</span></span><br><span class="line">generated_text = generate_text_greedy_search(model, input_str) <span class="comment"># 模型跟着这个字生成后续文本</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;生成的文本:&quot;</span>, generated_text) <span class="comment"># 打印预测文本</span></span><br></pre></td></tr></table></figure><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 定义集束搜索的函数</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">generate_text_beam_search</span>(<span class="params">model, input_str, max_len=<span class="number">50</span>, beam_width=<span class="number">5</span></span>):</span><br><span class="line">    model.<span class="built_in">eval</span>()  <span class="comment"># 将模型设置为评估（测试）模式，关闭dropout和batch normalization等训练相关的层</span></span><br><span class="line">    <span class="comment"># 将输入字符串中的每个token 转换为其在词汇表中的索引</span></span><br><span class="line">    input_tokens = [vocab[token] <span class="keyword">for</span> token <span class="keyword">in</span> input_str.split()]</span><br><span class="line">    <span class="comment"># 创建一个列表，用于存储候选序列</span></span><br><span class="line">    candidates = [(input_tokens, <span class="number">0.0</span>)]</span><br><span class="line">    <span class="keyword">with</span> torch.no_grad():  <span class="comment"># 禁用梯度计算，以节省内存并加速测试过程</span></span><br><span class="line">        <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(max_len):  <span class="comment"># 生成最多max_len个tokens</span></span><br><span class="line">            new_candidates = []</span><br><span class="line">            <span class="keyword">for</span> candidate, candidate_score <span class="keyword">in</span> candidates:</span><br><span class="line">                inputs = torch.LongTensor(candidate).unsqueeze(<span class="number">0</span>).to(device)</span><br><span class="line">                outputs = model(inputs) <span class="comment"># 输出 logits形状为[1, len(output_tokens), vocab_size]</span></span><br><span class="line">                logits = outputs[:, -<span class="number">1</span>, :] <span class="comment"># 只关心最后一个时间步（即最新生成的token）的logits</span></span><br><span class="line">                <span class="comment"># 找到具有最高分数的前beam_width个tokens</span></span><br><span class="line">                scores, next_tokens = torch.topk(logits, beam_width, dim=-<span class="number">1</span>)</span><br><span class="line">                final_results = [] <span class="comment"># 初始化输出序列</span></span><br><span class="line">                <span class="keyword">for</span> score, next_token <span class="keyword">in</span> <span class="built_in">zip</span>(scores.squeeze(), next_tokens.squeeze()):</span><br><span class="line">                    new_candidate = candidate + [next_token.item()]</span><br><span class="line">                    new_score = candidate_score - score.item()  <span class="comment"># 使用负数，因为我们需要降序排列</span></span><br><span class="line">                    <span class="keyword">if</span> next_token.item() == vocab[<span class="string">&quot;&lt;eos&gt;&quot;</span>]:</span><br><span class="line">                        <span class="comment"># 如果生成的token是EOS（结束符），将其添加到最终结果中</span></span><br><span class="line">                        final_results.append((new_candidate, new_score))</span><br><span class="line">                    <span class="keyword">else</span>:</span><br><span class="line">                        <span class="comment"># 将新生成的候选序列添加到新候选列表中</span></span><br><span class="line">                        new_candidates.append((new_candidate, new_score))</span><br><span class="line">            <span class="comment"># 从新候选列表中选择得分最高的beam_width个序列</span></span><br><span class="line">            candidates = <span class="built_in">sorted</span>(new_candidates, key=<span class="keyword">lambda</span> x: x[<span class="number">1</span>])[:beam_width]</span><br><span class="line">    <span class="comment"># 选择得分最高的候选序列</span></span><br><span class="line">    best_candidate, _ = <span class="built_in">sorted</span>(candidates, key=<span class="keyword">lambda</span> x: x[<span class="number">1</span>])[<span class="number">0</span>]</span><br><span class="line">    <span class="comment"># 将输出 token 转换回文本字符串</span></span><br><span class="line">    output_str = <span class="string">&quot; &quot;</span>.join([vocab.get_itos()[token] <span class="keyword">for</span> token <span class="keyword">in</span> best_candidate <span class="keyword">if</span> vocab.get_itos()[token] != <span class="string">&quot;&lt;pad&gt;&quot;</span>])</span><br><span class="line">    <span class="keyword">return</span> output_str</span><br><span class="line"></span><br><span class="line">model.load_state_dict(torch.load(<span class="string">&#x27;trained_model_2023-05-05_14-08-24.pt&#x27;</span>)) <span class="comment"># 加载模型</span></span><br><span class="line">input_str = <span class="string">&quot;my name&quot;</span>  <span class="comment"># 输入几个词</span></span><br><span class="line">generated_text = generate_text_beam_search(model, input_str)  <span class="comment"># 模型跟着这些词生成后续文本</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;生成的文本:&quot;</span>, generated_text)  <span class="comment"># 打印生成的文本</span></span><br></pre></td></tr></table></figure><figure class="highlight markdown"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">生成的文本: my name was also used in 1897 by lucasfilm games in the common by lucasfilm games in the common by lucasfilm games in the common by lucasfilm games in the common by lucasfilm games in the common by lucasfilm games in the common by lucasfilm games in the common by lucasfilm games</span><br></pre></td></tr></table></figure>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;从零到一：如何训练简版生成式GPT模型，快速实现创意写作&quot;&gt;&lt;a href=&quot;#从零到一：如何训练简版生成式GPT模型，快速实现创意写作&quot; class=&quot;headerlink&quot; title=&quot;从零到一：如何训练简版生成式GPT模型，快速实现创意写作&quot;&gt;&lt;/a&gt;从零</summary>
      
    
    
    
    
  </entry>
  
  <entry>
    <title></title>
    <link href="https://rozen12123.github.io/posts/0.html"/>
    <id>https://rozen12123.github.io/posts/0.html</id>
    <published>2025-03-16T11:46:57.692Z</published>
    <updated>2025-03-17T11:44:49.179Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Transformer：GPT背后的”造脑工程”全解析（含手搓过程）"><a href="#Transformer：GPT背后的”造脑工程”全解析（含手搓过程）" class="headerlink" title="Transformer：GPT背后的”造脑工程”全解析（含手搓过程）"></a>Transformer：GPT背后的”造脑工程”全解析（含手搓过程）</h1><p><strong>Transformer</strong> 是人工智能领域的革命性架构，通过<strong>自注意力机制</strong>让模型像人类一样”全局理解”上下文关系。它摒弃传统循环结构，采用<strong>并行计算</strong>实现高效训练，配合<strong>位置编码</strong>破解序列的时空密码，在机器翻译、文本生成等任务中实现质的飞跃。GPT、BERT等顶尖模型均基于Transformer，其<strong>多头注意力</strong>设计如同给AI装上”多核大脑”，可同时捕捉词语间的语法、语义、指代等多维关系，成为通向通用人工智能的重要基石。</p><p><img src="https://rozen-picture.oss-cn-beijing.aliyuncs.com/img/202503161949640.png" alt="image-20250316194945482"></p><h2 id="一、从”人工智障”到”智能涌现”：Transformer的降维打击"><a href="#一、从”人工智障”到”智能涌现”：Transformer的降维打击" class="headerlink" title="一、从”人工智障”到”智能涌现”：Transformer的降维打击"></a>一、从”人工智障”到”智能涌现”：Transformer的降维打击</h2><p><strong>震撼对比实验</strong>：<br>使用相同训练数据（维基百科+图书语料）  </p><ul><li>RNN模型：”巴黎是法国的首都，位于__” → “塞纳河畔”（正确率68%）  </li><li>Transformer：”巴黎是法国的首都，位于__” → “北部法兰西岛大区”（正确率92%）  </li></ul><p><strong>传统模型三大痛点</strong>：</p><ol><li>梯度消失：长距离依赖难以捕捉（如”虽然…但是…”结构）</li><li>计算低效：无法并行处理序列数据</li><li>记忆瓶颈：固定长度上下文窗口</li></ol><p>&nbsp;</p><h2 id="二、位置编码"><a href="#二、位置编码" class="headerlink" title="二、位置编码"></a>二、位置编码</h2><p>transfomer的其他结构均在之前文章有过涉及，这里着重讲一下位置编码。</p><p><img src="https://rozen-picture.oss-cn-beijing.aliyuncs.com/img/202503161952174.png" alt="image-20250316195236095"></p><p>由于Transformer模型不使⽤循环神经⽹络，因此⽆法从序列中学习到位置信息。为了解决这个问题，需要为输⼊序列添加位置编码，将每个词的位置信息加⼊词向量中。</p><p>==通过位置编码加入每一个token的位置信息==</p><p>图中的类似于太极图的那个符号其实是“正弦”符号。正弦位置编码使⽤不同频率的正弦和余弦函数对每个位置进⾏编码。编码后，每个位置都会得到⼀个固定的位置编码，与词向量拼接或相加后，可以作为模型的输⼊。</p><p>正弦位置编码具有平滑性和保留相对位置信息等优点，因此在原始的Transformer论⽂中被采⽤。当然，也有其他位置编码⽅法，如可学习的位置编码，它将位置信息作为模型参数进⾏学习。</p><h2 id="三、分部手搓Transformer核心组件"><a href="#三、分部手搓Transformer核心组件" class="headerlink" title="三、分部手搓Transformer核心组件"></a>三、分部手搓Transformer核心组件</h2><p>这个逐步拆解的过程是从中⼼到两边、从左到右进⾏的。也就是从中⼼组件到外围延展，从编码器到解码器延展，然后把它们组合成Transformer类。</p><p><img src="https://rozen-picture.oss-cn-beijing.aliyuncs.com/img/202503161955544.png" alt="image-20250316195524344">以下是代码的关键组件。</p><p>（1）多头⾃注意⼒：通过<code>ScaledDotProductAttention</code>类实现缩放点积注意⼒机制，然后通过<code>MultiHeadAttention</code>类实现多头⾃注意⼒机制。</p><p>（2）逐位置前馈⽹络：通过<code>PoswiseFeedForwardNet</code>类实现逐位置前馈⽹络。</p><p>（3）正弦位置编码表：通过<code>get_sin_code_table</code>函数⽣成正弦位置编码表。</p><p>（4）填充掩码：通过<code>get_attn_pad_mask</code>函数为填充令牌⽣成注意⼒掩码，避免注意⼒机制关注⽆⽤的信息。</p><p>（5）编码器层：通过<code>EncoderLayer</code>类定义编码器的单层。</p><p>（6）编码器：通过<code>Encoder</code>类定义<code>Transformer</code>完整的编码器部分。</p><p>（7）后续掩码：通过<code>get_attn_subsequent_mask</code>函数为后续令牌（当前位置后⾯的信息）⽣成注意⼒掩码，避免解码器中的注意⼒机制“偷窥”未来的⽬标数据。</p><p>（8）解码器层：通过<code>DecoderLayer</code>类定义解码器的单层。</p><p>（9）解码器：通过<code>Decoder</code>类定义<code>Transformer</code>完整的解码器部分。</p><p>（10）<code>Transformer</code>类：此类将编码器和解码器整合为完整的<code>Transformer</code>模型。</p><h3 id="3-1-多头自注意力（包含残差连接和归一化）"><a href="#3-1-多头自注意力（包含残差连接和归一化）" class="headerlink" title="3.1 多头自注意力（包含残差连接和归一化）"></a>3.1 多头自注意力（包含残差连接和归一化）</h3><p>多头自注意力的结构如下：</p><p><img src="https://rozen-picture.oss-cn-beijing.aliyuncs.com/img/202503161958988.png" alt="image-20250316195800877"></p><p><img src="https://rozen-picture.oss-cn-beijing.aliyuncs.com/img/202503161958133.png" alt="image-20250316195813913" style="zoom: 50%;"></p><p>这⾥我们有两个⼦组件：<code>ScaledDotProductAttention</code>（缩放点积注意⼒）类和<code>MultiHeadAttention</code>（多头⾃注意⼒）类。它们在Transformer架构中负责实现⾃注意⼒机制。</p><p>其中，<code>ScaledDotProductAttention</code>类是构成<code>MultiHeadAttention</code>类的组件元素，也就是说，在多头⾃注意⼒中的每⼀个头，都使⽤缩放点积注意⼒来实现。</p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np <span class="comment"># 导入 numpy 库</span></span><br><span class="line"><span class="keyword">import</span> torch <span class="comment"># 导入 torch 库</span></span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn <span class="comment"># 导入 torch.nn 库</span></span><br><span class="line">d_k = <span class="number">64</span> <span class="comment"># K(=Q) 维度</span></span><br><span class="line">d_v = <span class="number">64</span> <span class="comment"># V 维度</span></span><br><span class="line"><span class="comment"># 定义缩放点积注意力类</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">ScaledDotProductAttention</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="built_in">super</span>(ScaledDotProductAttention, self).__init__()        </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, Q, K, V, attn_mask</span>):</span><br><span class="line">        <span class="comment">#------------------------- 维度信息 --------------------------------        </span></span><br><span class="line">        <span class="comment"># Q K V [batch_size, n_heads, len_q/k/v, dim_q=k/v] (dim_q=dim_k)</span></span><br><span class="line">        <span class="comment"># attn_mask [batch_size, n_heads, len_q, len_k]</span></span><br><span class="line">        <span class="comment">#----------------------------------------------------------------</span></span><br><span class="line">        <span class="comment"># 计算注意力分数（原始权重）[batch_size，n_heads，len_q，len_k]</span></span><br><span class="line">        scores = torch.matmul(Q, K.transpose(-<span class="number">1</span>, -<span class="number">2</span>)) / np.sqrt(d_k) </span><br><span class="line">        <span class="comment">#------------------------- 维度信息 --------------------------------        </span></span><br><span class="line">        <span class="comment"># scores [batch_size, n_heads, len_q, len_k]</span></span><br><span class="line">        <span class="comment">#-----------------------------------------------------------------        </span></span><br><span class="line">        <span class="comment"># 使用注意力掩码，将 attn_mask 中值为 1 的位置的权重替换为极小值</span></span><br><span class="line">        <span class="comment">#------------------------- 维度信息 -------------------------------- </span></span><br><span class="line">        <span class="comment"># attn_mask [batch_size, n_heads, len_q, len_k], 形状和 scores 相同</span></span><br><span class="line">        <span class="comment">#-----------------------------------------------------------------    </span></span><br><span class="line">        scores.masked_fill_(attn_mask, -<span class="number">1e9</span>) </span><br><span class="line">        <span class="comment"># 对注意力分数进行 softmax 归一化</span></span><br><span class="line">        weights = nn.Softmax(dim=-<span class="number">1</span>)(scores) </span><br><span class="line">        <span class="comment">#------------------------- 维度信息 -------------------------------- </span></span><br><span class="line">        <span class="comment"># weights [batch_size, n_heads, len_q, len_k], 形状和 scores 相同</span></span><br><span class="line">        <span class="comment">#-----------------------------------------------------------------         </span></span><br><span class="line">        <span class="comment"># 计算上下文向量（也就是注意力的输出）, 是上下文信息的紧凑表示</span></span><br><span class="line">        context = torch.matmul(weights, V) </span><br><span class="line">        <span class="comment">#------------------------- 维度信息 -------------------------------- </span></span><br><span class="line">        <span class="comment"># context [batch_size, n_heads, len_q, dim_v]</span></span><br><span class="line">        <span class="comment">#-----------------------------------------------------------------    </span></span><br><span class="line">        <span class="keyword">return</span> context, weights <span class="comment"># 返回上下文向量和注意力分数</span></span><br></pre></td></tr></table></figure><p>整个过程配合代码如下图所示：</p><p><img src="https://rozen-picture.oss-cn-beijing.aliyuncs.com/img/202503162011238.png" alt="image-20250316201105122" style="zoom:50%;"></p><p> 前向传播（forward）：</p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, Q, K, V, attn_mask</span>):</span><br></pre></td></tr></table></figure><ul><li><strong><code>Q</code></strong>: 查询向量 (query)，形状是 <code>[batch_size, n_heads, len_q, dim_q]</code>。</li><li><strong><code>K</code></strong>: 键向量 (key)，形状是 <code>[batch_size, n_heads, len_k, dim_k]</code>。</li><li><strong><code>V</code></strong>: 值向量 (value)，形状是 <code>[batch_size, n_heads, len_v, dim_v]</code>。</li><li><strong><code>attn_mask</code></strong>: 注意力掩码 (mask)，形状是 <code>[batch_size, n_heads, len_q, len_k]</code>。用于==在计算注意力时屏蔽某些位置==（例如在解码器中，避免未来位置被看到）。</li></ul><p>&nbsp;</p><p>应用掩码:</p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">scores.masked_fill_(attn_mask, -<span class="number">1e9</span>)</span><br></pre></td></tr></table></figure><ul><li><strong>掩码应用</strong>：<code>attn_mask</code> 具有和 <code>scores</code> 相同的形状（<code>[batch_size, n_heads, len_q, len_k]</code>）。将 <code>attn_mask</code> 中为 <code>1</code> 的位置替换为一个非常小的值 <code>-1e9</code>，这些小值在后续的 <code>softmax</code> 操作中会被“屏蔽”，即变为 0，避免这些位置的注意力权重被关注。</li></ul><p>即1是需要忽略的部分，0是不需要忽略的部分。</p><p>attn_mask生成方式通常取决于以下几个因素：</p><ol><li><strong>解码器中的未来掩码（用于防止信息泄漏）</strong></li></ol><p>在 Transformer 的解码器中，我们需要确保模型只能看到当前时刻及之前的词，而不能看到未来时刻的词。例如，当前时刻的第 <code>t</code> 个位置的查询 <code>Q[t]</code> 应该只依赖于前 <code>t</code> 个位置的键 <code>K</code> 和对应的值 <code>V</code>。这样做的目的是防止在训练时未来信息泄漏。</p><p>例如，如果序列长度是 <code>4</code>，<code>attn_mask</code> 应该是一个上三角矩阵，表示模型不能看到未来时刻的内容。具体来说，<code>attn_mask</code> 会是一个形状为 <code>[batch_size, n_heads, len_q, len_k]</code> 的矩阵，其中 <code>attn_mask[i, j, p, q] = 1</code> 表示第 <code>i</code> 个样本、第 <code>j</code> 个头、第 <code>p</code> 个查询位置和第 <code>q</code> 个键位置之间的注意力需要被屏蔽。</p><p>举个例子，假设 <code>attn_mask</code> 为：</p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">[<span class="number">1</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>]   <span class="comment"># 第 0 个位置只能看到自己</span></span><br><span class="line">[<span class="number">1</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">0</span>]   <span class="comment"># 第 1 个位置可以看到自己和第 0 个位置</span></span><br><span class="line">[<span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">0</span>]   <span class="comment"># 第 2 个位置可以看到自己和前两个位置</span></span><br><span class="line">[<span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>]   <span class="comment"># 第 3 个位置可以看到自己和前三个位置</span></span><br></pre></td></tr></table></figure><p>这样，对于 <code>attn_mask</code> 中的每个位置为 <code>1</code> 的部分，<code>scores</code> 中对应的位置会被屏蔽（设为极小的值 <code>-1e9</code>），从而避免模型在生成时刻 <code>t</code> 的预测时“看到”未来的信息。</p><ol><li><strong>填充（Padding）掩码（用于忽略填充位置）</strong></li></ol><p>在处理变长输入序列时，序列中的某些位置可能是填充符（<code>&lt;PAD&gt;</code>），这些填充符并不包含实际的信息，因此我们希望忽略它们对注意力计算的影响。为了避免填充符影响模型的注意力计算，我们会将填充符对应的位置的 <code>attn_mask</code> 设置为 <code>1</code>（表示屏蔽这些位置）。</p><p>假设输入序列是：</p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">0</span>, <span class="number">0</span>]  <span class="comment"># 1, 2, 3 是实际内容，0 是填充符</span></span><br></pre></td></tr></table></figure><p>对应的 <code>attn_mask</code> 可以是：</p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[<span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">1</span>]  <span class="comment"># 填充符位置被标记为 1，表示要屏蔽</span></span><br></pre></td></tr></table></figure><p>这样，<code>attn_mask</code> 中为 <code>1</code> 的位置就会在计算注意力时被屏蔽，确保填充符不会影响计算。</p><ol><li><strong>其他任务相关掩码</strong></li></ol><p>有时，<code>attn_mask</code> 也可以根据特定任务的需求自定义。例如，某些任务可能要求在计算注意力时忽略特定的区域，或者仅在特定的部分计算注意力。这种情况通常是通过任务外部的逻辑生成掩码。</p><p>&nbsp;</p><p>下⾯定义多头⾃注意⼒另⼀个⼦组件，多头⾃注意⼒类（这⾥同时包含残差连接和层归⼀化操作）</p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 定义多头自注意力类</span></span><br><span class="line">d_embedding = <span class="number">512</span>  <span class="comment"># Embedding 的维度</span></span><br><span class="line">n_heads = <span class="number">8</span>  <span class="comment"># Multi-Head Attention 中头的个数</span></span><br><span class="line">batch_size = <span class="number">3</span> <span class="comment"># 每一批的数据大小</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">MultiHeadAttention</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="built_in">super</span>(MultiHeadAttention, self).__init__()</span><br><span class="line">        self.W_Q = nn.Linear(d_embedding, d_k * n_heads) <span class="comment"># Q的线性变换层</span></span><br><span class="line">        self.W_K = nn.Linear(d_embedding, d_k * n_heads) <span class="comment"># K的线性变换层</span></span><br><span class="line">        self.W_V = nn.Linear(d_embedding, d_v * n_heads) <span class="comment"># V的线性变换层</span></span><br><span class="line">        self.linear = nn.Linear(n_heads * d_v, d_embedding)</span><br><span class="line">        self.layer_norm = nn.LayerNorm(d_embedding)</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, Q, K, V, attn_mask</span>): </span><br><span class="line">        <span class="comment">#------------------------- 维度信息 -------------------------------- </span></span><br><span class="line">        <span class="comment"># Q K V [batch_size, len_q/k/v, embedding_dim] </span></span><br><span class="line">        <span class="comment">#-----------------------------------------------------------------        </span></span><br><span class="line">        residual, batch_size = Q, Q.size(<span class="number">0</span>) <span class="comment"># 保留残差连接</span></span><br><span class="line">        <span class="comment"># 将输入进行线性变换和重塑，以便后续处理</span></span><br><span class="line">        q_s = self.W_Q(Q).view(batch_size, -<span class="number">1</span>, n_heads, d_k).transpose(<span class="number">1</span>,<span class="number">2</span>)        </span><br><span class="line">        k_s = self.W_K(K).view(batch_size, -<span class="number">1</span>, n_heads, d_k).transpose(<span class="number">1</span>,<span class="number">2</span>)</span><br><span class="line">        v_s = self.W_V(V).view(batch_size, -<span class="number">1</span>, n_heads, d_v).transpose(<span class="number">1</span>,<span class="number">2</span>)</span><br><span class="line">        <span class="comment">#------------------------- 维度信息 -------------------------------- </span></span><br><span class="line">        <span class="comment"># q_s k_s v_s: [batch_size, n_heads, len_q/k/v, d_q=k/v]</span></span><br><span class="line">        <span class="comment">#----------------------------------------------------------------- </span></span><br><span class="line">        <span class="comment"># 将注意力掩码复制到多头 attn_mask: [batch_size, n_heads, len_q, len_k]</span></span><br><span class="line">        attn_mask = attn_mask.unsqueeze(<span class="number">1</span>).repeat(<span class="number">1</span>, n_heads, <span class="number">1</span>, <span class="number">1</span>)</span><br><span class="line">        <span class="comment">#------------------------- 维度信息 -------------------------------- </span></span><br><span class="line">        <span class="comment"># attn_mask [batch_size, n_heads, len_q, len_k]</span></span><br><span class="line">        <span class="comment">#----------------------------------------------------------------- </span></span><br><span class="line">        <span class="comment"># 使用缩放点积注意力计算上下文和注意力权重</span></span><br><span class="line">        context, weights = ScaledDotProductAttention()(q_s, k_s, v_s, attn_mask)</span><br><span class="line">        <span class="comment">#------------------------- 维度信息 -------------------------------- </span></span><br><span class="line">        <span class="comment"># context [batch_size, n_heads, len_q, dim_v]</span></span><br><span class="line">        <span class="comment"># weights [batch_size, n_heads, len_q, len_k]</span></span><br><span class="line">        <span class="comment">#----------------------------------------------------------------- </span></span><br><span class="line">        <span class="comment"># 通过调整维度将多个头的上下文向量连接在一起</span></span><br><span class="line">        context = context.transpose(<span class="number">1</span>, <span class="number">2</span>).contiguous().view(batch_size, -<span class="number">1</span>, n_heads * d_v) </span><br><span class="line">        <span class="comment">#------------------------- 维度信息 -------------------------------- </span></span><br><span class="line">        <span class="comment"># context [batch_size, len_q, n_heads * dim_v]</span></span><br><span class="line">        <span class="comment">#-----------------------------------------------------------------        </span></span><br><span class="line">        <span class="comment"># 用一个线性层把连接后的多头自注意力结果转换，原始地嵌入维度</span></span><br><span class="line">        output = self.linear(context) </span><br><span class="line">        <span class="comment">#------------------------- 维度信息 -------------------------------- </span></span><br><span class="line">        <span class="comment"># output [batch_size, len_q, embedding_dim]</span></span><br><span class="line">        <span class="comment">#-----------------------------------------------------------------        </span></span><br><span class="line">        <span class="comment"># 与输入 (Q) 进行残差链接，并进行层归一化后输出</span></span><br><span class="line">        output = self.layer_norm(output + residual)</span><br><span class="line">        <span class="comment">#------------------------- 维度信息 -------------------------------- </span></span><br><span class="line">        <span class="comment"># output [batch_size, len_q, embedding_dim]</span></span><br><span class="line">        <span class="comment">#-----------------------------------------------------------------        </span></span><br><span class="line">        <span class="keyword">return</span> output, weights <span class="comment"># 返回层归一化的输出和注意力权重</span></span><br></pre></td></tr></table></figure><p><img src="https://rozen-picture.oss-cn-beijing.aliyuncs.com/img/202503162025160.png" alt="image-20250316202555045"></p><p>==将输⼊进⾏线性变换和重塑，就是为了形成多个头==</p><p>&nbsp;</p><h3 id="3-2-逐位置前馈网络（包含残差连接和层归一化）"><a href="#3-2-逐位置前馈网络（包含残差连接和层归一化）" class="headerlink" title="3.2 逐位置前馈网络（包含残差连接和层归一化）"></a>3.2 逐位置前馈网络（包含残差连接和层归一化）</h3><p>前馈神经⽹络（Feed-Forward Network）我们都了解，是⼀个包含全连接层的神经络。这种⽹络在计算过程中是按照从输⼊到输出的⽅向进⾏前馈传播的。</p><p>但是这个“Position- wise”如何理解？</p><p>在这⾥，“Poswise”或“Position-wise”是指这个前馈神经⽹络独⽴地作⽤在输⼊序列的每个位置（即token）上，也就是对⾃注意⼒机制处理后的结果上的各个位置进⾏独⽴处理，⽽不是把⾃注意⼒结果展平之后，以⼀个⼤的⼀维张量的形式整体输⼊前馈⽹络。这意味着对于序列中的每个位置，我们都在该位置应⽤相同的神经⽹络，做相同的处理，并且不会受到其他位置的影响。因此，逐位置操作保持了输⼊序列的原始顺序</p><p>所以⽆论是多头⾃注意⼒组件，还是前馈神经⽹络组件，都严格地保证“队形”，不打乱、不整合、不循环，⽽这种对序列位置信息的完整保持和并⾏处理，正是Transformer的核⼼思路。</p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 定义逐位置前馈网络类</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">PoswiseFeedForwardNet</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, d_ff=<span class="number">2048</span></span>):</span><br><span class="line">        <span class="built_in">super</span>(PoswiseFeedForwardNet, self).__init__()</span><br><span class="line">        <span class="comment"># 定义一维卷积层 1，用于将输入映射到更高维度</span></span><br><span class="line">        self.conv1 = nn.Conv1d(in_channels=d_embedding, out_channels=d_ff, kernel_size=<span class="number">1</span>)</span><br><span class="line">        <span class="comment"># 定义一维卷积层 2，用于将输入映射回原始维度</span></span><br><span class="line">        self.conv2 = nn.Conv1d(in_channels=d_ff, out_channels=d_embedding, kernel_size=<span class="number">1</span>)</span><br><span class="line">        <span class="comment"># 定义层归一化</span></span><br><span class="line">        self.layer_norm = nn.LayerNorm(d_embedding)</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, inputs</span>): </span><br><span class="line">        <span class="comment">#------------------------- 维度信息 -------------------------------- </span></span><br><span class="line">        <span class="comment"># inputs [batch_size, len_q, embedding_dim]</span></span><br><span class="line">        <span class="comment">#----------------------------------------------------------------                       </span></span><br><span class="line">        residual = inputs  <span class="comment"># 保留残差连接 </span></span><br><span class="line">        <span class="comment"># 在卷积层 1 后使用 ReLU 激活函数 </span></span><br><span class="line">        output = nn.ReLU()(self.conv1(inputs.transpose(<span class="number">1</span>, <span class="number">2</span>))) </span><br><span class="line">        <span class="comment">#------------------------- 维度信息 -------------------------------- </span></span><br><span class="line">        <span class="comment"># output [batch_size, d_ff, len_q]</span></span><br><span class="line">        <span class="comment">#----------------------------------------------------------------</span></span><br><span class="line">        <span class="comment"># 使用卷积层 2 进行降维 </span></span><br><span class="line">        output = self.conv2(output).transpose(<span class="number">1</span>, <span class="number">2</span>) </span><br><span class="line">        <span class="comment">#------------------------- 维度信息 -------------------------------- </span></span><br><span class="line">        <span class="comment"># output [batch_size, len_q, embedding_dim]</span></span><br><span class="line">        <span class="comment">#----------------------------------------------------------------</span></span><br><span class="line">        <span class="comment"># 与输入进行残差链接，并进行层归一化</span></span><br><span class="line">        output = self.layer_norm(output + residual) </span><br><span class="line">        <span class="comment">#------------------------- 维度信息 -------------------------------- </span></span><br><span class="line">        <span class="comment"># output [batch_size, len_q, embedding_dim]</span></span><br><span class="line">        <span class="comment">#----------------------------------------------------------------</span></span><br><span class="line">        <span class="keyword">return</span> output <span class="comment"># 返回加入残差连接后层归一化的结果</span></span><br></pre></td></tr></table></figure><p>PoswiseFeedForwardNet类实现了逐位置前馈⽹络，⽤于处理Transformer中⾃注意⼒机制的输出。其中包含两个⼀维卷积层，它们⼀个负责将输⼊映射到更⾼维度，⼀个再把它映射回原始维度。在两个卷积层之间，使⽤了<code>ReLU</code>函数。</p><p>在这⾥，⽤⼀维卷积层代替了论⽂中的全连接层（线性层）来实现前馈神经⽹络。其原因是全连接层不共享权重，⽽⼀维卷积层在各个位置上共享权重，所以能够减少⽹络参数的数量。</p><p>⼀维卷积层的⼯作原理是将卷积核（也称为过滤器或特征映射）沿输⼊序列的⼀个维度滑动（如下图所示），并在每个位置进⾏点积操作。在这种情况下，我们使⽤⼤⼩为1的卷积核。这样，卷积操作实际上只会在输⼊序列的⼀个位置进⾏计算，因此它能够独⽴地处理输⼊序列中的每个位置。</p><p><img src="https://rozen-picture.oss-cn-beijing.aliyuncs.com/img/202503162049287.png" alt="image-20250316204907198" style="zoom:50%;"></p><p>在PoswiseFeedForwardNet类中，⾸先通过使⽤conv1的多个卷积核将输⼊序列映射到更⾼的维度（程序中是2048维，这是⼀个可调节的超参数），并应⽤ReLU函数。</p><p>接着，conv2将映射后的序列降维到原始维度。这个过程在输⼊序列的每个位置上都是独⽴完成的，因为⼀维卷积层会在每个位置进⾏逐点操作。所以，逐位置前馈神经⽹络能够在每个位置上分别应⽤相同的运算，从⽽捕捉输⼊序列中各个位置的信息。</p><p>逐位置前馈神经网络有下⾯⼏个作⽤。</p><p>（1）增强模型的表达能⼒。FFN为模型提供了更强⼤的表达能⼒，==使其能够捕捉输⼊序列中更复杂的模式==。通过逐位置前馈神经⽹络和⾃注意⼒机制的组合，Transformer可以==学习到不同位置之间的⻓距离依赖关系==。</p><p>（2）信息融合。==FFN可以将⾃注意⼒机制输出的信息进⾏融合。==每个位置上的信息在经过FFN后，都会得到⼀个新表示。这个新表示可以看作原始信息在经过⼀定程度的⾮线性变换之后的结果。</p><p>（3）层间传递。在Transformer中，逐位置前馈神经⽹络将在每个编码器和解码器层中使⽤。</p><p>这样可以确保每⼀层的输出都经过了FFN的处理，从⽽在多层次上捕捉到序列中的特征。多头⾃注意⼒层和逐位置前馈神经⽹络层是编码器层结构中的两个主要组件，不过，在开始构建编码器层之前，还要再定义两个辅助性的组件。第⼀个是位置编码表，第⼆个是⽣成填充注意⼒掩码的函数。</p><p>&nbsp;</p><h3 id="3-3-正弦编码表"><a href="#3-3-正弦编码表" class="headerlink" title="3.3 正弦编码表"></a>3.3 正弦编码表</h3><p>Transformer模型的并⾏结构导致它不是按位置顺序来处理序列的，但是在处理序列尤其是注意⼒计算的过程中，仍需要位置信息来帮助捕捉序列中的顺序关系。为了解决这个问题，需要向输⼊序列中添加位置编码。</p><p>Tansformer的原始论⽂中使⽤的是正弦位置编码。它的计算公式如下：</p><script type="math/tex; mode=display">PE(\mathrm{pos},2i)=\sin\left(\frac{\mathrm{pos}}{10000^{2i/d}}\right)</script><script type="math/tex; mode=display">PE(\mathrm{pos},2i+1)=\cos\left(\frac{\mathrm{pos}}{10000^{2ild}}\right)</script><p>这种位置编码⽅式具有周期性和连续性的特点，可以让模型学会捕捉位置之间的相对关系和全</p><p>局关系。这个公式可以⽤于计算位置嵌⼊向量中每个维度的⻆度值。</p><p>■ pos：单词/标记在句⼦中的位置，从0到seq_len-1。</p><p>■ <em>d</em>：单词/标记嵌⼊向量的维度embedding_dim。</p><p>■ <em>i</em>：嵌⼊向量中的每个维度，从0到 $\frac{d}{2}-1$</p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 生成正弦位置编码表的函数，用于在 Transformer 中引入位置信息</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">get_sin_enc_table</span>(<span class="params">n_position, embedding_dim</span>):</span><br><span class="line">    <span class="comment">#------------------------- 维度信息 --------------------------------</span></span><br><span class="line">    <span class="comment"># n_position: 输入序列的最大长度</span></span><br><span class="line">    <span class="comment"># embedding_dim: 词嵌入向量的维度</span></span><br><span class="line">    <span class="comment">#-----------------------------------------------------------------    </span></span><br><span class="line">    <span class="comment"># 根据位置和维度信息，初始化正弦位置编码表</span></span><br><span class="line">    sinusoid_table = np.zeros((n_position, embedding_dim))    </span><br><span class="line">    <span class="comment"># 遍历所有位置和维度，计算角度值</span></span><br><span class="line">    <span class="keyword">for</span> pos_i <span class="keyword">in</span> <span class="built_in">range</span>(n_position):</span><br><span class="line">        <span class="keyword">for</span> hid_j <span class="keyword">in</span> <span class="built_in">range</span>(embedding_dim):</span><br><span class="line">            angle = pos_i / np.power(<span class="number">10000</span>, <span class="number">2</span> * (hid_j // <span class="number">2</span>) / embedding_dim)</span><br><span class="line">            sinusoid_table[pos_i, hid_j] = angle    </span><br><span class="line">    <span class="comment"># 计算正弦和余弦值</span></span><br><span class="line">    sinusoid_table[:, <span class="number">0</span>::<span class="number">2</span>] = np.sin(sinusoid_table[:, <span class="number">0</span>::<span class="number">2</span>])  <span class="comment"># dim 2i 偶数维</span></span><br><span class="line">    sinusoid_table[:, <span class="number">1</span>::<span class="number">2</span>] = np.cos(sinusoid_table[:, <span class="number">1</span>::<span class="number">2</span>])  <span class="comment"># dim 2i+1 奇数维    </span></span><br><span class="line">    <span class="comment">#------------------------- 维度信息 --------------------------------</span></span><br><span class="line">    <span class="comment"># sinusoid_table 的维度是 [n_position, embedding_dim]</span></span><br><span class="line">    <span class="comment">#----------------------------------------------------------------   </span></span><br><span class="line">    <span class="keyword">return</span> torch.FloatTensor(sinusoid_table)  <span class="comment"># 返回正弦位置编码表</span></span><br></pre></td></tr></table></figure><p>事实上，使⽤1、2、3、4等⾃然数序列作为位置编码确实可以为序列中的不同位置提供区分性。然⽽，这种⽅法可能在某些⽅⾯不如正弦和余弦函数⽣成的位置嵌⼊向量有效。</p><p>当我们使⽤⾃然数序列作为位置编码时，这些==编码是线性的==。这意味着相邻位置之间的差异在整个序列中保持恒定。然⽽，在许多任务中，==不同位置之间的关系可能更复杂==，可能需要⼀种能够捕捉到这种复杂关系的编码⽅法。</p><p>正弦和余弦函数⽣成的位置嵌⼊向量具有周期性和正交性，因此可以产⽣在各个尺度上都有区分性的位置嵌⼊。这使得模型可以==更容易地学习到序列中不同位置之间的关系==，特别是在捕捉⻓距离依赖关系时可能表现得更好。</p><p>所以，虽然使⽤⾃然数序列（1、2、3、4等）作为位置编码可以做⼀定的区分，但正弦和余弦函数⽣成的位置嵌⼊向量在捕捉序列中更复杂的位置关系⽅⾯更具优势。</p><p>&nbsp;</p><h3 id="3-4-填充掩码"><a href="#3-4-填充掩码" class="headerlink" title="3.4 填充掩码"></a>3.4 填充掩码</h3><p>在NLP任务中，输⼊序列的⻓度通常是不固定的。为了能够同时处理多个序列，我们需要将这些序列填充到相同的⻓度，将不等⻓的序列补充到等⻓，这样才能将它们整合成同⼀个批次进⾏训练。</p><p>通常使⽤⼀个特殊的标记（如，编码后这个token的值通常是0）来表示填充部分。</p><p>然⽽，这些填充符号并没有实际的含义，所以我们希望模型在计算注意⼒时忽略它们。因此，在编码器的输⼊部分，我们使⽤了填充位的注意⼒掩码机制（如下⻚图所示）。这个掩码机制的作⽤是在注意⼒计算的时候把⽆⽤的信息屏蔽，防⽌模型在计算注意⼒权重时关注到填充位。</p><p><img src="https://rozen-picture.oss-cn-beijing.aliyuncs.com/img/202503171820336.png" alt="image-20250317181955015" style="zoom:67%;"></p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 定义填充注意力掩码函数</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">get_attn_pad_mask</span>(<span class="params">seq_q, seq_k</span>):</span><br><span class="line">    <span class="comment">#------------------------- 维度信息 --------------------------------</span></span><br><span class="line">    <span class="comment"># seq_q 的维度是 [batch_size, len_q]</span></span><br><span class="line">    <span class="comment"># seq_k 的维度是 [batch_size, len_k]</span></span><br><span class="line">    <span class="comment">#-----------------------------------------------------------------</span></span><br><span class="line">    batch_size, len_q = seq_q.size()</span><br><span class="line">    batch_size, len_k = seq_k.size()</span><br><span class="line">    <span class="comment"># 生成布尔类型张量</span></span><br><span class="line">    pad_attn_mask = seq_k.data.eq(<span class="number">0</span>).unsqueeze(<span class="number">1</span>)  <span class="comment"># &lt;PAD&gt;token 的编码值为 0</span></span><br><span class="line">    <span class="comment">#------------------------- 维度信息 --------------------------------</span></span><br><span class="line">    <span class="comment"># pad_attn_mask 的维度是 [batch_size，1，len_k]</span></span><br><span class="line">    <span class="comment">#-----------------------------------------------------------------</span></span><br><span class="line">    <span class="comment"># 变形为与注意力分数相同形状的张量 </span></span><br><span class="line">    pad_attn_mask = pad_attn_mask.expand(batch_size, len_q, len_k)</span><br><span class="line">    <span class="comment">#------------------------- 维度信息 --------------------------------</span></span><br><span class="line">    <span class="comment"># pad_attn_mask 的维度是 [batch_size，len_q，len_k]</span></span><br><span class="line">    <span class="comment">#-----------------------------------------------------------------</span></span><br><span class="line">    <span class="keyword">return</span> pad_attn_mask</span><br></pre></td></tr></table></figure><p>我们为填充的⽂本序列创建⼀个与其形状相同的⼆维矩阵，称为填充掩码矩阵。填充掩码矩阵的⽬的是在注意⼒计算中屏蔽填充位置的影响。屏蔽流程如下。</p><p>（1）根据输⼊⽂本序列创建⼀个与其形状相同的⼆维矩阵。对于原始⽂本中的每个单词，矩阵中对应位置填充0；对于填充的符号，矩阵中对应位置填充1。</p><p>（2）为了将填充部分的权重降⾄接近负⽆穷，我们可以先将填充掩码矩阵中的1替换为⼀个⾮常⼤的负数（例如-1e9），再将处理后的填充掩码矩阵与注意⼒分数矩阵进⾏元素相加。这样，==有意义的token加了0，值保持不变，⽽填充部分加了⽆穷⼩值==，在注意⼒分数矩阵中的权重就会变得⾮常⼩。</p><p>（3）对注意⼒分数矩阵应⽤softmax函数进⾏归⼀化。==由于填充部分的权重接近负⽆穷，softmax函数会使其归⼀化后的权重接近于0==。这样，模型在计算注意⼒时就能够忽略填充部分的信息，专注于序列中实际包含的有效内容。</p><p>&nbsp;</p><h3 id="3-5-编码器层"><a href="#3-5-编码器层" class="headerlink" title="3.5 编码器层"></a>3.5 编码器层</h3><p>有了多头⾃注意⼒和逐位置前馈⽹络这两个主要组件，以及正弦位置编码表和填充掩码这两个辅助函数后，现在我们终于可以搭建编码器层这个核⼼组件了。</p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 定义编码器层类</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">EncoderLayer</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="built_in">super</span>(EncoderLayer, self).__init__()        </span><br><span class="line">        self.enc_self_attn = MultiHeadAttention() <span class="comment"># 多头自注意力层        </span></span><br><span class="line">        self.pos_ffn = PoswiseFeedForwardNet() <span class="comment"># 位置前馈神经网络层</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, enc_inputs, enc_self_attn_mask</span>):</span><br><span class="line">        <span class="comment">#------------------------- 维度信息 --------------------------------</span></span><br><span class="line">        <span class="comment"># enc_inputs 的维度是 [batch_size, seq_len, embedding_dim]</span></span><br><span class="line">        <span class="comment"># enc_self_attn_mask 的维度是 [batch_size, seq_len, seq_len]</span></span><br><span class="line">        <span class="comment">#-----------------------------------------------------------------</span></span><br><span class="line">        <span class="comment"># 将相同的 Q，K，V 输入多头自注意力层 , 返回的 attn_weights 增加了头数  </span></span><br><span class="line">        enc_outputs, attn_weights = self.enc_self_attn(enc_inputs, enc_inputs,</span><br><span class="line">                                               enc_inputs, enc_self_attn_mask)</span><br><span class="line">        <span class="comment">#------------------------- 维度信息 --------------------------------</span></span><br><span class="line">        <span class="comment"># enc_outputs 的维度是 [batch_size, seq_len, embedding_dim] </span></span><br><span class="line">        <span class="comment"># attn_weights 的维度是 [batch_size, n_heads, seq_len, seq_len]      </span></span><br><span class="line">        <span class="comment"># 将多头自注意力 outputs 输入位置前馈神经网络层</span></span><br><span class="line">        enc_outputs = self.pos_ffn(enc_outputs) <span class="comment"># 维度与 enc_inputs 相同</span></span><br><span class="line">        <span class="comment">#------------------------- 维度信息 --------------------------------</span></span><br><span class="line">        <span class="comment"># enc_outputs 的维度是 [batch_size, seq_len, embedding_dim] </span></span><br><span class="line">        <span class="comment">#-----------------------------------------------------------------</span></span><br><span class="line">        <span class="keyword">return</span> enc_outputs, attn_weights <span class="comment"># 返回编码器输出和每层编码器注意力权重</span></span><br></pre></td></tr></table></figure><p>这个类将多头自注意力层和位置前馈神经网络层组合在一起，并完成前向传播的计算。</p><p><code>EncoderLayer</code> 类将 <strong>多头自注意力机制</strong> 和 <strong>位置前馈神经网络</strong> 结合，完成了 Transformer 编码器层的基本结构。具体过程是：</p><ol><li>输入经过多头自注意力层，计算查询与键的注意力权重，并生成上下文向量。</li><li>将上下文向量输入到位置前馈神经网络中，得到最终的编码器输出。</li><li>返回编码器输出和每层的注意力权重。</li></ol><p>这种结构是 Transformer 编码器的核心部分，支持在输入序列中捕捉远距离依赖并进行非线性变换。</p><p><img src="https://rozen-picture.oss-cn-beijing.aliyuncs.com/img/202503171850255.png" alt="image-20250317185048109" style="zoom:50%;"></p><p>&nbsp;</p><h3 id="3-6-编码器"><a href="#3-6-编码器" class="headerlink" title="3.6 编码器"></a>3.6 编码器</h3><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 定义编码器类</span></span><br><span class="line">n_layers = <span class="number">6</span>  <span class="comment"># 设置 Encoder 的层数</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Encoder</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, corpus</span>):</span><br><span class="line">        <span class="built_in">super</span>(Encoder, self).__init__()        </span><br><span class="line">        self.src_emb = nn.Embedding(<span class="built_in">len</span>(corpus.src_vocab), d_embedding) <span class="comment"># 词嵌入层</span></span><br><span class="line">        self.pos_emb = nn.Embedding.from_pretrained( \</span><br><span class="line">          get_sin_enc_table(corpus.src_len+<span class="number">1</span>, d_embedding), freeze=<span class="literal">True</span>) <span class="comment"># 位置嵌入层</span></span><br><span class="line">        self.layers = nn.ModuleList(EncoderLayer() <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(n_layers))<span class="comment"># 编码器层数</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, enc_inputs</span>):  </span><br><span class="line">        <span class="comment">#------------------------- 维度信息 --------------------------------</span></span><br><span class="line">        <span class="comment"># enc_inputs 的维度是 [batch_size, source_len]</span></span><br><span class="line">        <span class="comment">#-----------------------------------------------------------------</span></span><br><span class="line">        <span class="comment"># 创建一个从 1 到 source_len 的位置索引序列</span></span><br><span class="line">        pos_indices = torch.arange(<span class="number">1</span>, enc_inputs.size(<span class="number">1</span>) + <span class="number">1</span>).unsqueeze(<span class="number">0</span>).to(enc_inputs)</span><br><span class="line">        <span class="comment">#------------------------- 维度信息 --------------------------------</span></span><br><span class="line">        <span class="comment"># pos_indices 的维度是 [1, source_len]</span></span><br><span class="line">        <span class="comment">#-----------------------------------------------------------------             </span></span><br><span class="line">        <span class="comment"># 对输入进行词嵌入和位置嵌入相加 [batch_size, source_len，embedding_dim]</span></span><br><span class="line">        enc_outputs = self.src_emb(enc_inputs) + self.pos_emb(pos_indices)</span><br><span class="line">        <span class="comment">#------------------------- 维度信息 --------------------------------</span></span><br><span class="line">        <span class="comment"># enc_outputs 的维度是 [batch_size, seq_len, embedding_dim]</span></span><br><span class="line">        <span class="comment">#-----------------------------------------------------------------</span></span><br><span class="line">        <span class="comment"># 生成自注意力掩码</span></span><br><span class="line">        enc_self_attn_mask = get_attn_pad_mask(enc_inputs, enc_inputs) </span><br><span class="line">        <span class="comment">#------------------------- 维度信息 --------------------------------</span></span><br><span class="line">        <span class="comment"># enc_self_attn_mask 的维度是 [batch_size, len_q, len_k]        </span></span><br><span class="line">        <span class="comment">#-----------------------------------------------------------------         </span></span><br><span class="line">        enc_self_attn_weights = [] <span class="comment"># 初始化 enc_self_attn_weights</span></span><br><span class="line">        <span class="comment"># 通过编码器层 [batch_size, seq_len, embedding_dim]</span></span><br><span class="line">        <span class="keyword">for</span> layer <span class="keyword">in</span> self.layers: </span><br><span class="line">            enc_outputs, enc_self_attn_weight = layer(enc_outputs, enc_self_attn_mask)</span><br><span class="line">            enc_self_attn_weights.append(enc_self_attn_weight)</span><br><span class="line">        <span class="comment">#------------------------- 维度信息 --------------------------------</span></span><br><span class="line">        <span class="comment"># enc_outputs 的维度是 [batch_size, seq_len, embedding_dim] 维度与 enc_inputs 相同</span></span><br><span class="line">        <span class="comment"># enc_self_attn_weights 是一个列表，每个元素的维度是 [batch_size, n_heads, seq_len, seq_len]          </span></span><br><span class="line">        <span class="comment">#-----------------------------------------------------------------</span></span><br><span class="line">        <span class="keyword">return</span> enc_outputs, enc_self_attn_weights <span class="comment"># 返回编码器输出和编码器注意力权重</span></span><br></pre></td></tr></table></figure><p>这个编码器类实现了Transformer模型中的编码器部分，包括词嵌⼊、位置嵌⼊和多个编码器层。通过这个编码器，可以处理输⼊序列，并从中提取深层次的特征表示。这些特征表示可以直接应⽤于后续的任务，如序列到序列的⽣成任务（如机器翻译）或者分类任务（如情感分析）等。</p><p>BERT模型就只包含Transformer模型中的编码器部分，因此它很适合为各种NLP下游任务提供有⽤的特征表示。</p><p>编码器的定义⾄此结束，下⾯我们进⼊解码器组件。不过，在开始构建解码器层之前，也有⼀个⼩组件需要说明，它就是==⽣成后续注意⼒掩码的函数==。</p><p>&nbsp;</p><h3 id="3-7-后续掩码"><a href="#3-7-后续掩码" class="headerlink" title="3.7 后续掩码"></a>3.7 后续掩码</h3><p>在⾃然语⾔处理中，尤其是Seq2Seq任务中，我们需要为解码器提供正确的输⼊，对于已经⽣成的部分，我们要让解码器看到序列是否正确，然后⽤正确的信息（Ground Truth）来预测下⼀个词。但是与此同时，为了确保模型不会提前获取未来的信息，我们⼜需要在注意⼒计算中遮住当前位置后⾯的信息（Subsequent Positions）。</p><p>所以，对序列中的第⼀个位置，我们需要遮住后⾯所有的词；⽽对后⾯的词，需要遮住的词会逐渐减少（如下图所示）。⽐如把“咖哥 喜欢 ⼩冰”这句话输⼊解码器，当对“咖哥”计算注意⼒时，解码器不可以看到“喜欢”“⼩冰”这两个词。当对“喜欢”计算注意⼒时，解码器可以看到“咖哥”，不能看到“⼩冰”，因为它正是需要根据“咖哥”和“喜欢”这个上下⽂，来猜测咖哥喜欢谁。当对最后⼀个词”⼩冰”计算注意⼒的时候，前两个词就不是秘密了。</p><p><img src="https://rozen-picture.oss-cn-beijing.aliyuncs.com/img/202503171905263.png" alt="image-20250317190524034" style="zoom:80%;"></p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 生成后续注意力掩码的函数，用于在多头自注意力计算中忽略未来信息</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">get_attn_subsequent_mask</span>(<span class="params">seq</span>):</span><br><span class="line">    <span class="comment">#------------------------- 维度信息 --------------------------------</span></span><br><span class="line">    <span class="comment"># seq 的维度是 [batch_size, seq_len(Q)=seq_len(K)]</span></span><br><span class="line">    <span class="comment">#-----------------------------------------------------------------</span></span><br><span class="line">    <span class="comment"># 获取输入序列的形状</span></span><br><span class="line">    attn_shape = [seq.size(<span class="number">0</span>), seq.size(<span class="number">1</span>), seq.size(<span class="number">1</span>)]  </span><br><span class="line">    <span class="comment">#------------------------- 维度信息 --------------------------------</span></span><br><span class="line">    <span class="comment"># attn_shape 是一个一维张量 [batch_size, seq_len(Q), seq_len(K)]</span></span><br><span class="line">    <span class="comment">#-----------------------------------------------------------------</span></span><br><span class="line">    <span class="comment"># 使用 numpy 创建一个上三角矩阵（triu = triangle upper）</span></span><br><span class="line">    subsequent_mask = np.triu(np.ones(attn_shape), k=<span class="number">1</span>)</span><br><span class="line">    <span class="comment">#------------------------- 维度信息 --------------------------------</span></span><br><span class="line">    <span class="comment"># subsequent_mask 的维度是 [batch_size, seq_len(Q), seq_len(K)]</span></span><br><span class="line">    <span class="comment">#-----------------------------------------------------------------</span></span><br><span class="line">    <span class="comment"># 将 numpy 数组转换为 PyTorch 张量，并将数据类型设置为 byte（布尔值）</span></span><br><span class="line">    subsequent_mask = torch.from_numpy(subsequent_mask).byte()</span><br><span class="line">    <span class="comment">#------------------------- 维度信息 --------------------------------</span></span><br><span class="line">    <span class="comment"># 返回的 subsequent_mask 的维度是 [batch_size, seq_len(Q), seq_len(K)]</span></span><br><span class="line">    <span class="comment">#-----------------------------------------------------------------</span></span><br><span class="line">    <span class="keyword">return</span> subsequent_mask <span class="comment"># 返回后续位置的注意力掩码</span></span><br></pre></td></tr></table></figure><p>此段代码最终生成的是注意力掩码，根据上图第一行为例，因为咖哥只能看到自己来推测下面的词，所以先写出咖哥对整个句子的权重，在人为将看不到的地方取消关注（也就是替换成Zero weight），从上到下一行是一个时间的步长。</p><p>&nbsp;</p><h3 id="3-8-解码器层"><a href="#3-8-解码器层" class="headerlink" title="3.8 解码器层"></a>3.8 解码器层</h3><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 定义解码器层类</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">DecoderLayer</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="built_in">super</span>(DecoderLayer, self).__init__()        </span><br><span class="line">        self.dec_self_attn = MultiHeadAttention() <span class="comment"># 多头自注意力层       </span></span><br><span class="line">        self.dec_enc_attn = MultiHeadAttention()  <span class="comment"># 多头自注意力层，连接编码器和解码器        </span></span><br><span class="line">        self.pos_ffn = PoswiseFeedForwardNet() <span class="comment"># 位置前馈神经网络层</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, dec_inputs, enc_outputs, dec_self_attn_mask, dec_enc_attn_mask</span>):</span><br><span class="line">        <span class="comment">#------------------------- 维度信息 --------------------------------</span></span><br><span class="line">        <span class="comment"># dec_inputs 的维度是 [batch_size, target_len, embedding_dim]</span></span><br><span class="line">        <span class="comment"># enc_outputs 的维度是 [batch_size, source_len, embedding_dim]</span></span><br><span class="line">        <span class="comment"># dec_self_attn_mask 的维度是 [batch_size, target_len, target_len]</span></span><br><span class="line">        <span class="comment"># dec_enc_attn_mask 的维度是 [batch_size, target_len, source_len]</span></span><br><span class="line">        <span class="comment">#-----------------------------------------------------------------      </span></span><br><span class="line">        <span class="comment"># 将相同的 Q，K，V 输入多头自注意力层</span></span><br><span class="line">        dec_outputs, dec_self_attn = self.dec_self_attn(dec_inputs, dec_inputs, </span><br><span class="line">                                                        dec_inputs, dec_self_attn_mask)</span><br><span class="line">        <span class="comment">#------------------------- 维度信息 --------------------------------</span></span><br><span class="line">        <span class="comment"># dec_outputs 的维度是 [batch_size, target_len, embedding_dim]</span></span><br><span class="line">        <span class="comment"># dec_self_attn 的维度是 [batch_size, n_heads, target_len, target_len]</span></span><br><span class="line">        <span class="comment">#-----------------------------------------------------------------        </span></span><br><span class="line">        <span class="comment"># 将解码器输出和编码器输出输入多头自注意力层</span></span><br><span class="line">        dec_outputs, dec_enc_attn = self.dec_enc_attn(dec_outputs, enc_outputs, </span><br><span class="line">                                                      enc_outputs, dec_enc_attn_mask)</span><br><span class="line">        <span class="comment">#------------------------- 维度信息 --------------------------------</span></span><br><span class="line">        <span class="comment"># dec_outputs 的维度是 [batch_size, target_len, embedding_dim]</span></span><br><span class="line">        <span class="comment"># dec_enc_attn 的维度是 [batch_size, n_heads, target_len, source_len]</span></span><br><span class="line">        <span class="comment">#-----------------------------------------------------------------          </span></span><br><span class="line">        <span class="comment"># 输入位置前馈神经网络层</span></span><br><span class="line">        dec_outputs = self.pos_ffn(dec_outputs)</span><br><span class="line">        <span class="comment">#------------------------- 维度信息 --------------------------------</span></span><br><span class="line">        <span class="comment"># dec_outputs 的维度是 [batch_size, target_len, embedding_dim]</span></span><br><span class="line">        <span class="comment"># dec_self_attn 的维度是 [batch_size, n_heads, target_len, target_len]</span></span><br><span class="line">        <span class="comment"># dec_enc_attn 的维度是 [batch_size, n_heads, target_len, source_len]   </span></span><br><span class="line">        <span class="comment">#-----------------------------------------------------------------</span></span><br><span class="line">        <span class="comment"># 返回解码器层输出，每层的自注意力和解 - 编码器注意力权重</span></span><br><span class="line">        <span class="keyword">return</span> dec_outputs, dec_self_attn, dec_enc_attn</span><br></pre></td></tr></table></figure><p>定义了一个标准的解码器层，通过三个主要步骤处理输入：</p><ol><li><strong>自注意力</strong>：通过多头自注意力机制理解目标语言的上下文。</li><li><strong>编码器-解码器注意力</strong>：通过与编码器的输出进行交互，理解目标语言与源语言的关系。</li><li><strong>前馈神经网络</strong>：对解码器输出进行进一步的转换和处理。</li></ol><p>这些步骤共同作用，使得解码器能够生成目标语言的翻译或输出。</p><p><img src="https://rozen-picture.oss-cn-beijing.aliyuncs.com/img/202503171930172.png" alt="image-20250317193028046" style="zoom: 80%;"></p><p>&nbsp;</p><h3 id="3-9-解码器"><a href="#3-9-解码器" class="headerlink" title="3.9 解码器"></a>3.9 解码器</h3><p><img src="https://rozen-picture.oss-cn-beijing.aliyuncs.com/img/202503171930351.png" alt="image-20250317193057213" style="zoom: 50%;"></p><p>解码器类的实现代码如下：</p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#  定义解码器类</span></span><br><span class="line">n_layers = <span class="number">6</span>  <span class="comment"># 设置 Decoder 的层数</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Decoder</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, corpus</span>):</span><br><span class="line">        <span class="built_in">super</span>(Decoder, self).__init__()</span><br><span class="line">        self.tgt_emb = nn.Embedding(<span class="built_in">len</span>(corpus.tgt_vocab), d_embedding) <span class="comment"># 词嵌入层</span></span><br><span class="line">        self.pos_emb = nn.Embedding.from_pretrained( \</span><br><span class="line">           get_sin_enc_table(corpus.tgt_len+<span class="number">1</span>, d_embedding), freeze=<span class="literal">True</span>) <span class="comment"># 位置嵌入层        </span></span><br><span class="line">        self.layers = nn.ModuleList([DecoderLayer() <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(n_layers)]) <span class="comment"># 叠加多层</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, dec_inputs, enc_inputs, enc_outputs</span>): </span><br><span class="line">        <span class="comment">#------------------------- 维度信息 --------------------------------</span></span><br><span class="line">        <span class="comment"># dec_inputs 的维度是 [batch_size, target_len]</span></span><br><span class="line">        <span class="comment"># enc_inputs 的维度是 [batch_size, source_len]</span></span><br><span class="line">        <span class="comment"># enc_outputs 的维度是 [batch_size, source_len, embedding_dim]</span></span><br><span class="line">        <span class="comment">#-----------------------------------------------------------------   </span></span><br><span class="line">        <span class="comment"># 创建一个从 1 到 source_len 的位置索引序列</span></span><br><span class="line">        pos_indices = torch.arange(<span class="number">1</span>, dec_inputs.size(<span class="number">1</span>) + <span class="number">1</span>).unsqueeze(<span class="number">0</span>).to(dec_inputs)</span><br><span class="line">        <span class="comment">#------------------------- 维度信息 --------------------------------</span></span><br><span class="line">        <span class="comment"># pos_indices 的维度是 [1, target_len]</span></span><br><span class="line">        <span class="comment">#-----------------------------------------------------------------              </span></span><br><span class="line">        <span class="comment"># 对输入进行词嵌入和位置嵌入相加</span></span><br><span class="line">        dec_outputs = self.tgt_emb(dec_inputs) + self.pos_emb(pos_indices)</span><br><span class="line">        <span class="comment">#------------------------- 维度信息 --------------------------------</span></span><br><span class="line">        <span class="comment"># dec_outputs 的维度是 [batch_size, target_len, embedding_dim]</span></span><br><span class="line">         <span class="comment">#-----------------------------------------------------------------        </span></span><br><span class="line">        <span class="comment"># 生成解码器自注意力掩码和解码器 - 编码器注意力掩码</span></span><br><span class="line">        dec_self_attn_pad_mask = get_attn_pad_mask(dec_inputs, dec_inputs) <span class="comment"># 填充位掩码</span></span><br><span class="line">        dec_self_attn_subsequent_mask = get_attn_subsequent_mask(dec_inputs) <span class="comment"># 后续位掩码</span></span><br><span class="line">        dec_self_attn_mask = torch.gt((dec_self_attn_pad_mask \</span><br><span class="line">                                       + dec_self_attn_subsequent_mask), <span class="number">0</span>) </span><br><span class="line">        dec_enc_attn_mask = get_attn_pad_mask(dec_inputs, enc_inputs) <span class="comment"># 解码器 - 编码器掩码</span></span><br><span class="line">        <span class="comment">#------------------------- 维度信息 --------------------------------        </span></span><br><span class="line">        <span class="comment"># dec_self_attn_pad_mask 的维度是 [batch_size, target_len, target_len]</span></span><br><span class="line">        <span class="comment"># dec_self_attn_subsequent_mask 的维度是 [batch_size, target_len, target_len]</span></span><br><span class="line">        <span class="comment"># dec_self_attn_mask 的维度是 [batch_size, target_len, target_len]</span></span><br><span class="line">        <span class="comment"># dec_enc_attn_mask 的维度是 [batch_size, target_len, source_len]</span></span><br><span class="line">         <span class="comment">#-----------------------------------------------------------------       </span></span><br><span class="line">        dec_self_attns, dec_enc_attns = [], [] <span class="comment"># 初始化 dec_self_attns, dec_enc_attns</span></span><br><span class="line">        <span class="comment"># 通过解码器层 [batch_size, seq_len, embedding_dim]</span></span><br><span class="line">        <span class="keyword">for</span> layer <span class="keyword">in</span> self.layers:</span><br><span class="line">            dec_outputs, dec_self_attn, dec_enc_attn = layer(dec_outputs, enc_outputs, </span><br><span class="line">                                               dec_self_attn_mask, dec_enc_attn_mask)</span><br><span class="line">            dec_self_attns.append(dec_self_attn)</span><br><span class="line">            dec_enc_attns.append(dec_enc_attn)</span><br><span class="line">        <span class="comment">#------------------------- 维度信息 --------------------------------</span></span><br><span class="line">        <span class="comment"># dec_outputs 的维度是 [batch_size, target_len, embedding_dim]</span></span><br><span class="line">        <span class="comment"># dec_self_attns 是一个列表，每个元素的维度是 [batch_size, n_heads, target_len, target_len]</span></span><br><span class="line">        <span class="comment"># dec_enc_attns 是一个列表，每个元素的维度是 [batch_size, n_heads, target_len, source_len]</span></span><br><span class="line">        <span class="comment">#----------------------------------------------------------------- </span></span><br><span class="line">        <span class="comment"># 返回解码器输出，解码器自注意力和解码器 - 编码器注意力权重       </span></span><br><span class="line">        <span class="keyword">return</span> dec_outputs, dec_self_attns, dec_enc_attns</span><br></pre></td></tr></table></figure><p>1.<strong>词嵌入</strong>：输入目标语言的词索引，并结合位置编码来生成解码器的输入。</p><p>2.<strong>掩码计算</strong>：生成自注意力掩码和解码器-编码器掩码，确保模型不会使用未来信息或填充位置的信息。</p><p>3.<strong>多层解码器</strong>：通过多层解码器来处理输入，生成目标语言的最终表示。</p><p>4.<strong>返回结果</strong>：解码器的输出和每一层的注意力权重。</p><p>&nbsp;</p><h3 id="3-10-transfomer类"><a href="#3-10-transfomer类" class="headerlink" title="3.10 transfomer类"></a>3.10 transfomer类</h3><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 定义 Transformer 模型</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Transformer</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, corpus</span>):</span><br><span class="line">        <span class="built_in">super</span>(Transformer, self).__init__()        </span><br><span class="line">        self.encoder = Encoder(corpus) <span class="comment"># 初始化编码器实例        </span></span><br><span class="line">        self.decoder = Decoder(corpus) <span class="comment"># 初始化解码器实例</span></span><br><span class="line">        <span class="comment"># 定义线性投影层，将解码器输出转换为目标词汇表大小的概率分布</span></span><br><span class="line">        self.projection = nn.Linear(d_embedding, <span class="built_in">len</span>(corpus.tgt_vocab), bias=<span class="literal">False</span>)</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, enc_inputs, dec_inputs</span>):</span><br><span class="line">        <span class="comment">#------------------------- 维度信息 --------------------------------</span></span><br><span class="line">        <span class="comment"># enc_inputs 的维度是 [batch_size, source_seq_len]</span></span><br><span class="line">        <span class="comment"># dec_inputs 的维度是 [batch_size, target_seq_len]</span></span><br><span class="line">        <span class="comment">#-----------------------------------------------------------------        </span></span><br><span class="line">        <span class="comment"># 将输入传递给编码器，并获取编码器输出和自注意力权重        </span></span><br><span class="line">        enc_outputs, enc_self_attns = self.encoder(enc_inputs)</span><br><span class="line">        <span class="comment">#------------------------- 维度信息 --------------------------------</span></span><br><span class="line">        <span class="comment"># enc_outputs 的维度是 [batch_size, source_len, embedding_dim]</span></span><br><span class="line">        <span class="comment"># enc_self_attns 是一个列表，每个元素的维度是 [batch_size, n_heads, src_seq_len, src_seq_len]        </span></span><br><span class="line">        <span class="comment">#-----------------------------------------------------------------          </span></span><br><span class="line">        <span class="comment"># 将编码器输出、解码器输入和编码器输入传递给解码器</span></span><br><span class="line">        <span class="comment"># 获取解码器输出、解码器自注意力权重和编码器 - 解码器注意力权重     </span></span><br><span class="line">        dec_outputs, dec_self_attns, dec_enc_attns = self.decoder(dec_inputs, enc_inputs, enc_outputs)</span><br><span class="line">        <span class="comment">#------------------------- 维度信息 --------------------------------</span></span><br><span class="line">        <span class="comment"># dec_outputs 的维度是 [batch_size, target_len, embedding_dim]</span></span><br><span class="line">        <span class="comment"># dec_self_attns 是一个列表，每个元素的维度是 [batch_size, n_heads, tgt_seq_len, src_seq_len]</span></span><br><span class="line">        <span class="comment"># dec_enc_attns 是一个列表，每个元素的维度是 [batch_size, n_heads, tgt_seq_len, src_seq_len]   </span></span><br><span class="line">        <span class="comment">#-----------------------------------------------------------------                </span></span><br><span class="line">        <span class="comment"># 将解码器输出传递给投影层，生成目标词汇表大小的概率分布</span></span><br><span class="line">        dec_logits = self.projection(dec_outputs)  </span><br><span class="line">        <span class="comment">#------------------------- 维度信息 --------------------------------</span></span><br><span class="line">        <span class="comment"># dec_logits 的维度是 [batch_size, tgt_seq_len, tgt_vocab_size]</span></span><br><span class="line">        <span class="comment">#-----------------------------------------------------------------</span></span><br><span class="line">        <span class="comment"># 返回逻辑值 ( 原始预测结果 ), 编码器自注意力权重，解码器自注意力权重，解 - 编码器注意力权重</span></span><br><span class="line">        <span class="keyword">return</span> dec_logits, enc_self_attns, dec_self_attns, dec_enc_attns</span><br></pre></td></tr></table></figure><p>⾸先初始化编码器、解码器和投影层。在forward⽅法中，将源序列输⼊传递给编码器，获取编码器输出和⾃注意⼒权重。然后将编码器输出、解码器输⼊和编码器输⼊传递给解码器，获取解码器输出、解码器⾃注意⼒权重和编码器-解码器注意⼒权重。最后，将解码器输出传递给投影层，⽣成⽬标词汇表⼤⼩的概率分布。</p><p>这个概率分布将被⽤于计算损失和评估模型的性能。</p><p>&nbsp;</p><h2 id="四、举一个栗子跑跑"><a href="#四、举一个栗子跑跑" class="headerlink" title="四、举一个栗子跑跑"></a>四、举一个栗子跑跑</h2><h3 id="4-1-数据准备"><a href="#4-1-数据准备" class="headerlink" title="4.1 数据准备"></a>4.1 数据准备</h3><p>先准备几个中英翻译例句</p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">sentences = [</span><br><span class="line">    [<span class="string">&#x27;咖哥 喜欢 小冰&#x27;</span>, <span class="string">&#x27;KaGe likes XiaoBing&#x27;</span>],</span><br><span class="line">    [<span class="string">&#x27;我 爱 学习 人工智能&#x27;</span>, <span class="string">&#x27;I love studying AI&#x27;</span>],</span><br><span class="line">    [<span class="string">&#x27;深度学习 改变 世界&#x27;</span>, <span class="string">&#x27; DL changed the world&#x27;</span>],</span><br><span class="line">    [<span class="string">&#x27;自然语言处理 很 强大&#x27;</span>, <span class="string">&#x27;NLP is powerful&#x27;</span>],</span><br><span class="line">    [<span class="string">&#x27;神经网络 非常 复杂&#x27;</span>, <span class="string">&#x27;Neural-networks are complex&#x27;</span>] ]</span><br></pre></td></tr></table></figure><p>然后，创建TranslationCorpus类，⽤于读⼊中英翻译语料，并⽣成字典和模型可以读取的数据批次。</p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> collections <span class="keyword">import</span> Counter <span class="comment"># 导入 Counter 类</span></span><br><span class="line"><span class="comment"># 定义 TranslationCorpus 类</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">TranslationCorpus</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, sentences</span>):</span><br><span class="line">        self.sentences = sentences</span><br><span class="line">        <span class="comment"># 计算源语言和目标语言的最大句子长度，并分别加 1 和 2 以容纳填充符和特殊符号</span></span><br><span class="line">        self.src_len = <span class="built_in">max</span>(<span class="built_in">len</span>(sentence[<span class="number">0</span>].split()) <span class="keyword">for</span> sentence <span class="keyword">in</span> sentences) + <span class="number">1</span></span><br><span class="line">        self.tgt_len = <span class="built_in">max</span>(<span class="built_in">len</span>(sentence[<span class="number">1</span>].split()) <span class="keyword">for</span> sentence <span class="keyword">in</span> sentences) + <span class="number">2</span></span><br><span class="line">        <span class="comment"># 创建源语言和目标语言的词汇表</span></span><br><span class="line">        self.src_vocab, self.tgt_vocab = self.create_vocabularies()</span><br><span class="line">        <span class="comment"># 创建索引到单词的映射</span></span><br><span class="line">        self.src_idx2word = &#123;v: k <span class="keyword">for</span> k, v <span class="keyword">in</span> self.src_vocab.items()&#125;</span><br><span class="line">        self.tgt_idx2word = &#123;v: k <span class="keyword">for</span> k, v <span class="keyword">in</span> self.tgt_vocab.items()&#125;</span><br><span class="line">    <span class="comment"># 定义创建词汇表的函数</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">create_vocabularies</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="comment"># 统计源语言和目标语言的单词频率</span></span><br><span class="line">        src_counter = Counter(word <span class="keyword">for</span> sentence <span class="keyword">in</span> self.sentences <span class="keyword">for</span> word <span class="keyword">in</span> sentence[<span class="number">0</span>].split())</span><br><span class="line">        tgt_counter = Counter(word <span class="keyword">for</span> sentence <span class="keyword">in</span> self.sentences <span class="keyword">for</span> word <span class="keyword">in</span> sentence[<span class="number">1</span>].split())        </span><br><span class="line">        <span class="comment"># 创建源语言和目标语言的词汇表，并为每个单词分配一个唯一的索引</span></span><br><span class="line">        src_vocab = &#123;<span class="string">&#x27;&lt;pad&gt;&#x27;</span>: <span class="number">0</span>, **&#123;word: i+<span class="number">1</span> <span class="keyword">for</span> i, word <span class="keyword">in</span> <span class="built_in">enumerate</span>(src_counter)&#125;&#125;</span><br><span class="line">        tgt_vocab = &#123;<span class="string">&#x27;&lt;pad&gt;&#x27;</span>: <span class="number">0</span>, <span class="string">&#x27;&lt;sos&gt;&#x27;</span>: <span class="number">1</span>, <span class="string">&#x27;&lt;eos&gt;&#x27;</span>: <span class="number">2</span>, </span><br><span class="line">                     **&#123;word: i+<span class="number">3</span> <span class="keyword">for</span> i, word <span class="keyword">in</span> <span class="built_in">enumerate</span>(tgt_counter)&#125;&#125;        </span><br><span class="line">        <span class="keyword">return</span> src_vocab, tgt_vocab</span><br><span class="line">    <span class="comment"># 定义创建批次数据的函数</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">make_batch</span>(<span class="params">self, batch_size, test_batch=<span class="literal">False</span></span>):</span><br><span class="line">        input_batch, output_batch, target_batch = [], [], []</span><br><span class="line">        <span class="comment"># 随机选择句子索引</span></span><br><span class="line">        sentence_indices = torch.randperm(<span class="built_in">len</span>(self.sentences))[:batch_size]</span><br><span class="line">        <span class="keyword">for</span> index <span class="keyword">in</span> sentence_indices:</span><br><span class="line">            src_sentence, tgt_sentence = self.sentences[index]</span><br><span class="line">            <span class="comment"># 将源语言和目标语言的句子转换为索引序列</span></span><br><span class="line">            src_seq = [self.src_vocab[word] <span class="keyword">for</span> word <span class="keyword">in</span> src_sentence.split()]</span><br><span class="line">            tgt_seq = [self.tgt_vocab[<span class="string">&#x27;&lt;sos&gt;&#x27;</span>]] + [self.tgt_vocab[word] \</span><br><span class="line">                         <span class="keyword">for</span> word <span class="keyword">in</span> tgt_sentence.split()] + [self.tgt_vocab[<span class="string">&#x27;&lt;eos&gt;&#x27;</span>]]            </span><br><span class="line">            <span class="comment"># 对源语言和目标语言的序列进行填充</span></span><br><span class="line">            src_seq += [self.src_vocab[<span class="string">&#x27;&lt;pad&gt;&#x27;</span>]] * (self.src_len - <span class="built_in">len</span>(src_seq))</span><br><span class="line">            tgt_seq += [self.tgt_vocab[<span class="string">&#x27;&lt;pad&gt;&#x27;</span>]] * (self.tgt_len - <span class="built_in">len</span>(tgt_seq))            </span><br><span class="line">            <span class="comment"># 将处理好的序列添加到批次中</span></span><br><span class="line">            input_batch.append(src_seq)</span><br><span class="line">            output_batch.append([self.tgt_vocab[<span class="string">&#x27;&lt;sos&gt;&#x27;</span>]] + ([self.tgt_vocab[<span class="string">&#x27;&lt;pad&gt;&#x27;</span>]] * \</span><br><span class="line">                                    (self.tgt_len - <span class="number">2</span>)) <span class="keyword">if</span> test_batch <span class="keyword">else</span> tgt_seq[:-<span class="number">1</span>])</span><br><span class="line">            target_batch.append(tgt_seq[<span class="number">1</span>:])        </span><br><span class="line">          <span class="comment"># 将批次转换为 LongTensor 类型</span></span><br><span class="line">        input_batch = torch.LongTensor(input_batch)</span><br><span class="line">        output_batch = torch.LongTensor(output_batch)</span><br><span class="line">        target_batch = torch.LongTensor(target_batch)            </span><br><span class="line">        <span class="keyword">return</span> input_batch, output_batch, target_batch</span><br><span class="line"><span class="comment"># 创建语料库类实例</span></span><br><span class="line">corpus = TranslationCorpus(sentences)</span><br></pre></td></tr></table></figure><h3 id="4-2-训练Transfomer模型"><a href="#4-2-训练Transfomer模型" class="headerlink" title="4.2 训练Transfomer模型"></a>4.2 训练Transfomer模型</h3><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch <span class="comment"># 导入 torch</span></span><br><span class="line"><span class="keyword">import</span> torch.optim <span class="keyword">as</span> optim <span class="comment"># 导入优化器</span></span><br><span class="line">model = Transformer(corpus) <span class="comment"># 创建模型实例</span></span><br><span class="line">criterion = nn.CrossEntropyLoss() <span class="comment"># 损失函数</span></span><br><span class="line">optimizer = optim.Adam(model.parameters(), lr=<span class="number">0.0001</span>) <span class="comment"># 优化器</span></span><br><span class="line">epochs = <span class="number">5</span> <span class="comment"># 训练轮次</span></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(epochs): <span class="comment"># 训练 100 轮</span></span><br><span class="line">    optimizer.zero_grad() <span class="comment"># 梯度清零</span></span><br><span class="line">    enc_inputs, dec_inputs, target_batch = corpus.make_batch(batch_size) <span class="comment"># 创建训练数据        </span></span><br><span class="line">    outputs, _, _, _ = model(enc_inputs, dec_inputs) <span class="comment"># 获取模型输出 </span></span><br><span class="line">    loss = criterion(outputs.view(-<span class="number">1</span>, <span class="built_in">len</span>(corpus.tgt_vocab)), target_batch.view(-<span class="number">1</span>)) <span class="comment"># 计算损失</span></span><br><span class="line">    <span class="keyword">if</span> (epoch + <span class="number">1</span>) % <span class="number">1</span> == <span class="number">0</span>: <span class="comment"># 打印损失</span></span><br><span class="line">        <span class="built_in">print</span>(<span class="string">f&quot;Epoch: <span class="subst">&#123;epoch + <span class="number">1</span>:04d&#125;</span> cost = <span class="subst">&#123;loss:<span class="number">.6</span>f&#125;</span>&quot;</span>)</span><br><span class="line">    loss.backward()<span class="comment"># 反向传播        </span></span><br><span class="line">    optimizer.step()<span class="comment"># 更新参数</span></span><br></pre></td></tr></table></figure><p>训练100轮之后，损失会减⼩到⼀个较⼩的值。</p><p>&nbsp;</p><h3 id="4-3-测试Transfomer模型"><a href="#4-3-测试Transfomer模型" class="headerlink" title="4.3 测试Transfomer模型"></a>4.3 测试Transfomer模型</h3><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 创建一个大小为 1 的批次，目标语言序列 dec_inputs 在测试阶段，仅包含句子开始符号 &lt;sos&gt;</span></span><br><span class="line">enc_inputs, dec_inputs, target_batch = corpus.make_batch(batch_size=<span class="number">1</span>,test_batch=<span class="literal">True</span>) </span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;编码器输入 :&quot;</span>, enc_inputs) <span class="comment"># 打印编码器输入</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;解码器输入 :&quot;</span>, dec_inputs) <span class="comment"># 打印解码器输入</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;目标数据 :&quot;</span>, target_batch) <span class="comment"># 打印目标数据</span></span><br><span class="line">predict, enc_self_attns, dec_self_attns, dec_enc_attns = model(enc_inputs, dec_inputs) <span class="comment"># 用模型进行翻译</span></span><br><span class="line">predict = predict.view(-<span class="number">1</span>, <span class="built_in">len</span>(corpus.tgt_vocab)) <span class="comment"># 将预测结果维度重塑</span></span><br><span class="line">predict = predict.data.<span class="built_in">max</span>(<span class="number">1</span>, keepdim=<span class="literal">True</span>)[<span class="number">1</span>] <span class="comment"># 找到每个位置概率最大的词汇的索引</span></span><br><span class="line"><span class="comment"># 解码预测的输出，将所预测的目标句子中的索引转换为单词</span></span><br><span class="line">translated_sentence = [corpus.tgt_idx2word[idx.item()] <span class="keyword">for</span> idx <span class="keyword">in</span> predict.squeeze()]</span><br><span class="line"><span class="comment"># 将输入的源语言句子中的索引转换为单词</span></span><br><span class="line">input_sentence = <span class="string">&#x27; &#x27;</span>.join([corpus.src_idx2word[idx.item()] <span class="keyword">for</span> idx <span class="keyword">in</span> enc_inputs[<span class="number">0</span>]])</span><br><span class="line"><span class="built_in">print</span>(input_sentence, <span class="string">&#x27;-&gt;&#x27;</span>, translated_sentence) <span class="comment"># 打印原始句子和翻译后的句子</span></span><br></pre></td></tr></table></figure><figure class="highlight markdown"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">编码器输入 : tensor([[11, 12, 13,  0,  0]])</span><br><span class="line"></span><br><span class="line">解码器输入 : tensor([[1, 0, 0, 0, 0]])</span><br><span class="line"></span><br><span class="line">目标数据 : tensor([[14, 15, 16,  2,  0]])</span><br><span class="line"></span><br><span class="line">自然语言处理 很 强大 <span class="language-xml"><span class="tag">&lt;<span class="name">pad</span>&gt;</span></span> <span class="language-xml"><span class="tag">&lt;<span class="name">pad</span>&gt;</span></span> -&gt; [&#x27;NLP&#x27;, &#x27;NLP&#x27;, &#x27;NLP&#x27;, &#x27;NLP&#x27;, &#x27;NLP&#x27;]</span><br></pre></td></tr></table></figure><p>这个Transformer能训练，能⽤。不过，其输出结果并不理想，模型只成功翻译了⼀个单词“NLP”，之后就不断重复这个词。</p><p>对于这样简单的数据集，在设计和选择模型时，应该优先考虑简单的模型，像Transformer这样⽐较复杂的模型并不⼀定效果更好。</p><p>这次测试效果不理想的真正原因和模型的简单或者复杂⽆关，主要是因为此处我们并没有利⽤解码器的⾃回归机制进⾏逐位置（即逐词、逐令牌、逐元素或逐时间步）的⽣成式输出。</p>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;Transformer：GPT背后的”造脑工程”全解析（含手搓过程）&quot;&gt;&lt;a href=&quot;#Transformer：GPT背后的”造脑工程”全解析（含手搓过程）&quot; class=&quot;headerlink&quot; title=&quot;Transformer：GPT背后的”造脑工程”</summary>
      
    
    
    
    
  </entry>
  
  <entry>
    <title></title>
    <link href="https://rozen12123.github.io/posts/0.html"/>
    <id>https://rozen12123.github.io/posts/0.html</id>
    <published>2025-03-15T14:14:13.566Z</published>
    <updated>2025-03-15T14:42:23.339Z</updated>
    
    <content type="html"><![CDATA[<h1 id="注意力机制：让AI拥有”黄金七秒记忆”的魔法—（自注意力）"><a href="#注意力机制：让AI拥有”黄金七秒记忆”的魔法—（自注意力）" class="headerlink" title="注意力机制：让AI拥有”黄金七秒记忆”的魔法—（自注意力）"></a>注意力机制：让AI拥有”黄金七秒记忆”的魔法—（自注意力）</h1><p>⾃注意⼒就是⾃⼰对⾃⼰的注意，它允许模型在同⼀序列中的不同位置之间建⽴依赖关系。⽤我们刚才讲过的最简单的注意⼒来理解，如果我们把x2替换为x1⾃身，那么我们其实就实现了x1每⼀个位置对⾃身其他序列的所有位置的加权和。</p><p>如下：</p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line"><span class="comment"># 一个形状为 (batch_size, seq_len, feature_dim) 的张量 x</span></span><br><span class="line">x = torch.randn(<span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>)</span><br><span class="line"><span class="comment"># 计算原始权重，形状为 (batch_size, seq_len, seq_len)</span></span><br><span class="line">raw_weights = torch.bmm(x, x.transpose(<span class="number">1</span>, <span class="number">2</span>))</span><br><span class="line"><span class="comment"># 对原始权重进行 softmax 归一化，形状为 (batch_size, seq_len, seq_len)</span></span><br><span class="line">attn_weights = F.softmax(raw_weights, dim=<span class="number">2</span>)</span><br><span class="line"><span class="comment"># 计算加权和，形状为 (batch_size, seq_len, feature_dim) </span></span><br><span class="line">attn_outputs = torch.bmm(attn_weights, x)</span><br></pre></td></tr></table></figure><p>知道了这个，那么下面继续展示一下如何对输入序列进行不同的线性变换，得到Q，K和V向量，然后应用放缩点积注意力即可：</p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 一个形状为 (batch_size, seq_len, feature_dim) 的张量 x</span></span><br><span class="line">x = torch.randn(<span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>) <span class="comment"># 形状 (batch_size, seq_len, feature_dim)</span></span><br><span class="line"><span class="comment"># 定义线性层用于将 x 转换为 Q, K, V 向量</span></span><br><span class="line">linear_q = torch.nn.Linear(<span class="number">4</span>, <span class="number">4</span>)</span><br><span class="line">linear_k = torch.nn.Linear(<span class="number">4</span>, <span class="number">4</span>)</span><br><span class="line">linear_v = torch.nn.Linear(<span class="number">4</span>, <span class="number">4</span>)</span><br><span class="line"><span class="comment"># 通过线性层计算 Q, K, V</span></span><br><span class="line">Q = linear_q(x) <span class="comment"># 形状 (batch_size, seq_len, feature_dim)</span></span><br><span class="line">K = linear_k(x) <span class="comment"># 形状 (batch_size, seq_len, feature_dim)</span></span><br><span class="line">V = linear_v(x) <span class="comment"># 形状 (batch_size, seq_len, feature_dim)</span></span><br><span class="line"><span class="comment"># 计算 Q 和 K 的点积，作为相似度分数 , 也就是自注意力原始权重</span></span><br><span class="line">raw_weights = torch.bmm(Q, K.transpose(<span class="number">1</span>, <span class="number">2</span>)) <span class="comment"># 形状 (batch_size, seq_len, seq_len)</span></span><br><span class="line"><span class="comment"># 将自注意力原始权重进行缩放</span></span><br><span class="line">scale_factor = K.size(-<span class="number">1</span>) ** <span class="number">0.5</span>  <span class="comment"># 这里是 4 ** 0.5</span></span><br><span class="line">scaled_weights = raw_weights / scale_factor <span class="comment"># 形状 (batch_size, seq_len, seq_len)</span></span><br><span class="line"><span class="comment"># 对缩放后的权重进行 softmax 归一化，得到注意力权重</span></span><br><span class="line">attn_weights = F.softmax(scaled_weights, dim=<span class="number">2</span>) <span class="comment"># 形状 (batch_size, seq_len, seq_len)</span></span><br><span class="line"><span class="comment"># 将注意力权重应用于 V 向量，计算加权和，得到加权信息</span></span><br><span class="line">attn_outputs = torch.bmm(attn_weights, V) <span class="comment"># 形状 (batch_size, seq_len, feature_dim)</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot; 加权信息 :&quot;</span>, attn_outputs)</span><br></pre></td></tr></table></figure><figure class="highlight markdown"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">加权信息 : tensor([[[ 0.5676, -0.0132, -0.8214, -0.0548],</span><br><span class="line"></span><br><span class="line"><span class="code">        [ 0.5352, -0.1170, -0.5392, -0.0256],</span></span><br><span class="line"><span class="code"></span></span><br><span class="line"><span class="code">        [ 0.6141, -0.1343, -0.5587, -0.0331]],</span></span><br><span class="line"><span class="code"></span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="code">       [[ 0.5973, -0.2426, -0.3217, -0.0335],</span></span><br><span class="line"><span class="code"></span></span><br><span class="line"><span class="code">        [ 0.5996, -0.1914, -0.2840,  0.0152],</span></span><br><span class="line"><span class="code"></span></span><br><span class="line"><span class="code">        [ 0.6117, -0.2507, -0.3363, -0.0404]]], grad_fn=&lt;BmmBackward0&gt;)</span></span><br></pre></td></tr></table></figure><h2 id="1-多头自注意力"><a href="#1-多头自注意力" class="headerlink" title="1.多头自注意力"></a>1.多头自注意力</h2><p>多头⾃注意⼒（Multi-head Attention）机制是注意⼒机制的⼀种扩展，它可以帮助模型从不同的表示⼦空间捕获输⼊数据的多种特征。具体⽽⾔，多头⾃注意⼒在计算注意⼒权重和输出时，会对<strong>Q</strong>、<strong>K</strong>、<strong>V</strong>向量分别进⾏多次线性变换，从⽽获得不同的头（Head），并进⾏并⾏计算</p><p>如下图所示：</p><p><img src="https://rozen-picture.oss-cn-beijing.aliyuncs.com/img/202503152226156.png" alt="image-20250315222533884" style="zoom:50%;"></p><p>以下是多头⾃注意⼒的计算过程。</p><p>（1）初始化：设定多头⾃注意⼒的头数。每个头将处理输⼊数据的⼀个⼦空间。</p><p>（2）线性变换：对<strong>Q</strong>、<strong>K</strong>、<strong>V</strong>向量进⾏数次线性变换，每次变换使⽤不同的权重矩阵。这样，我们可以获得多组不同的<strong>Q</strong>、<strong>K</strong>、<strong>V</strong>向量。</p><p>（3）缩放点积注意⼒：将每组<strong>Q</strong>、<strong>K</strong>、<strong>V</strong>向量输⼊缩放点积注意⼒中进⾏计算，每个头将⽣成⼀个加权输出。</p><p>（4）合并：将所有头的加权输出拼接起来，并进⾏⼀次线性变换，得到多头⾃注意⼒的最终输出。</p><ul><li>假设我们有一个输入序列的表示矩阵 X（例如编码器的输出或者词嵌入），</li><li>我们通过三个不同的线性层（也就是不同的权重矩阵）分别计算 Query、Key 和 Value：<ul><li>$q=XW_q$</li><li>$k=XW_k$</li><li>$v= XW_v$</li></ul></li><li>这里，$W_q$、$W_k$ 和 $W_v$ 是模型在训练过程中学习到的参数矩阵。</li></ul><p>多头⾃注意⼒机制的优势在于，==通过同时学习多个⼦空间的特征==，可以==提⾼模型捕捉⻓距离依赖和不同语义层次的能⼒==。</p><p>如下列代码：</p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line"><span class="comment"># 一个形状为 (batch_size, seq_len, feature_dim) 的张量 x</span></span><br><span class="line">x = torch.randn(<span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>)  <span class="comment"># 形状 (batch_size, seq_len, feature_dim) </span></span><br><span class="line"><span class="comment"># 定义头数和每个头的维度</span></span><br><span class="line">num_heads = <span class="number">2</span></span><br><span class="line">head_dim = <span class="number">2</span></span><br><span class="line"><span class="comment"># feature_dim 必须是 num_heads * head_dim 的整数倍</span></span><br><span class="line"><span class="keyword">assert</span> x.size(-<span class="number">1</span>) == num_heads * head_dim</span><br><span class="line"><span class="comment"># 定义线性层用于将 x 转换为 Q, K, V 向量</span></span><br><span class="line">linear_q = torch.nn.Linear(<span class="number">4</span>, <span class="number">4</span>)</span><br><span class="line">linear_k = torch.nn.Linear(<span class="number">4</span>, <span class="number">4</span>)</span><br><span class="line">linear_v = torch.nn.Linear(<span class="number">4</span>, <span class="number">4</span>)</span><br><span class="line"><span class="comment"># 通过线性层计算 Q, K, V</span></span><br><span class="line">Q = linear_q(x)  <span class="comment"># 形状 (batch_size, seq_len, feature_dim) </span></span><br><span class="line">K = linear_k(x)  <span class="comment"># 形状 (batch_size, seq_len, feature_dim) </span></span><br><span class="line">V = linear_v(x)  <span class="comment"># 形状 (batch_size, seq_len, feature_dim) </span></span><br><span class="line"><span class="comment"># 将 Q, K, V 分割成 num_heads 个头</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">split_heads</span>(<span class="params">tensor, num_heads</span>):</span><br><span class="line">    batch_size, seq_len, feature_dim = tensor.size()</span><br><span class="line">    head_dim = feature_dim // num_heads</span><br><span class="line">    output = tensor.view(batch_size, seq_len, num_heads, head_dim).transpose(<span class="number">1</span>, <span class="number">2</span>)</span><br><span class="line">    <span class="keyword">return</span>  output <span class="comment"># 形状 (batch_size, num_heads, seq_len, feature_dim)</span></span><br><span class="line">Q = split_heads(Q, num_heads)  <span class="comment"># 形状 (batch_size, num_heads, seq_len, head_dim)</span></span><br><span class="line">K = split_heads(K, num_heads)  <span class="comment"># 形状 (batch_size, num_heads, seq_len, head_dim)</span></span><br><span class="line">V = split_heads(V, num_heads)  <span class="comment"># 形状 (batch_size, num_heads, seq_len, head_dim)</span></span><br><span class="line"><span class="comment"># 计算 Q 和 K 的点积，作为相似度分数 , 也就是自注意力原始权重</span></span><br><span class="line">raw_weights = torch.matmul(Q, K.transpose(-<span class="number">2</span>, -<span class="number">1</span>))  <span class="comment"># 形状 (batch_size, num_heads, seq_len, seq_len)</span></span><br><span class="line"><span class="comment"># 对自注意力原始权重进行缩放</span></span><br><span class="line">scale_factor = K.size(-<span class="number">1</span>) ** <span class="number">0.5</span></span><br><span class="line">scaled_weights = raw_weights / scale_factor  <span class="comment"># 形状 (batch_size, num_heads, seq_len, seq_len)</span></span><br><span class="line"><span class="comment"># 对缩放后的权重进行 softmax 归一化，得到注意力权重</span></span><br><span class="line">attn_weights = F.softmax(scaled_weights, dim=-<span class="number">1</span>)  <span class="comment"># 形状 (batch_size, num_heads, seq_len, seq_len)</span></span><br><span class="line"><span class="comment"># 将注意力权重应用于 V 向量，计算加权和，得到加权信息</span></span><br><span class="line">attn_outputs = torch.matmul(attn_weights, V)  <span class="comment"># 形状 (batch_size, num_heads, seq_len, head_dim)</span></span><br><span class="line"><span class="comment"># 将所有头的结果拼接起来</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">combine_heads</span>(<span class="params">tensor, num_heads</span>):</span><br><span class="line">    batch_size, num_heads, seq_len, head_dim = tensor.size()</span><br><span class="line">    feature_dim = num_heads * head_dim</span><br><span class="line">    output = tensor.transpose(<span class="number">1</span>, <span class="number">2</span>).contiguous().view(batch_size, seq_len, feature_dim)</span><br><span class="line">    <span class="keyword">return</span> output<span class="comment"># 形状 : (batch_size, seq_len, feature_dim)</span></span><br><span class="line">attn_outputs = combine_heads(attn_outputs, num_heads)  <span class="comment"># 形状 (batch_size, seq_len, feature_dim) </span></span><br><span class="line"><span class="comment"># 对拼接后的结果进行线性变换</span></span><br><span class="line">linear_out = torch.nn.Linear(<span class="number">4</span>, <span class="number">4</span>)</span><br><span class="line">attn_outputs = linear_out(attn_outputs)  <span class="comment"># 形状 (batch_size, seq_len, feature_dim) </span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot; 加权信息 :&quot;</span>, attn_outputs)</span><br></pre></td></tr></table></figure><p>多头自注意力机制就是将输⼊向量投影到多个向量空间，在每个向量空间中执⾏点积注意⼒计算，然后连接各头的结果。</p><p>在实际应⽤中，多头⾃注意⼒通常作为更复杂模型（如Transformer）的⼀个组成部分。这些复杂的模型通常包含其他组件，例如前馈神经⽹络（Feed-Forward Neural Network）和层归⼀化（Layer Normalization），以提⾼模型的表达能⼒和稳定性。</p><p>&nbsp;</p><h2 id="2-注意力掩码"><a href="#2-注意力掩码" class="headerlink" title="2.注意力掩码"></a>2.注意力掩码</h2><p>注意⼒中的掩码机制，不同于BERT训练过程中的那种对训练⽂本的“掩码”。注意⼒掩码的作⽤是避免模型在计算注意⼒分数时，将不相关的单词考虑进来。掩码操作可以防⽌模型学习到不必要的信息。</p><p>要直观地解释掩码，我们先回忆⼀下填充（Padding）的概念。在NLP任务中，我们经常需要将不同⻓度的⽂本输⼊模型。为了能够批量处理这些⽂本，我们需要将它们填充⾄相同的⻓度。</p><p>以这段有关损失函数的代码为例。</p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">criterion = nn.CrossEntropyLoss(ignore_index=word2idx_en[<span class="string">&#x27;&#x27;</span>]) <span class="comment"># 损失函数</span></span><br></pre></td></tr></table></figure><p>这段代码中的ignore_index=word2idx_en[‘’]，就是为了告诉模型，是附加的冗余信息，模型在反向传播更新参数的时候没有必要关注它，因此也没有什么单词会被翻译成。</p><p>&nbsp;</p><p>填充掩码（Padding Mask）的作⽤和上⾯损失函数中的ignore_index参数有点类似，都是避免在计算注意⼒分数时，将填充位置的单词考虑进来（⻅右图）。因为填充位置的单词对于实际任务来说是⽆意义的，⽽且可能会引⼊噪声，影响模型的性能。</p><p><img src="https://rozen-picture.oss-cn-beijing.aliyuncs.com/img/202503152239578.png" alt="image-20250315223900477"></p><p>加⼊了掩码机制之后的注意⼒如下图所示，我们会把==将注意⼒权重矩阵与⼀个注意⼒掩码矩阵相加==，使得不需要的信息所对应的权重变得⾮常⼩（接近负⽆穷）。然后，通过应⽤softmax函数，将不需要的信息对应的权重变得接近于0，从⽽实现忽略它们的⽬的。</p><p><img src="https://rozen-picture.oss-cn-beijing.aliyuncs.com/img/202503152239187.png" alt="image-20250315223935095"></p><p>在Transformer中，使⽤了⾃注意⼒机制、多头⾃注意⼒机制和掩码，不仅有前⾯介绍的填充掩码，还有⼀种解码器专⽤的==后续注意⼒掩码==（Subsequent Attention Mask），简称后续掩码，也叫前瞻掩码（Look-ahead Masking），这是为了在训练时为解码器遮蔽未来的信息。</p>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;注意力机制：让AI拥有”黄金七秒记忆”的魔法—（自注意力）&quot;&gt;&lt;a href=&quot;#注意力机制：让AI拥有”黄金七秒记忆”的魔法—（自注意力）&quot; class=&quot;headerlink&quot; title=&quot;注意力机制：让AI拥有”黄金七秒记忆”的魔法—（自注意力）&quot;&gt;&lt;/a</summary>
      
    
    
    
    
  </entry>
  
  <entry>
    <title></title>
    <link href="https://rozen12123.github.io/posts/0.html"/>
    <id>https://rozen12123.github.io/posts/0.html</id>
    <published>2025-03-15T14:01:14.244Z</published>
    <updated>2025-03-15T14:11:56.551Z</updated>
    
    <content type="html"><![CDATA[<h1 id="注意力机制：让AI拥有”黄金七秒记忆”的魔法—（注意力机制中的Q、K、V）"><a href="#注意力机制：让AI拥有”黄金七秒记忆”的魔法—（注意力机制中的Q、K、V）" class="headerlink" title="注意力机制：让AI拥有”黄金七秒记忆”的魔法—（注意力机制中的Q、K、V）"></a>注意力机制：让AI拥有”黄金七秒记忆”的魔法—（注意力机制中的Q、K、V）</h1><p>在注意⼒机制中，查询（Query）、键（Key）和值（Value）是三个关键部分。</p><p>■ 查询（Query）：是指当前需要处理的信息。模型根据查询向量在输⼊序列中查找相关信息。</p><p>■ 键（Key）：是指来⾃输⼊序列的⼀组表示。它们⽤于根据查询向量计算注意⼒权重。注意⼒权重反映了不同位置的输⼊数据与查询的相关性。</p><p>■ 值（Value）：是指来⾃输⼊序列的⼀组表示。它们⽤于根据注意⼒权重计算加权和，得到最终的注意⼒输出向量，其包含了与查询最相关的输⼊信息。</p><p>用下面栗子打一个比方：</p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch <span class="comment"># 导入 torch</span></span><br><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F <span class="comment"># 导入 nn.functional</span></span><br><span class="line"><span class="comment"># 1. 创建两个张量 x1 和 x2</span></span><br><span class="line">x1 = torch.randn(<span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>) <span class="comment"># 形状 (batch_size, seq_len1, feature_dim)</span></span><br><span class="line">x2 = torch.randn(<span class="number">2</span>, <span class="number">5</span>, <span class="number">4</span>) <span class="comment"># 形状 (batch_size, seq_len2, feature_dim)</span></span><br><span class="line"><span class="comment"># 2. 计算原始权重</span></span><br><span class="line">raw_weights = torch.bmm(x1, x2.transpose(<span class="number">1</span>, <span class="number">2</span>)) <span class="comment"># 形状 (batch_size, seq_len1, seq_len2)</span></span><br><span class="line"><span class="comment"># 3. 用 softmax 函数对原始权重进行归一化</span></span><br><span class="line">attn_weights = F.softmax(raw_weights, dim=<span class="number">2</span>) <span class="comment"># 形状 (batch_size, seq_len1, seq_len2)</span></span><br><span class="line"><span class="comment"># 4. 将注意力权重与 x2 相乘，计算加权和</span></span><br><span class="line">attn_output = torch.bmm(attn_weights, x2)  <span class="comment"># 形状 (batch_size, seq_len1, feature_dim)</span></span><br></pre></td></tr></table></figure><p>我们可以将x1视为查询（Query，<strong>Q</strong>）向量，将x2视为键（Key，<strong>K</strong>）和值（Value，<strong>V</strong>）向量。这是因为我们直接使⽤x1和x2的点积作为相似度得分，并将权重应⽤于x2本身来计算加权信息。所以，在这个简化示例中，<strong>Q</strong>对应于x1，<strong>K</strong>和<strong>V</strong>都对应于x2。</p><p>然⽽，在Transformer中，<strong>Q</strong>、<strong>K</strong>和<strong>V</strong>通常是从相同的输⼊序列经过不同的线性变换得到的不同向量。</p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line"><span class="comment">#1. 创建 Query、Key 和 Value 张量</span></span><br><span class="line">q = torch.randn(<span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>) <span class="comment"># 形状 (batch_size, seq_len1, feature_dim)</span></span><br><span class="line">k = torch.randn(<span class="number">2</span>, <span class="number">4</span>, <span class="number">4</span>) <span class="comment"># 形状 (batch_size, seq_len2, feature_dim)</span></span><br><span class="line">v = torch.randn(<span class="number">2</span>, <span class="number">4</span>, <span class="number">4</span>) <span class="comment"># 形状 (batch_size, seq_len2, feature_dim)</span></span><br><span class="line"><span class="comment"># 2. 计算点积，得到原始权重，形状为 (batch_size, seq_len1, seq_len2)</span></span><br><span class="line">raw_weights = torch.bmm(q, k.transpose(<span class="number">1</span>, <span class="number">2</span>))</span><br><span class="line"><span class="comment"># 3. 将原始权重进行缩放（可选），形状仍为 (batch_size, seq_len1, seq_len2)</span></span><br><span class="line">scaling_factor = q.size(-<span class="number">1</span>) ** <span class="number">0.5</span></span><br><span class="line">scaled_weights = raw_weights / scaling_factor</span><br><span class="line"><span class="comment"># 4. 应用 softmax 函数，使结果的值在 0 和 1 之间，且每一行的和为 1</span></span><br><span class="line">attn_weights = F.softmax(scaled_weights, dim=-<span class="number">1</span>) <span class="comment"># 形状仍为 (batch_size, seq_len1, seq_len2)</span></span><br><span class="line"><span class="comment"># 5. 与 Value 相乘，得到注意力分布的加权和 , 形状为 (batch_size, seq_len1, feature_dim)</span></span><br><span class="line">attn_output = torch.bmm(attn_weights, v)</span><br></pre></td></tr></table></figure><p><strong>K</strong>和<strong>V</strong>的维度是否需完全相同呢？</p><p>在缩放点积注意⼒中，<strong>K</strong>和<strong>V</strong>向量的维度不⼀定需要完全相同。在这种注意⼒机制中，<strong>K</strong>和<strong>V</strong>的序列⻓度维度（在这⾥是第2维）应该相同，因为它们描述了同⼀个序列的不同部分。然⽽，它们的特征（或隐藏层）维度（在这⾥是第3维）可以不同。<strong>V</strong>向量的第⼆个维度则决定了最终输出张量的特征维度，这个维度可以根据具体任务和模型设计进⾏调整。</p><p>⽽<strong>K</strong>向量的序列⻓度维度（在这⾥是第2维）和<strong>Q</strong>向量的序列⻓度维度可以不同，因为它们可以来⾃不同的输⼊序列，但是，<strong>K</strong>向量的特征维度（在这⾥是第3维）需要与<strong>Q</strong>向量的特征维度相同，因为它们之间要计算点积。</p><p>在实践中，<strong>K</strong>和<strong>V</strong>的各个维度通常是相同的，因为它们通常来⾃同⼀个输⼊序列并经过不同的线性变换。</p><p>在注意力机制中，<strong>k</strong>（Key）和 <strong>v</strong>（Value）的初始值并不是随机产生的，而是由输入数据经过各自的线性变换得到的。具体来说：</p><p>来源相同但变换不同：</p><ul><li>假设我们有一个输入序列的表示矩阵 X（例如编码器的输出或者词嵌入），</li><li>我们通过三个不同的线性层（也就是不同的权重矩阵）分别计算 Query、Key 和 Value：<ul><li>$q=XW_q$</li><li>$k=XW_k$</li><li>$v= XW_v$</li></ul></li><li>这里，$W_q$、$W_k$ 和 $W_v$ 是模型在训练过程中学习到的参数矩阵。</li></ul><ul><li><strong>确定方式</strong>：<br> 这些矩阵 $W_k$ 和 $W_v$ 在模型设计时就被定义好，并在训练过程中通过反向传播进行更新。</li><li>作用不同<ul><li><strong>Key (k)</strong>：通过 $W_k$ 得到，用来与 Query 进行匹配，计算注意力分数，决定输入中哪些部分对当前 Query 最重要。</li><li><strong>Value (v)</strong>：通过 $W_v$得到，它携带的是具体的信息内容，最终会根据注意力分数被加权求和，形成输出的上下文向量。</li></ul></li></ul><ul><li>k 与 v 的初始值都源自相同的输入 X，但它们经过了各自独立的线性变换，参数 $W_k$ 和 $W_v$ 决定了它们具体的数值和表示。</li><li>这两个过程是在训练过程中自动学习并调整的，确保模型能够有效地捕捉和利用输入信息。</li></ul><p>这样，通过学习到的权重矩阵，模型可以从输入中抽取出适合进行匹配（Key）和传递信息（Value）的表示。</p><p>现在，重写缩放点积注意⼒的计算过程，如下所述。</p><p>（1）计算<strong>Q</strong>向量和<strong>K</strong>向量的点积。</p><p>（2）将点积结果除以缩放因⼦（<strong>Q</strong>向量特征维度的平⽅根）。</p><p>（3）应⽤softmax函数得到注意⼒权重。</p><p>（4）使⽤注意⼒权重对<strong>V</strong>向量进⾏加权求和。</p><p>这个过程的图示如下⻚图所示:</p><p><img src="https://rozen-picture.oss-cn-beijing.aliyuncs.com/img/202503152205095.png" alt="image-20250315220205401"></p><p>具体到编码器-解码器注意⼒来说，可以这样理解<strong>Q</strong>、<strong>K</strong>、<strong>V</strong>向量。</p><p>■ <strong>Q</strong>向量代表了解码器在当前时间步的表示，⽤于和<strong>K</strong>向量进⾏匹配，以==计算注意⼒权重==。<strong>Q</strong>向量通常是==解码器隐藏状态的线性变换==。</p><p>■ <strong>K</strong>向量是编码器输出的⼀种表示，⽤于和<strong>Q</strong>向量进⾏匹配，以确定哪些编码器输出对于当前解码器时间步来说==最相关==。<strong>K</strong>向量通常是==编码器隐藏状态的线性变换==。</p><p>■ <strong>V</strong>向量是编码器输出的另⼀种表示，⽤于计算加权求和，⽣成注意⼒上下⽂向量。注意⼒权重会作⽤在<strong>V</strong>向量上，以便在解码过程中关注输⼊序列中的特定部分。<strong>V</strong>向量通常也是==编码器隐藏状态的线性变换==。</p><p>在刚才的编码器-解码器注意⼒示例中，直接使⽤了编码器隐藏状态和解码器隐藏状态来计算注意⼒。这⾥的<strong>Q</strong>、<strong>K</strong>和<strong>V</strong>向量并没有显式地表示出来（⽽且，此处<strong>K</strong>和<strong>V</strong>是同⼀个向量），但它们的概念仍然隐含在实现中：</p><p>■ 编码器隐藏状态（encoder_hidden_states）充当了<strong>K</strong>和<strong>V</strong>向量的⻆⾊。</p><p>■ 解码器隐藏状态（decoder_hidden_states）充当了<strong>Q</strong>向量的⻆⾊。</p><p>我们计算<strong>Q</strong>向量（解码器隐藏状态）与<strong>K</strong>向量（编码器隐藏状态）之间的点积来得到注意⼒权重，然后⽤这些权重对<strong>V</strong>向量（编码器隐藏状态）进⾏加权求和，得到上下⽂向量。</p><p>当然了，在⼀些更复杂的注意⼒机制（如Transformer中的多头⾃注意⼒机制）中，<strong>Q</strong>、<strong>K</strong>、<strong>V</strong>向量通常会更明确地表示出来，因为我们需要通过使⽤不同的线性层将相同的输⼊序列显式地映射到不同的<strong>Q</strong>、<strong>K</strong>、<strong>V</strong>向量空间。</p><p><strong>V</strong>向量表示值，⽤于计算加权信息。通过将注意⼒权重应⽤于<strong>V</strong>向量，我们可以获取输⼊序列中与<strong>Q</strong>向量相关的信息。它们（<strong>Q</strong>、<strong>K</strong>和<strong>V</strong>）其实都是输⼊序列，有时是编码器输⼊序列，有时是解码器输⼊序列，有时是神经⽹络中的隐藏状态（也来⾃输⼊序列）的线性表示，也都是序列的“嵌⼊向量”。</p>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;注意力机制：让AI拥有”黄金七秒记忆”的魔法—（注意力机制中的Q、K、V）&quot;&gt;&lt;a href=&quot;#注意力机制：让AI拥有”黄金七秒记忆”的魔法—（注意力机制中的Q、K、V）&quot; class=&quot;headerlink&quot; title=&quot;注意力机制：让AI拥有”黄金七秒记忆</summary>
      
    
    
    
    
  </entry>
  
  <entry>
    <title></title>
    <link href="https://rozen12123.github.io/posts/0.html"/>
    <id>https://rozen12123.github.io/posts/0.html</id>
    <published>2025-03-15T11:00:50.825Z</published>
    <updated>2025-03-15T11:00:50.826Z</updated>
    
    <content type="html"><![CDATA[<h1 id="注意力机制：让AI拥有”黄金七秒记忆”的魔法—（缩放点积注意力）"><a href="#注意力机制：让AI拥有”黄金七秒记忆”的魔法—（缩放点积注意力）" class="headerlink" title="注意力机制：让AI拥有”黄金七秒记忆”的魔法—（缩放点积注意力）"></a>注意力机制：让AI拥有”黄金七秒记忆”的魔法—（缩放点积注意力）</h1><h2 id="一、-缩放点积注意力"><a href="#一、-缩放点积注意力" class="headerlink" title="一、 缩放点积注意力"></a>一、 缩放点积注意力</h2><p>缩放点积注意⼒（<code>Scaled Dot-Product Attention</code>）和点积注意⼒（<code>Dot-Product Attention</code>）之间的主要区别在于：缩放点积注意⼒在计算注意⼒权重之前，会将点积结果也就是原始权重除以⼀个缩放因⼦，得到缩放后的原始权重。通常，这个缩放因⼦是输⼊特征维度的平⽅根。</p><p>为什么要使⽤缩放因⼦呢？在深度学习模型中，点积的值可能会变得⾮常⼤，尤其是当特征维度较⼤时。当点积值特别⼤时，<code>softmax</code>函数可能会在⼀个⾮常陡峭的区域内运⾏，导致梯度变得⾮常⼩，也可能会导致训练过程中梯度消失。通过使⽤缩放因⼦，可以确保<code>softmax</code>函数在⼀个较为平缓的区域内⼯作，从⽽减轻梯度消失问题，提⾼模型的稳定性。</p><p><img src="https://rozen-picture.oss-cn-beijing.aliyuncs.com/img/202503151721031.png" alt="image-20250315172146857"></p><p>如上图，相比于点积注意力，多了第三步：将原始权重（得分）除以缩放因子。</p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch <span class="comment"># 导入 torch</span></span><br><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F <span class="comment"># 导入 nn.functional</span></span><br><span class="line"><span class="comment"># 1. 创建两个张量 x1 和 x2</span></span><br><span class="line">x1 = torch.randn(<span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>) <span class="comment"># 形状 (batch_size, seq_len1, feature_dim)</span></span><br><span class="line">x2 = torch.randn(<span class="number">2</span>, <span class="number">5</span>, <span class="number">4</span>) <span class="comment"># 形状 (batch_size, seq_len2, feature_dim)</span></span><br><span class="line"><span class="comment"># 2. 计算张量点积，得到原始权重</span></span><br><span class="line">raw_weights = torch.bmm(x1, x2.transpose(<span class="number">1</span>, <span class="number">2</span>)) <span class="comment"># 形状 (batch_size, seq_len1, seq_len2)</span></span><br><span class="line"><span class="comment"># 3. 将原始权重除以缩放因子</span></span><br><span class="line">scaling_factor = x1.size(-<span class="number">1</span>) ** <span class="number">0.5</span></span><br><span class="line">scaled_weights = raw_weights  / scaling_factor <span class="comment"># 形状 (batch_size, seq_len1, seq_len2)</span></span><br><span class="line"><span class="comment"># 4. 对原始权重进行归一化</span></span><br><span class="line">attn_weights  =  F.softmax(raw_weights, dim=<span class="number">2</span>) <span class="comment">#  形 状 (batch_size,  seq_len1,  seq_len2)</span></span><br><span class="line"><span class="comment"># 5. 使用注意力权重对 x2 加权求和</span></span><br><span class="line">attn_output = torch.bmm(attn_weights, x2)  <span class="comment"># 形状 (batch_size, seq_len1, feature_dim)</span></span><br></pre></td></tr></table></figure><h2 id="二、编码器-解码器注意力"><a href="#二、编码器-解码器注意力" class="headerlink" title="二、编码器-解码器注意力"></a>二、编码器-解码器注意力</h2><p>刚才，为了简化讲解的难度，也为了让你把全部注意⼒放在注意⼒机制本身上⾯。我们并没有说明，x1、x2在实际应⽤中分别代表着什么。</p><p>当我们应⽤点积注意⼒时，解码器的每个时间步都会根据编码器的隐藏状态计算⼀个注意⼒权重，然后将这些权重应⽤于编码器隐藏状态，以⽣成⼀个上下⽂向量（编码器-解码器注意⼒的输出）。这个上下⽂向量将包含关于编码器输⼊序列的有⽤信息，解码器可以利⽤这个信息⽣成更准确的输出序列，如下图所示。</p><p><img src="https://rozen-picture.oss-cn-beijing.aliyuncs.com/img/202503151731186.png" alt="image-20250315173111077"></p><p>要在这个程序中加⼊编码器-解码器注意⼒机制，我们可以按照以下步骤进⾏修改。</p><p>（1）定义Attention类。⽤于计算注意⼒权重和注意⼒上下⽂向量。</p><p>（2）重构Decoder类。更新Decoder类的初始化部分和前向传播⽅法，使其包含注意⼒层并在解码过程中利⽤注意⼒权重。</p><p>（3）重构Seq2Seq类。更新Seq2Seq类的前向传播⽅法，以便将编码器的输出传递给解码器。</p><p>（4）可视化注意⼒权重。</p><h3 id="2-1-构建语料库"><a href="#2-1-构建语料库" class="headerlink" title="2.1 构建语料库"></a>2.1 构建语料库</h3><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 构建语料库，每行包含中文、英文（解码器输入）和翻译成英文后的目标输出 3 个句子</span></span><br><span class="line">sentences = [</span><br><span class="line">    [<span class="string">&#x27;咖哥 喜欢 小冰&#x27;</span>, <span class="string">&#x27;&lt;sos&gt; KaGe likes XiaoBing&#x27;</span>, <span class="string">&#x27;KaGe likes XiaoBing &lt;eos&gt;&#x27;</span>],</span><br><span class="line">    [<span class="string">&#x27;我 爱 学习 人工智能&#x27;</span>, <span class="string">&#x27;&lt;sos&gt; I love studying AI&#x27;</span>, <span class="string">&#x27;I love studying AI &lt;eos&gt;&#x27;</span>],</span><br><span class="line">    [<span class="string">&#x27;深度学习 改变 世界&#x27;</span>, <span class="string">&#x27;&lt;sos&gt; DL changed the world&#x27;</span>, <span class="string">&#x27;DL changed the world &lt;eos&gt;&#x27;</span>],</span><br><span class="line">    [<span class="string">&#x27;自然 语言 处理 很 强大&#x27;</span>, <span class="string">&#x27;&lt;sos&gt; NLP is so powerful&#x27;</span>, <span class="string">&#x27;NLP is so powerful &lt;eos&gt;&#x27;</span>],</span><br><span class="line">    [<span class="string">&#x27;神经网络 非常 复杂&#x27;</span>, <span class="string">&#x27;&lt;sos&gt; Neural-Nets are complex&#x27;</span>, <span class="string">&#x27;Neural-Nets are complex &lt;eos&gt;&#x27;</span>]]</span><br><span class="line">word_list_cn, word_list_en = [], []  <span class="comment"># 初始化中英文词汇表</span></span><br><span class="line"><span class="comment"># 遍历每一个句子并将单词添加到词汇表中</span></span><br><span class="line"><span class="keyword">for</span> s <span class="keyword">in</span> sentences:</span><br><span class="line">    word_list_cn.extend(s[<span class="number">0</span>].split())</span><br><span class="line">    word_list_en.extend(s[<span class="number">1</span>].split())</span><br><span class="line">    word_list_en.extend(s[<span class="number">2</span>].split())</span><br><span class="line"><span class="comment"># 去重，得到没有重复单词的词汇表</span></span><br><span class="line">word_list_cn = <span class="built_in">list</span>(<span class="built_in">set</span>(word_list_cn))</span><br><span class="line">word_list_en = <span class="built_in">list</span>(<span class="built_in">set</span>(word_list_en))</span><br><span class="line"><span class="comment"># 构建单词到索引的映射</span></span><br><span class="line">word2idx_cn = &#123;w: i <span class="keyword">for</span> i, w <span class="keyword">in</span> <span class="built_in">enumerate</span>(word_list_cn)&#125;</span><br><span class="line">word2idx_en = &#123;w: i <span class="keyword">for</span> i, w <span class="keyword">in</span> <span class="built_in">enumerate</span>(word_list_en)&#125;</span><br><span class="line"><span class="comment"># 构建索引到单词的映射</span></span><br><span class="line">idx2word_cn = &#123;i: w <span class="keyword">for</span> i, w <span class="keyword">in</span> <span class="built_in">enumerate</span>(word_list_cn)&#125;</span><br><span class="line">idx2word_en = &#123;i: w <span class="keyword">for</span> i, w <span class="keyword">in</span> <span class="built_in">enumerate</span>(word_list_en)&#125;</span><br><span class="line"><span class="comment"># 计算词汇表的大小</span></span><br><span class="line">voc_size_cn = <span class="built_in">len</span>(word_list_cn)</span><br><span class="line">voc_size_en = <span class="built_in">len</span>(word_list_en)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot; 句子数量：&quot;</span>, <span class="built_in">len</span>(sentences)) <span class="comment"># 打印句子数</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot; 中文词汇表大小：&quot;</span>, voc_size_cn) <span class="comment"># 打印中文词汇表大小</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot; 英文词汇表大小：&quot;</span>, voc_size_en) <span class="comment"># 打印英文词汇表大小</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot; 中文词汇到索引的字典：&quot;</span>, word2idx_cn) <span class="comment"># 打印中文词汇到索引的字典</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot; 英文词汇到索引的字典：&quot;</span>, word2idx_en) <span class="comment"># 打印英文词汇到索引的字典</span></span><br></pre></td></tr></table></figure><h3 id="2-2-设定编码器和解码器设定的张量"><a href="#2-2-设定编码器和解码器设定的张量" class="headerlink" title="2.2 设定编码器和解码器设定的张量"></a>2.2 设定编码器和解码器设定的张量</h3><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np <span class="comment"># 导入 numpy</span></span><br><span class="line"><span class="keyword">import</span> torch <span class="comment"># 导入 torch</span></span><br><span class="line"><span class="keyword">import</span> random <span class="comment"># 导入 random 库</span></span><br><span class="line"><span class="comment"># 定义一个函数，随机选择一个句子和词汇表生成输入、输出和目标数据</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">make_data</span>(<span class="params">sentences</span>):</span><br><span class="line">    <span class="comment"># 随机选择一个句子进行训练</span></span><br><span class="line">    random_sentence = random.choice(sentences)</span><br><span class="line">    <span class="comment"># 将输入句子中的单词转换为对应的索引</span></span><br><span class="line">    encoder_input = np.array([[word2idx_cn[n] <span class="keyword">for</span> n <span class="keyword">in</span> random_sentence[<span class="number">0</span>].split()]])</span><br><span class="line">    <span class="comment"># 将输出句子中的单词转换为对应的索引</span></span><br><span class="line">    decoder_input = np.array([[word2idx_en[n] <span class="keyword">for</span> n <span class="keyword">in</span> random_sentence[<span class="number">1</span>].split()]])</span><br><span class="line">    <span class="comment"># 将目标句子中的单词转换为对应的索引</span></span><br><span class="line">    target = np.array([[word2idx_en[n] <span class="keyword">for</span> n <span class="keyword">in</span> random_sentence[<span class="number">2</span>].split()]])</span><br><span class="line">    <span class="comment"># 将输入、输出和目标批次转换为 LongTensor</span></span><br><span class="line">    encoder_input = torch.LongTensor(encoder_input)</span><br><span class="line">    decoder_input = torch.LongTensor(decoder_input)</span><br><span class="line">    target = torch.LongTensor(target)</span><br><span class="line">    <span class="keyword">return</span> encoder_input, decoder_input, target </span><br><span class="line"><span class="comment"># 使用 make_data 函数生成输入、输出和目标张量</span></span><br><span class="line">encoder_input, decoder_input, target = make_data(sentences)</span><br><span class="line"><span class="keyword">for</span> s <span class="keyword">in</span> sentences: <span class="comment"># 获取原始句子</span></span><br><span class="line">    <span class="keyword">if</span> <span class="built_in">all</span>([word2idx_cn[w] <span class="keyword">in</span> encoder_input[<span class="number">0</span>] <span class="keyword">for</span> w <span class="keyword">in</span> s[<span class="number">0</span>].split()]):</span><br><span class="line">        original_sentence = s</span><br><span class="line">        <span class="keyword">break</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot; 原始句子：&quot;</span>, original_sentence) <span class="comment"># 打印原始句子</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot; 编码器输入张量的形状：&quot;</span>, encoder_input.shape)  <span class="comment"># 打印输入张量形状</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot; 解码器输入张量的形状：&quot;</span>, decoder_input.shape) <span class="comment"># 打印输出张量形状</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot; 目标张量的形状：&quot;</span>, target.shape) <span class="comment"># 打印目标张量形状</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot; 编码器输入张量：&quot;</span>, encoder_input) <span class="comment"># 打印输入张量</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot; 解码器输入张量：&quot;</span>, decoder_input) <span class="comment"># 打印输出张量</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot; 目标张量：&quot;</span>, target) <span class="comment"># 打印目标张量</span></span><br></pre></td></tr></table></figure><h3 id="2-3-定义attention类"><a href="#2-3-定义attention类" class="headerlink" title="2.3 定义attention类"></a>2.3 定义attention类</h3><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 定义 Attention 类</span></span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn <span class="comment"># 导入 torch.nn 库</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Attention</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="built_in">super</span>(Attention, self).__init__()</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, decoder_context, encoder_context</span>):</span><br><span class="line">        <span class="comment"># 计算 decoder_context 和 encoder_context 的点积，得到注意力分数</span></span><br><span class="line">        scores = torch.matmul(decoder_context, encoder_context.transpose(-<span class="number">2</span>, -<span class="number">1</span>))</span><br><span class="line">        <span class="comment"># 归一化分数</span></span><br><span class="line">        attn_weights = nn.functional.softmax(scores, dim=-<span class="number">1</span>)</span><br><span class="line">        <span class="comment"># 将注意力权重乘以 encoder_context，得到加权的上下文向量</span></span><br><span class="line">        context = torch.matmul(attn_weights, encoder_context)</span><br><span class="line">        <span class="keyword">return</span> context, attn_weights</span><br></pre></td></tr></table></figure><p>点积解码器和译码器内容得到注意力分数后进行归一化得到权重，之后将权重乘解码器的内容以得到加权上下文的向量</p><h3 id="2-4-重构Decoder类"><a href="#2-4-重构Decoder类" class="headerlink" title="2.4 重构Decoder类"></a>2.4 重构Decoder类</h3><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn <span class="comment"># 导入 torch.nn 库</span></span><br><span class="line"><span class="comment"># 定义编码器类</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Encoder</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, input_size, hidden_size</span>):</span><br><span class="line">        <span class="built_in">super</span>(Encoder, self).__init__()       </span><br><span class="line">        self.hidden_size = hidden_size <span class="comment"># 设置隐藏层大小       </span></span><br><span class="line">        self.embedding = nn.Embedding(input_size, hidden_size) <span class="comment"># 创建词嵌入层       </span></span><br><span class="line">        self.rnn = nn.RNN(hidden_size, hidden_size, batch_first=<span class="literal">True</span>) <span class="comment"># 创建 RNN 层    </span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, inputs, hidden</span>): <span class="comment"># 前向传播函数</span></span><br><span class="line">        embedded = self.embedding(inputs) <span class="comment"># 将输入转换为嵌入向量       </span></span><br><span class="line">        output, hidden = self.rnn(embedded, hidden) <span class="comment"># 将嵌入向量输入 RNN 层并获取输出</span></span><br><span class="line">        <span class="keyword">return</span> output, hidden</span><br><span class="line"><span class="comment"># 定义解码器类</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">DecoderWithAttention</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, hidden_size, output_size</span>):</span><br><span class="line">        <span class="built_in">super</span>(DecoderWithAttention, self).__init__()</span><br><span class="line">        self.hidden_size = hidden_size <span class="comment"># 设置隐藏层大小</span></span><br><span class="line">        self.embedding = nn.Embedding(output_size, hidden_size) <span class="comment"># 创建词嵌入层</span></span><br><span class="line">        self.rnn = nn.RNN(hidden_size, hidden_size, batch_first=<span class="literal">True</span>) <span class="comment"># 创建 RNN 层</span></span><br><span class="line">        self.attention = Attention()  <span class="comment"># 创建注意力层</span></span><br><span class="line">        self.out = nn.Linear(<span class="number">2</span> * hidden_size, output_size)  <span class="comment"># 修改线性输出层，考虑隐藏状态和上下文向量</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, dec_input, hidden, enc_output</span>):</span><br><span class="line">        embedded = self.embedding(dec_input)  <span class="comment"># 将输入转换为嵌入向量</span></span><br><span class="line">        rnn_output, hidden = self.rnn(embedded, hidden)  <span class="comment"># 将嵌入向量输入 RNN 层并获取输出 </span></span><br><span class="line">        context, attn_weights = self.attention(rnn_output, enc_output)  <span class="comment"># 计算注意力上下文向量</span></span><br><span class="line">        dec_output = torch.cat((rnn_output, context), dim=-<span class="number">1</span>)  <span class="comment"># 将上下文向量与解码器的输出拼接</span></span><br><span class="line">        dec_output = self.out(dec_output)  <span class="comment"># 使用线性层生成最终输出</span></span><br><span class="line">        <span class="keyword">return</span> dec_output, hidden, attn_weights</span><br><span class="line">n_hidden = <span class="number">128</span> <span class="comment"># 设置隐藏层数量</span></span><br><span class="line"><span class="comment"># 创建编码器和解码器</span></span><br><span class="line">encoder = Encoder(voc_size_cn, n_hidden)</span><br><span class="line">decoder = DecoderWithAttention(n_hidden, voc_size_en)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27; 编码器结构：&#x27;</span>, encoder)  <span class="comment"># 打印编码器的结构</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27; 解码器结构：&#x27;</span>, decoder)  <span class="comment"># 打印解码器的结构</span></span><br></pre></td></tr></table></figure><p><strong>嵌入层</strong>：</p><ul><li>将解码器的输入（目标句子单词的索引）转成向量。</li></ul><p><strong>RNN 层</strong>：</p><ul><li>处理嵌入后的向量，得到当前时间步的输出以及更新后的隐藏状态，就像大厨根据配料进行预处理。</li></ul><p><strong>注意力机制</strong>：</p><ul><li>利用 Attention 层，根据当前解码器的输出（<code>rnn_output</code>）和编码器的输出（<code>enc_output</code>）计算注意力分数，并生成加权的上下文向量 <code>context</code>。</li></ul><p><strong>拼接与输出</strong>：</p><ul><li>将 RNN 输出和上下文向量在最后一维进行拼接，然后通过一个全连接层（线性层）得到最终的预测输出。</li></ul><figure class="highlight markdown"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">编码器结构： Encoder(</span><br><span class="line"></span><br><span class="line">  (embedding): Embedding(18, 128)</span><br><span class="line"></span><br><span class="line">  (rnn): RNN(128, 128, batch<span class="emphasis">_first=True)</span></span><br><span class="line"><span class="emphasis"></span></span><br><span class="line"><span class="emphasis">)</span></span><br><span class="line"><span class="emphasis"></span></span><br><span class="line"><span class="emphasis"> 解码器结构： DecoderWithAttention(</span></span><br><span class="line"><span class="emphasis"></span></span><br><span class="line"><span class="emphasis">  (embedding): Embedding(20, 128)</span></span><br><span class="line"><span class="emphasis"></span></span><br><span class="line"><span class="emphasis">  (rnn): RNN(128, 128, batch_</span>first=True)</span><br><span class="line"></span><br><span class="line">  (attention): Attention()</span><br><span class="line"></span><br><span class="line">  (out): Linear(in<span class="emphasis">_features=256, out_</span>features=20, bias=True)</span><br><span class="line"></span><br><span class="line">)</span><br></pre></td></tr></table></figure><h3 id="2-5-重构Seq2Seq类"><a href="#2-5-重构Seq2Seq类" class="headerlink" title="2.5 重构Seq2Seq类"></a>2.5 重构Seq2Seq类</h3><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 定义 Seq2Seq 类</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Seq2Seq</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, encoder, decoder</span>):</span><br><span class="line">        <span class="built_in">super</span>(Seq2Seq, self).__init__()</span><br><span class="line">        <span class="comment"># 初始化编码器和解码器</span></span><br><span class="line">        self.encoder = encoder</span><br><span class="line">        self.decoder = decoder    </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, encoder_input, hidden, decoder_input</span>): </span><br><span class="line">        <span class="comment"># 将输入序列通过编码器并获取输出和隐藏状态</span></span><br><span class="line">        encoder_output, encoder_hidden = self.encoder(encoder_input, hidden)</span><br><span class="line">        <span class="comment"># 将编码器的隐藏状态传递给解码器作为初始隐藏状态</span></span><br><span class="line">        decoder_hidden = encoder_hidden</span><br><span class="line">        <span class="comment"># 将目标序列通过解码器并获取输出 -  此处更新解码器调用</span></span><br><span class="line">        decoder_output, _, attn_weights = self.decoder(decoder_input, decoder_hidden, encoder_output) </span><br><span class="line">        <span class="keyword">return</span> decoder_output, attn_weights</span><br><span class="line"><span class="comment"># 创建 Seq2Seq 模型</span></span><br><span class="line">model = Seq2Seq(encoder, decoder)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;S2S 模型结构：&#x27;</span>, model)  <span class="comment"># 打印模型的结构</span></span><br></pre></td></tr></table></figure><figure class="highlight markdown"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line">S2S 模型结构： Seq2Seq(</span><br><span class="line"></span><br><span class="line">  (encoder): Encoder(</span><br><span class="line"></span><br><span class="line"><span class="code">    (embedding): Embedding(18, 128)</span></span><br><span class="line"><span class="code"></span></span><br><span class="line"><span class="code">    (rnn): RNN(128, 128, batch_first=True)</span></span><br><span class="line"><span class="code"></span></span><br><span class="line">  )</span><br><span class="line"></span><br><span class="line">  (decoder): DecoderWithAttention(</span><br><span class="line"></span><br><span class="line"><span class="code">    (embedding): Embedding(20, 128)</span></span><br><span class="line"><span class="code"></span></span><br><span class="line"><span class="code">    (rnn): RNN(128, 128, batch_first=True)</span></span><br><span class="line"><span class="code"></span></span><br><span class="line"><span class="code">    (attention): Attention()</span></span><br><span class="line"><span class="code"></span></span><br><span class="line"><span class="code">    (out): Linear(in_features=256, out_features=20, bias=True)</span></span><br><span class="line"><span class="code"></span></span><br><span class="line">  )</span><br><span class="line"></span><br><span class="line">)</span><br></pre></td></tr></table></figure><p>步骤解析:</p><p><strong>编码器部分</strong>：</p><p>使用 </p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">encoder_input</span><br></pre></td></tr></table></figure><p> 和初始隐藏状态 </p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hidden</span><br></pre></td></tr></table></figure><p> 作为输入，通过编码器处理后得到两个结果：</p><ul><li><code>encoder_output</code>：编码器在每个时间步的输出信息。</li><li><code>encoder_hidden</code>：编码器处理完所有输入后的最终隐藏状态。</li></ul><ol><li><strong>传递隐藏状态</strong>：<ul><li>将编码器的隐藏状态 <code>encoder_hidden</code> 直接作为解码器的初始隐藏状态，这样解码器便能“接续”编码器的记忆，保证信息的连续性。</li></ul></li><li><strong>解码器部分</strong>：<ul><li>将 <code>decoder_input</code>（目标序列的输入）、初始隐藏状态 <code>decoder_hidden</code> 以及编码器的输出 <code>encoder_output</code> 一同输入解码器。</li><li>==解码器内部会通过注意力机制计算当前解码状态与编码器输出之间的相关性，生成加权的上下文向量，并与解码器的 RNN 输出结合后产生最终预测==。</li><li>注意解码器返回了三个值，这里我们只需要 <code>decoder_output</code> 和 <code>attn_weights</code>（注意力权重），中间那个被忽略了，用 <code>_</code> 表示。</li></ul></li><li><strong>返回结果</strong>：<ul><li>函数最后返回解码器的输出 <code>decoder_output</code> 和注意力权重 <code>attn_weights</code>，这两个信息分别代表生成的目标序列和翻译过程中各个时间步关注的原始输入信息。</li></ul></li></ol><p>想象整个流程就像是编码器（大侦探）首先搜集所有线索，然后把搜集到的重要信息传给解码器（翻译大师），翻译大师通过“放大镜”（注意力机制）仔细核对细节，最终生成精准的译文。</p><h3 id="2-6可视化权重"><a href="#2-6可视化权重" class="headerlink" title="2.6可视化权重"></a>2.6可视化权重</h3><p>训练函数：</p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 定义训练函数</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">train_seq2seq</span>(<span class="params">model, criterion, optimizer, epochs</span>):</span><br><span class="line">    <span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(epochs):</span><br><span class="line">       encoder_input, decoder_input, target = make_data(sentences) <span class="comment"># 训练数据的创建</span></span><br><span class="line">       hidden = torch.zeros(<span class="number">1</span>, encoder_input.size(<span class="number">0</span>), n_hidden) <span class="comment"># 初始化隐藏状态      </span></span><br><span class="line">       optimizer.zero_grad()<span class="comment"># 梯度清零        </span></span><br><span class="line">       output, _ = model(encoder_input, hidden, decoder_input) <span class="comment"># 获取模型输出         </span></span><br><span class="line">       loss = criterion(output.view(-<span class="number">1</span>, voc_size_en), target.view(-<span class="number">1</span>)) <span class="comment"># 计算损失        </span></span><br><span class="line">       <span class="keyword">if</span> (epoch + <span class="number">1</span>) % <span class="number">40</span> == <span class="number">0</span>: <span class="comment"># 打印损失</span></span><br><span class="line">          <span class="built_in">print</span>(<span class="string">f&quot;Epoch: <span class="subst">&#123;epoch + <span class="number">1</span>:04d&#125;</span> cost = <span class="subst">&#123;loss:<span class="number">.6</span>f&#125;</span>&quot;</span>)         </span><br><span class="line">       loss.backward()<span class="comment"># 反向传播        </span></span><br><span class="line">       optimizer.step()<span class="comment"># 更新参数      </span></span><br><span class="line"><span class="comment"># 训练模型</span></span><br><span class="line">epochs = <span class="number">400</span> <span class="comment"># 训练轮次</span></span><br><span class="line">criterion = nn.CrossEntropyLoss() <span class="comment"># 损失函数</span></span><br><span class="line">optimizer = torch.optim.Adam(model.parameters(), lr=<span class="number">0.001</span>) <span class="comment"># 优化器</span></span><br><span class="line">train_seq2seq(model, criterion, optimizer, epochs) <span class="comment"># 调用函数训练模型</span></span><br></pre></td></tr></table></figure><p>定义一个可用于可视化注意力的函数：</p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt <span class="comment"># 导入 matplotlib</span></span><br><span class="line"><span class="keyword">import</span> seaborn <span class="keyword">as</span> sns <span class="comment"># 导入 seaborn</span></span><br><span class="line">plt.rcParams[<span class="string">&quot;font.family&quot;</span>]=[<span class="string">&#x27;SimHei&#x27;</span>] <span class="comment"># 用来设定字体样式</span></span><br><span class="line">plt.rcParams[<span class="string">&#x27;font.sans-serif&#x27;</span>]=[<span class="string">&#x27;SimHei&#x27;</span>] <span class="comment"># 用来设定无衬线字体样式</span></span><br><span class="line">plt.rcParams[<span class="string">&#x27;axes.unicode_minus&#x27;</span>]=<span class="literal">False</span> <span class="comment">#  用 来 正 常 显 示 负 号 </span></span><br><span class="line"><span class="keyword">def</span>  <span class="title function_">visualize_attention</span>(<span class="params">source_sentence, predicted_sentence, attn_weights</span>):    </span><br><span class="line">    plt.figure(figsize=(<span class="number">10</span>, <span class="number">10</span>)) <span class="comment"># 画布</span></span><br><span class="line">    ax = sns.heatmap(attn_weights, annot=<span class="literal">True</span>, cbar=<span class="literal">False</span>, </span><br><span class="line">                     xticklabels=source_sentence.split(), </span><br><span class="line">                     yticklabels=predicted_sentence, cmap=<span class="string">&quot;Greens&quot;</span>) <span class="comment"># 热力图</span></span><br><span class="line">    plt.xlabel(<span class="string">&quot; 源序列 &quot;</span>) </span><br><span class="line">    plt.ylabel(<span class="string">&quot; 目标序列 &quot;</span>)</span><br><span class="line">    plt.show() <span class="comment"># 显示图片</span></span><br></pre></td></tr></table></figure><p>在测试模型的过程中，可视化注意力权重：</p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 定义测试函数</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">test_seq2seq</span>(<span class="params">model, source_sentence</span>):</span><br><span class="line">    <span class="comment"># 将输入的句子转换为索引</span></span><br><span class="line">    encoder_input = np.array([[word2idx_cn[n] <span class="keyword">for</span> n <span class="keyword">in</span> source_sentence.split()]])</span><br><span class="line">    <span class="comment"># 构建输出的句子的索引，以 &#x27;&lt;sos&gt;&#x27; 开始，后面跟 &#x27;&lt;eos&gt;&#x27;，长度与输入句子相同</span></span><br><span class="line">    decoder_input = np.array([word2idx_en[<span class="string">&#x27;&lt;sos&gt;&#x27;</span>]] + [word2idx_en[<span class="string">&#x27;&lt;eos&gt;&#x27;</span>]]*(<span class="built_in">len</span>(encoder_input[<span class="number">0</span>])-<span class="number">1</span>))</span><br><span class="line">    <span class="comment"># 转换为 LongTensor 类型</span></span><br><span class="line">    encoder_input = torch.LongTensor(encoder_input)</span><br><span class="line">    decoder_input = torch.LongTensor(decoder_input).unsqueeze(<span class="number">0</span>) <span class="comment"># 增加一维    </span></span><br><span class="line">    hidden = torch.zeros(<span class="number">1</span>, encoder_input.size(<span class="number">0</span>), n_hidden) <span class="comment"># 初始化隐藏状态    </span></span><br><span class="line">    <span class="comment"># 获取模型输出和注意力权重</span></span><br><span class="line">    predict, attn_weights = model(encoder_input, hidden, decoder_input)    </span><br><span class="line">    predict = predict.data.<span class="built_in">max</span>(<span class="number">2</span>, keepdim=<span class="literal">True</span>)[<span class="number">1</span>] <span class="comment"># 获取概率最大的索引</span></span><br><span class="line">    <span class="comment"># 打印输入的句子和预测的句子</span></span><br><span class="line">    <span class="built_in">print</span>(source_sentence, <span class="string">&#x27;-&gt;&#x27;</span>, [idx2word_en[n.item()] <span class="keyword">for</span> n <span class="keyword">in</span> predict.squeeze()])</span><br><span class="line">    <span class="comment"># 可视化注意力权重</span></span><br><span class="line">    attn_weights = attn_weights.squeeze(<span class="number">0</span>).cpu().detach().numpy()</span><br><span class="line">    visualize_attention(source_sentence, [idx2word_en[n.item()] <span class="keyword">for</span> n <span class="keyword">in</span> predict.squeeze()], attn_weights)    </span><br><span class="line"><span class="comment"># 测试模型</span></span><br><span class="line"></span><br><span class="line">test_seq2seq(model, <span class="string">&#x27;自然 语言 处理 很 强大&#x27;</span>)</span><br></pre></td></tr></table></figure><p><img src="https://rozen-picture.oss-cn-beijing.aliyuncs.com/img/202503151831691.png" alt="image-20250315183151508"></p><p>在这个注意⼒权重矩阵中，NLP对“⾃然”产⽣了很强的关注，权重值为1。</p><p>当然了，我们的训练数据过少，模型可能没有⾜够的数据来学习有效的注意⼒权重。在实际应⽤中，我们当然需要更⼤规模的数据来训练Seq2Seq模型，以便模型捕捉到更丰富的语⾔模式，这样模型才能够学习到更为复杂的注意⼒权重分布模式。</p>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;注意力机制：让AI拥有”黄金七秒记忆”的魔法—（缩放点积注意力）&quot;&gt;&lt;a href=&quot;#注意力机制：让AI拥有”黄金七秒记忆”的魔法—（缩放点积注意力）&quot; class=&quot;headerlink&quot; title=&quot;注意力机制：让AI拥有”黄金七秒记忆”的魔法—（缩放点积</summary>
      
    
    
    
    
  </entry>
  
  <entry>
    <title></title>
    <link href="https://rozen12123.github.io/posts/0.html"/>
    <id>https://rozen12123.github.io/posts/0.html</id>
    <published>2025-03-14T11:53:57.592Z</published>
    <updated>2025-03-15T10:56:29.032Z</updated>
    
    <content type="html"><![CDATA[<h1 id="注意力机制：让AI拥有”黄金七秒记忆”的魔法—（点积注意力）"><a href="#注意力机制：让AI拥有”黄金七秒记忆”的魔法—（点积注意力）" class="headerlink" title="注意力机制：让AI拥有”黄金七秒记忆”的魔法—（点积注意力）"></a>注意力机制：让AI拥有”黄金七秒记忆”的魔法—（点积注意力）</h1><p>注意⼒机制对于初学者来说有点难理解，我们⼀点⼀点地讲。现在先暂时忘记编码器、解码器、隐藏层和序列到序列这些概念。想象我们有两个张量x1和x2，我们希望⽤注意⼒机制把它俩给衔接起来，让x1看⼀看，x2有哪些特别值得关注的地⽅。</p><p>具体来说，要得到x1对x2的点积注意⼒，我们可以按照以下步骤进⾏操作。</p><p>（1）创建两个形状分别为(<code>batch_size</code>, <code>seq_len1</code>, <code>feature_dim</code>)和(<code>batch_size</code>, <code>seq_len2</code>, <code>feature_dim</code>)的张量x1和x2。</p><p>（2）将x1中的每个元素和x2中的每个元素进⾏点积，得到形状为 (<code>batch_size</code>, <code>seq_len1</code>, <code>seq_len2</code>)的原始权重<code>raw_weights</code>。</p><p>（3）⽤<code>softmax</code>函数对原始权重进⾏归⼀化，得到归⼀化后的注意⼒权重<code>attn_weights</code>（注意⼒权重的值在0和1之间，且每⼀⾏的和为1），形状仍为 (<code>batch_size</code>, <code>seq_len1</code>, <code>seq_len2</code>)。</p><p>（4）⽤注意⼒权重<code>attn_weights</code>对x2中的元素进⾏加权求和（与x2相乘），得到输出张量y，形状为 (<code>batch_size</code>, <code>seq_len1</code>, <code>feature_dim</code>)。这就是x1对x2的点积注意⼒。</p><p>程序结构如下：</p><p><img src="https://rozen-picture.oss-cn-beijing.aliyuncs.com/img/202503142012518.png" alt="image-20250314201213369"></p><h2 id="一、点积注意机制"><a href="#一、点积注意机制" class="headerlink" title="一、点积注意机制"></a>一、点积注意机制</h2><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch <span class="comment"># 导入 torch</span></span><br><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F <span class="comment"># 导入 nn.functional</span></span><br><span class="line"><span class="comment"># 1. 创建两个张量 x1 和 x2</span></span><br><span class="line">x1 = torch.randn(<span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>) <span class="comment"># 形状 (batch_size, seq_len1, feature_dim)</span></span><br><span class="line">x2 = torch.randn(<span class="number">2</span>, <span class="number">5</span>, <span class="number">4</span>) <span class="comment"># 形状 (batch_size, seq_len2, feature_dim)</span></span><br><span class="line"><span class="comment"># 2. 计算原始权重</span></span><br><span class="line">raw_weights = torch.bmm(x1, x2.transpose(<span class="number">1</span>, <span class="number">2</span>)) <span class="comment"># 形状 (batch_size, seq_len1, seq_len2)</span></span><br><span class="line"><span class="comment"># 3. 用 softmax 函数对原始权重进行归一化</span></span><br><span class="line">attn_weights = F.softmax(raw_weights, dim=<span class="number">2</span>) <span class="comment"># 形状 (batch_size, seq_len1, seq_len2)</span></span><br><span class="line"><span class="comment"># 4. 将注意力权重与 x2 相乘，计算加权和</span></span><br><span class="line">attn_output = torch.bmm(attn_weights, x2)  <span class="comment"># 形状 (batch_size, seq_len1, feature_dim)</span></span><br></pre></td></tr></table></figure><h3 id="1-1-创建两个张量x1和x2"><a href="#1-1-创建两个张量x1和x2" class="headerlink" title="1.1 创建两个张量x1和x2"></a>1.1 创建两个张量x1和x2</h3><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 创建两个张量 x1 和 x2</span></span><br><span class="line">x1 = torch.randn(<span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>) <span class="comment"># 形状 (batch_size, seq_len1, feature_dim)</span></span><br><span class="line">x2 = torch.randn(<span class="number">2</span>, <span class="number">5</span>, <span class="number">4</span>) <span class="comment"># 形状 (batch_size, seq_len2, feature_dim)</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;x1:&quot;</span>, x1)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;x2:&quot;</span>, x2)</span><br></pre></td></tr></table></figure><h3 id="1-2-计算张量点积，得到原始权重"><a href="#1-2-计算张量点积，得到原始权重" class="headerlink" title="1.2 计算张量点积，得到原始权重"></a>1.2 计算张量点积，得到原始权重</h3><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 计算点积，得到原始权重，形状为 (batch_size, seq_len1, seq_len2)</span></span><br><span class="line">raw_weights = torch.bmm(x1, x2.transpose(<span class="number">1</span>, <span class="number">2</span>))</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot; 原始权重：&quot;</span>, raw_weights) </span><br></pre></td></tr></table></figure><p>因为是对x1和x2的两个特征维度进行点积后归一化，所以要对x2数组进行转置。</p><p><img src="https://rozen-picture.oss-cn-beijing.aliyuncs.com/img/202503142215836.png" alt="image-20250314221502682"></p><p>⽐如，输出结果的第⼀⾏[ 1.2474, -0.6254, 1.4849, 2.9333, -0.1787]就代表着本批次第⼀个x1序列中第⼀个元素（每个x1序列有3个元素，所以第⼀批次共3⾏）与x2中第⼀批次5个元素的每⼀个元素的相似度得分（不难看出，x1中第⼀个元素与x2中第4个元素最相似，原始注意⼒分值为2.9333）。</p><p>==相似度的计算是注意⼒机制最核⼼的思想==。</p><p>因为点积其实可以一定程度上反应向量方向的相关性，所以通过将x1的元素与x2的各个元素进行点积就可以求出权重（原始得分）其中在x2中权重较大的（得分高的）对应的词即为与x1中相关度最高的，故x2可以根据这个原始的分来判断应该输出那些对应x1相关性最高的内容。</p><p>x1起到编译器，x2起到译码器的作用</p><p>在某些⽂献或代码中，有时会将相似度得分称为原始权重。这是因为它们实际上是在计算注意⼒权重之前的中间结果。严格来说，相似度得分表示输⼊序列中不同元素之间的关联性或相似度，⽽权重则是在应⽤某些操作（如缩放、掩码和归⼀化）后得到的归⼀化值。为了避免混淆，可以将这两个术语彻底区分开。</p><p>通常，将未处理的值称为得分，并在经过处理后将它们称为权重。这有助于更清晰地理解注意⼒机制的⼯作原理及其不同组件。</p><p>举一个栗子：</p><p>让我们⽤下⾯的图示来对向量点积和相似度得分进⾏相对直观的理解。在下图的例⼦中，有两个向量——电影的特征（<strong>M</strong>）和⽤户的兴趣（<strong>U</strong>）</p><p><img src="https://rozen-picture.oss-cn-beijing.aliyuncs.com/img/202503142227713.png" alt="image-20250314222742613"></p><p>向量<strong>U</strong>中可能蕴含⽤户是否喜欢爱情⽚、喜欢动作⽚等信息；⽽向量<strong>M</strong>中则包含电影含有动作、浪漫等特征的程度。</p><p>通过计算<strong>U</strong>和<strong>M</strong>的点积或相似度得分，我们可以得到⼀个衡量<strong>U</strong>对<strong>M</strong>兴趣程度的分数。例如，如果向量<strong>U</strong>中喜欢爱情⽚、喜欢动作⽚的权重较⾼，⽽向量<strong>M</strong>中的动作和浪漫特征的权重也较⾼，那么计算得到的点积或相似度得分就会⽐较⾼，表示<strong>U</strong>对<strong>M</strong>的兴趣较⼤，系统有可能推荐这部电影给⽤户。</p><h3 id="1-3-对原始权重进行归一化"><a href="#1-3-对原始权重进行归一化" class="headerlink" title="1.3 对原始权重进行归一化"></a>1.3 对原始权重进行归一化</h3><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F <span class="comment"># 导入 torch.nn.functional</span></span><br><span class="line"><span class="comment"># 应用 softmax 函数，使权重的值在 0 和 1 之间，且每一行的和为 1</span></span><br><span class="line">attn_weights = F.softmax(raw_weights, dim=-<span class="number">1</span>) <span class="comment"># 归一化</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot; 归一化后的注意力权重：&quot;</span>, attn_weights)</span><br></pre></td></tr></table></figure><p>所谓的归⼀化，其实理解起来很简单。得到每⼀个x1序列中的元素与其所对应的5个x2序列元素的相似度得分后，使⽤softmax函数进⾏缩放，让这5个数加起来等于1。</p><p><img src="https://rozen-picture.oss-cn-beijing.aliyuncs.com/img/202503142231623.png" alt="image-20250314223142527"></p><figure class="highlight markdown"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">归一化后的注意力权重： tensor([[[0.3154, 0.2383, 0.2145, 0.1589, 0.0729],</span><br><span class="line"></span><br><span class="line"><span class="code">        [0.0015, 0.9234, 0.0090, 0.0015, 0.0645],</span></span><br><span class="line"><span class="code"></span></span><br><span class="line"><span class="code">        [0.0533, 0.0576, 0.5788, 0.0858, 0.2245]],</span></span><br><span class="line"><span class="code"></span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="code">       [[0.4959, 0.0374, 0.1558, 0.0349, 0.2760],</span></span><br><span class="line"><span class="code"></span></span><br><span class="line"><span class="code">        [0.0034, 0.0470, 0.0424, 0.8826, 0.0246],</span></span><br><span class="line"><span class="code"></span></span><br><span class="line"><span class="code">        [0.2597, 0.0678, 0.0840, 0.1356, 0.4530]]])</span></span><br><span class="line"><span class="code"></span></span><br></pre></td></tr></table></figure><p>归⼀化后，<code>attn_weights</code>(权重)和<code>raw_weights</code>（得分）形状相同，但是值变了，第⼀⾏的5个数字加起来刚好是1。第4个数字是0.6697，这就表明：在本批次的第⼀⾏数据中，x2序列中的第4个元素和x1序列的第1个元素特别相关，应该加以注意。</p><h3 id="1-4-求出注意力机制的加权和"><a href="#1-4-求出注意力机制的加权和" class="headerlink" title="1.4 求出注意力机制的加权和"></a>1.4 求出注意力机制的加权和</h3><p>注意⼒权重与x2相乘，就得到==注意⼒分布的加权和==。</p><p>换句话说，我们将x2中的每个位置向量乘以它们在x1中对应位置的注意⼒权重，然后将这些加权向量求和——这是点积注意⼒计算的最后⼀个环节。这⼀步的⽬的是根据注意⼒权重计算x2的加权和。这个==加权和才是x1对x2的注意⼒输出==。</p><p>加权只是对应着一个关系表，并不代表输出。</p><p>相当于在一个函数中，已经求得了对应关系，现在需要给一个输入，才能得出一个输出值。</p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 与 x2 相乘，得到注意力分布的加权和，形状为 (batch_size, seq_len1, feature_dim)</span></span><br><span class="line">attn_output = torch.bmm(attn_weights, x2)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot; 注意力输出 :&quot;</span>, attn_output)</span><br></pre></td></tr></table></figure><p><img src="https://rozen-picture.oss-cn-beijing.aliyuncs.com/img/202503142242634.png" alt="image-20250314224208498"></p>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;注意力机制：让AI拥有”黄金七秒记忆”的魔法—（点积注意力）&quot;&gt;&lt;a href=&quot;#注意力机制：让AI拥有”黄金七秒记忆”的魔法—（点积注意力）&quot; class=&quot;headerlink&quot; title=&quot;注意力机制：让AI拥有”黄金七秒记忆”的魔法—（点积注意力）&quot;&gt;</summary>
      
    
    
    
    
  </entry>
  
  <entry>
    <title></title>
    <link href="https://rozen12123.github.io/posts/0.html"/>
    <id>https://rozen12123.github.io/posts/0.html</id>
    <published>2025-03-10T12:33:20.265Z</published>
    <updated>2025-03-11T14:09:56.161Z</updated>
    
    <content type="html"><![CDATA[<h1 id="循环神经网络：给AI装上”记忆芯片”"><a href="#循环神经网络：给AI装上”记忆芯片”" class="headerlink" title="循环神经网络：给AI装上”记忆芯片”"></a>循环神经网络：给AI装上”记忆芯片”</h1><p>RNN的核⼼思想是利⽤“循环”的机制，将⽹络的输出反馈到输⼊，这使得它能够==在处理数据时保留前⾯的信息==，从⽽捕获序列中的⻓距离依赖关系，在处理序列数据，如⽂本、语⾳和时间序列时具有明显的优势。</p><p><img src="https://rozen-picture.oss-cn-beijing.aliyuncs.com/img/202503112136064.png" alt="image-20250311213635894"></p><p>在每一次的输入处理中都会把之前已经提问过的问题通过与权重相乘到新的节点。</p><p>结合前⼀时间步的隐藏层状态$h_{(t-1)}$，计算当前时间步的隐藏层状态$h_t$（即上图中的<em>h</em>）。这通常通过⼀个激活函数（如tanh函数）实现。计算公式如下（其中，$W_{hh}$是隐藏层90到隐藏层的权重矩阵，$W_{xh}$是输⼊到隐藏层的权重矩阵）</p><script type="math/tex; mode=display">h_t = tanh(W_{hh} * h_{(t-1)} + W_{xh} * x_t + b_h)</script><p>（3）基于当前时间步的隐藏层状态$h_t$，计算输出层$y_t$（RNN在时间步<em>t</em>的输出）。通常通过⼀个线性变换和激活函数实现。计算公式如下：</p><script type="math/tex; mode=display">y_t = softmax(W_{hy} * h_t + b_y)</script><p>==RNN采⽤BPTT算法进⾏训练==。</p><p>与普通反向传播不同，BPTT算法需要在时间维度上展开RNN，以便在处理时序依赖性时计算损失梯度。</p><p>因此，BPTT算法可以看作⼀种针对具有时间结构的数据的反向传播算法。在BPTT算法中，我们⾸先⽤损失函数计算模型的损失（如交叉熵损失），然后使⽤梯度下降法（或其他优化算法）来更新模型参数。</p><p>BPTT算法的关键在于，我们需要将梯度沿着时间步（对于⾃然语⾔处理问题来说，时间步就是⽂本序列的token）反向传播，从输出层⼀直传播到输⼊层。具体步骤如下。</p><p>（1）根据模型的输出和实际标签计算损失。对每个时间步，都可以计算⼀个损失值，然后对所有时间步的损失值求和，得到总损失。</p><p>（2）计算损失函数关于模型参数（权重矩阵和偏置）的梯度。这需要应⽤链式求导法则，分别计算损失函数关于输出层、隐藏层和输⼊层的梯度。然后将这些梯度沿着时间步传播回去。</p><p>（3）使⽤优化算法（如梯度下降法、Adam等）来更新模型参数。这包括更新权重矩阵（$W_{hh}$，$W_{xh}$和$W_{hy}$）和偏置（$b_h$和$b_y$）。</p><p>如下图此类多输入多输出的情况可以有特定信息识别的功能:</p><p><img src="https://rozen-picture.oss-cn-beijing.aliyuncs.com/img/202503112140864.png" alt="image-20250311214006742"></p><p>再如下图：</p><p><img src="https://rozen-picture.oss-cn-beijing.aliyuncs.com/img/202503112203846.png" alt="image-20250311220303708"></p><p>应用于情感识别</p><p>eg：l feel happy watching the movie</p><p>判断：可通过happy判断为正向情感</p><p>&nbsp;</p><p>又如下图：</p><p><img src="https://rozen-picture.oss-cn-beijing.aliyuncs.com/img/202503112204767.png" alt="image-20250311220408658"></p><p>可做序列生成器</p><p>例如文章生成或者音乐生成</p><p>&nbsp;</p><p>再如下图</p><p><img src="https://rozen-picture.oss-cn-beijing.aliyuncs.com/img/202503112206404.png" alt="image-20250311220654298"></p><p>可做语言翻译</p><h2 id="1-RNN实现流程"><a href="#1-RNN实现流程" class="headerlink" title="1.RNN实现流程"></a>1.RNN实现流程</h2><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn <span class="comment"># 导⼊神经⽹络模块</span></span><br><span class="line"><span class="comment"># 定义神经概率语⾔模型（NPLM）</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">NPLM</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="built_in">super</span>(NPLM, self).__init__() <span class="comment"># 调⽤⽗类的构造函数</span></span><br><span class="line">        self.C = nn.Embedding(voc_size, embedding_size) <span class="comment"># 定义⼀个词嵌⼊层</span></span><br><span class="line">        <span class="comment"># ⽤LSTM层替代第⼀个线性层，其输⼊⼤⼩为embedding_size，隐藏层⼤⼩为n_hidden</span></span><br><span class="line">        self.lstm = nn.LSTM(embedding_size, n_hidden, batch_first=<span class="literal">True</span>)</span><br><span class="line">        <span class="comment"># 第⼆个线性层，其输⼊⼤⼩为n_hidden，输出⼤⼩为voc_size，即词汇表⼤⼩</span></span><br><span class="line">        self.linear = nn.Linear(n_hidden, voc_size)</span><br><span class="line"><span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, X</span>): <span class="comment"># 定义前向传播过程</span></span><br><span class="line">    <span class="comment"># 输⼊数据X张量的形状为[batch_size, n_step]</span></span><br><span class="line">    X = self.C(X) <span class="comment"># 将X通过词嵌⼊层，形状变为[batch_size, n_step, embedding_size]</span></span><br><span class="line">    <span class="comment"># 通过LSTM层</span></span><br><span class="line">    lstm_out, _ = self.lstm(X) <span class="comment"># lstm_out形状变为[batch_size, n_step, n_hidden]</span></span><br><span class="line">    <span class="comment"># 只选择最后⼀个时间步的输出作为全连接层的输⼊，通过第⼆个线性层得到输出</span></span><br><span class="line">    output = self.linear(lstm_out[:, -<span class="number">1</span>, :]) <span class="comment"># output的形状为[batch_size, voc_size]</span></span><br><span class="line">    <span class="keyword">return</span> output <span class="comment"># 返回输出结果</span></span><br></pre></td></tr></table></figure><figure class="highlight markdown"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">RNN模型结构：RNNLM(</span><br><span class="line">(embedding): Embedding(7, 2)</span><br><span class="line">(lstm): LSTM(2, 2, batch<span class="emphasis">_first=True)</span></span><br><span class="line"><span class="emphasis">(linear): Linear(in_</span>features=2, out<span class="emphasis">_features=7, bias=True))</span></span><br></pre></td></tr></table></figure><p><img src="https://rozen-picture.oss-cn-beijing.aliyuncs.com/img/202503112207356.png" alt="image-20250311220756261"></p><p>前部序列信息在传递到后部的同时，信息权重下降，导致重要信息丢失</p><p>求解过程中梯度消失</p><h2 id="2-LSTM：记忆宫殿的建造师"><a href="#2-LSTM：记忆宫殿的建造师" class="headerlink" title="2. LSTM：记忆宫殿的建造师"></a>2. LSTM：记忆宫殿的建造师</h2><p><img src="https://rozen-picture.oss-cn-beijing.aliyuncs.com/img/202503091827712.png" alt="image-20250309182715557"></p><p>LSTM会计算输⼊⻔、遗忘⻔和输出⻔的激活值。这些⻔控机制使得LSTM能够有选择地</p><p>保留或遗忘之前的信息，从⽽更好地捕捉⻓距离依赖关系。这些⻔的计算公式如下。</p><ul><li>输⼊⻔：i_t = sigmoid(W_ii <em> x_t + b_ii + W_hi </em> h_(t-1) + b_hi)</li><li>遗忘⻔：f_t = sigmoid(W_if <em> x_t + b_if + W_hf </em> h_(t-1) + b_hf)</li><li>输出⻔：o_t = sigmoid(W_io <em> x_t + b_io + W_ho </em> h_(t-1) + b_ho)</li></ul><p>LSTM更新细胞状态c_t。这是通过结合输⼊⻔、遗忘⻔和当前输⼊的信息来实现的。计算公式如下:</p><ul><li>细胞候选状态：g_t = tanh(W_ig <em> x_t + b_ig + W_hg </em> h_(t-1) + b_hg)</li><li>细胞状态更新：c_t = f_t <em> c_(t-1) + i_t </em> g_t</li></ul><p>最后，LSTM会计算当前时间步的隐藏状态h_t，这通常作为输出。计算公式如下:</p><ul><li>隐藏状态：h_t = o_t * tanh(c_t)</li></ul><p><strong>代码实现</strong>：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">LSTM</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, input_size, hidden_size</span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        self.input_size = input_size</span><br><span class="line">        self.hidden_size = hidden_size</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 定义各类门结构参数</span></span><br><span class="line">        self.W_xi = nn.Parameter(torch.Tensor(input_size, hidden_size))</span><br><span class="line">        self.W_hi = nn.Parameter(torch.Tensor(hidden_size, hidden_size))</span><br><span class="line">        <span class="comment"># ...其他参数初始化...</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x, init_states=<span class="literal">None</span></span>):</span><br><span class="line">        <span class="comment"># 实现门控逻辑</span></span><br><span class="line">        i_t = torch.sigmoid(x @ self.W_xi + h_prev @ self.W_hi)</span><br><span class="line">        <span class="comment"># ...其他门计算...</span></span><br><span class="line">        <span class="keyword">return</span> output, (h_t, c_t)</span><br></pre></td></tr></table></figure><h3 id><a href="#" class="headerlink" title=" "></a> </h3><h2 id="3-从AI到AGI：记忆与推理的终极进化"><a href="#3-从AI到AGI：记忆与推理的终极进化" class="headerlink" title="3.从AI到AGI：记忆与推理的终极进化"></a>3.从AI到AGI：记忆与推理的终极进化</h2><p><strong>前沿突破</strong>：</p><ul><li>Neural Turing Machine：给RNN配备可微分内存</li><li>Differentiable Neural Computer：实现类比推理</li><li>Liquid Neural Network：模拟生物神经网络特性</li></ul><p><strong>未来展望</strong>：</p><ul><li>实时翻译：具备上下文记忆的同声传译</li><li>文学创作：续写《红楼梦》后四十回</li><li>科研突破：预测蛋白质折叠轨迹</li></ul>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;循环神经网络：给AI装上”记忆芯片”&quot;&gt;&lt;a href=&quot;#循环神经网络：给AI装上”记忆芯片”&quot; class=&quot;headerlink&quot; title=&quot;循环神经网络：给AI装上”记忆芯片”&quot;&gt;&lt;/a&gt;循环神经网络：给AI装上”记忆芯片”&lt;/h1&gt;&lt;p&gt;RNN的核⼼</summary>
      
    
    
    
    
  </entry>
  
  <entry>
    <title></title>
    <link href="https://rozen12123.github.io/posts/0.html"/>
    <id>https://rozen12123.github.io/posts/0.html</id>
    <published>2025-03-09T10:18:47.335Z</published>
    <updated>2025-03-13T14:32:58.290Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Seq2Seq：让机器学会”同声传译”的魔法架构"><a href="#Seq2Seq：让机器学会”同声传译”的魔法架构" class="headerlink" title="Seq2Seq：让机器学会”同声传译”的魔法架构"></a>Seq2Seq：让机器学会”同声传译”的魔法架构</h1><h2 id="一、当AI遇上国际会议：传统模型的三大困境"><a href="#一、当AI遇上国际会议：传统模型的三大困境" class="headerlink" title="一、当AI遇上国际会议：传统模型的三大困境"></a>一、当AI遇上国际会议：传统模型的三大困境</h2><p><strong>震撼案例</strong>：2016年某国际峰会，机器翻译出现致命错误：</p><p>原文：”The agreement is not legally binding” 错误翻译：”协议没有装订书皮” 正确翻译：”该协议不具备法律约束力”</p><p>这个真实事故背后暴露了传统模型的三大局限：</p><ol><li><strong>长度桎梏</strong>：定长输入 vs 动态议程</li><li><strong>语境丢失</strong>：逐词翻译导致语义断裂</li><li><strong>歧义困境</strong>：无法处理一词多义（如”apple”指水果公司）</li></ol><p><img src="https://rozen-picture.oss-cn-beijing.aliyuncs.com/img/202503132232809.png" alt="image-20250313223249678"></p><p>Seq2Seq架构：编码器-解码器架构</p><p><img src="https://rozen-picture.oss-cn-beijing.aliyuncs.com/img/202503122056869.png" alt="image-20250312205631708"></p><h3 id="1-1构建实验语料库和词汇表"><a href="#1-1构建实验语料库和词汇表" class="headerlink" title="1.1构建实验语料库和词汇表"></a>1.1构建实验语料库和词汇表</h3><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 构建语料库，每行包含中文、英文（解码器输入）和翻译成英文后的目标输出 3 个句子</span></span><br><span class="line">sentences = [</span><br><span class="line">    [<span class="string">&#x27;咖哥 喜欢 小冰&#x27;</span>, <span class="string">&#x27;&lt;sos&gt; KaGe likes XiaoBing&#x27;</span>, <span class="string">&#x27;KaGe likes XiaoBing &lt;eos&gt;&#x27;</span>],</span><br><span class="line">    [<span class="string">&#x27;我 爱 学习 人工智能&#x27;</span>, <span class="string">&#x27;&lt;sos&gt; I love studying AI&#x27;</span>, <span class="string">&#x27;I love studying AI &lt;eos&gt;&#x27;</span>],</span><br><span class="line">    [<span class="string">&#x27;深度学习 改变 世界&#x27;</span>, <span class="string">&#x27;&lt;sos&gt; DL changed the world&#x27;</span>, <span class="string">&#x27;DL changed the world &lt;eos&gt;&#x27;</span>],</span><br><span class="line">    [<span class="string">&#x27;自然 语言 处理 很 强大&#x27;</span>, <span class="string">&#x27;&lt;sos&gt; NLP is so powerful&#x27;</span>, <span class="string">&#x27;NLP is so powerful &lt;eos&gt;&#x27;</span>],</span><br><span class="line">    [<span class="string">&#x27;神经网络 非常 复杂&#x27;</span>, <span class="string">&#x27;&lt;sos&gt; Neural-Nets are complex&#x27;</span>, <span class="string">&#x27;Neural-Nets are complex &lt;eos&gt;&#x27;</span>]]</span><br><span class="line">word_list_cn, word_list_en = [], []  <span class="comment"># 初始化中英文词汇表</span></span><br><span class="line"><span class="comment"># 遍历每一个句子并将单词添加到词汇表中</span></span><br><span class="line"><span class="keyword">for</span> s <span class="keyword">in</span> sentences:</span><br><span class="line">    word_list_cn.extend(s[<span class="number">0</span>].split())</span><br><span class="line">    word_list_en.extend(s[<span class="number">1</span>].split())</span><br><span class="line">    word_list_en.extend(s[<span class="number">2</span>].split())</span><br><span class="line"><span class="comment"># 去重，得到没有重复单词的词汇表</span></span><br><span class="line">word_list_cn = <span class="built_in">list</span>(<span class="built_in">set</span>(word_list_cn))</span><br><span class="line">word_list_en = <span class="built_in">list</span>(<span class="built_in">set</span>(word_list_en))</span><br><span class="line"><span class="comment"># 构建单词到索引的映射</span></span><br><span class="line">word2idx_cn = &#123;w: i <span class="keyword">for</span> i, w <span class="keyword">in</span> <span class="built_in">enumerate</span>(word_list_cn)&#125;</span><br><span class="line">word2idx_en = &#123;w: i <span class="keyword">for</span> i, w <span class="keyword">in</span> <span class="built_in">enumerate</span>(word_list_en)&#125;</span><br><span class="line"><span class="comment"># 构建索引到单词的映射</span></span><br><span class="line">idx2word_cn = &#123;i: w <span class="keyword">for</span> i, w <span class="keyword">in</span> <span class="built_in">enumerate</span>(word_list_cn)&#125;</span><br><span class="line">idx2word_en = &#123;i: w <span class="keyword">for</span> i, w <span class="keyword">in</span> <span class="built_in">enumerate</span>(word_list_en)&#125;</span><br><span class="line"><span class="comment"># 计算词汇表的大小</span></span><br><span class="line">voc_size_cn = <span class="built_in">len</span>(word_list_cn)</span><br><span class="line">voc_size_en = <span class="built_in">len</span>(word_list_en)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot; 句子数量：&quot;</span>, <span class="built_in">len</span>(sentences)) <span class="comment"># 打印句子数</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot; 中文词汇表大小：&quot;</span>, voc_size_cn) <span class="comment"># 打印中文词汇表大小</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot; 英文词汇表大小：&quot;</span>, voc_size_en) <span class="comment"># 打印英文词汇表大小</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot; 中文词汇到索引的字典：&quot;</span>, word2idx_cn) <span class="comment"># 打印中文词汇到索引的字典</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot; 英文词汇到索引的字典：&quot;</span>, word2idx_en) <span class="comment"># 打印英文词汇到索引的字典</span></span><br></pre></td></tr></table></figure><h3 id="1-2-生成seq2seq训练序列"><a href="#1-2-生成seq2seq训练序列" class="headerlink" title="1.2 生成seq2seq训练序列"></a>1.2 生成seq2seq训练序列</h3><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np <span class="comment"># 导入 numpy</span></span><br><span class="line"><span class="keyword">import</span> torch <span class="comment"># 导入 torch</span></span><br><span class="line"><span class="keyword">import</span> random <span class="comment"># 导入 random 库</span></span><br><span class="line"><span class="comment"># 定义一个函数，随机选择一个句子和词汇表生成输入、输出和目标数据</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">make_data</span>(<span class="params">sentences</span>):</span><br><span class="line">    <span class="comment"># 随机选择一个句子进行训练</span></span><br><span class="line">    random_sentence = random.choice(sentences)</span><br><span class="line">    <span class="comment"># 将输入句子中的单词转换为对应的索引</span></span><br><span class="line">    encoder_input = np.array([[word2idx_cn[n] <span class="keyword">for</span> n <span class="keyword">in</span> random_sentence[<span class="number">0</span>].split()]])</span><br><span class="line">    <span class="comment"># 将输出句子中的单词转换为对应的索引</span></span><br><span class="line">    decoder_input = np.array([[word2idx_en[n] <span class="keyword">for</span> n <span class="keyword">in</span> random_sentence[<span class="number">1</span>].split()]])</span><br><span class="line">    <span class="comment"># 将目标句子中的单词转换为对应的索引</span></span><br><span class="line">    target = np.array([[word2idx_en[n] <span class="keyword">for</span> n <span class="keyword">in</span> random_sentence[<span class="number">2</span>].split()]])</span><br><span class="line">    <span class="comment"># 将输入、输出和目标批次转换为 LongTensor</span></span><br><span class="line">    encoder_input = torch.LongTensor(encoder_input)</span><br><span class="line">    decoder_input = torch.LongTensor(decoder_input)</span><br><span class="line">    target = torch.LongTensor(target)</span><br><span class="line">    <span class="keyword">return</span> encoder_input, decoder_input, target </span><br><span class="line"><span class="comment"># 使用 make_data 函数生成输入、输出和目标张量</span></span><br><span class="line">encoder_input, decoder_input, target = make_data(sentences)</span><br><span class="line"><span class="keyword">for</span> s <span class="keyword">in</span> sentences: <span class="comment"># 获取原始句子</span></span><br><span class="line">    <span class="keyword">if</span> <span class="built_in">all</span>([word2idx_cn[w] <span class="keyword">in</span> encoder_input[<span class="number">0</span>] <span class="keyword">for</span> w <span class="keyword">in</span> s[<span class="number">0</span>].split()]):</span><br><span class="line">        original_sentence = s</span><br><span class="line">        <span class="keyword">break</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot; 原始句子：&quot;</span>, original_sentence) <span class="comment"># 打印原始句子</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot; 编码器输入张量的形状：&quot;</span>, encoder_input.shape)  <span class="comment"># 打印输入张量形状</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot; 解码器输入张量的形状：&quot;</span>, decoder_input.shape) <span class="comment"># 打印输出张量形状</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot; 目标张量的形状：&quot;</span>, target.shape) <span class="comment"># 打印目标张量形状</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot; 编码器输入张量：&quot;</span>, encoder_input) <span class="comment"># 打印输入张量</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot; 解码器输入张量：&quot;</span>, decoder_input) <span class="comment"># 打印输出张量</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot; 目标张量：&quot;</span>, target) <span class="comment"># 打印目标张量</span></span><br></pre></td></tr></table></figure><p><strong>生成数据</strong>：从一个多语言的语料库中随机挑选一个句子，并将其中的中文、英文输入和目标输出都转换成对应的数字索引张量。</p><p><strong>为神经网络准备</strong>：这种数字化的数据是神经网络处理文本数据时必不可少的，确保模型能理解和训练每个单词。</p><h3 id="1-3-定义编码器和解码器"><a href="#1-3-定义编码器和解码器" class="headerlink" title="1.3 定义编码器和解码器"></a>1.3 定义编码器和解码器</h3><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn <span class="comment"># 导入 torch.nn 库</span></span><br><span class="line"><span class="comment"># 定义编码器类，继承自 nn.Module</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Encoder</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, input_size, hidden_size</span>):</span><br><span class="line">        <span class="built_in">super</span>(Encoder, self).__init__()       </span><br><span class="line">        self.hidden_size = hidden_size <span class="comment"># 设置隐藏层大小       </span></span><br><span class="line">        self.embedding = nn.Embedding(input_size, hidden_size) <span class="comment"># 创建词嵌入层       </span></span><br><span class="line">        self.rnn = nn.RNN(hidden_size, hidden_size, batch_first=<span class="literal">True</span>) <span class="comment"># 创建 RNN 层    </span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, inputs, hidden</span>): <span class="comment"># 前向传播函数</span></span><br><span class="line">        embedded = self.embedding(inputs) <span class="comment"># 将输入转换为嵌入向量       </span></span><br><span class="line">        output, hidden = self.rnn(embedded, hidden) <span class="comment"># 将嵌入向量输入 RNN 层并获取输出</span></span><br><span class="line">        <span class="keyword">return</span> output, hidden</span><br><span class="line"><span class="comment"># 定义解码器类，继承自 nn.Module</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Decoder</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, hidden_size, output_size</span>):</span><br><span class="line">        <span class="built_in">super</span>(Decoder, self).__init__()       </span><br><span class="line">        self.hidden_size = hidden_size <span class="comment"># 设置隐藏层大小       </span></span><br><span class="line">        self.embedding = nn.Embedding(output_size, hidden_size) <span class="comment"># 创建词嵌入层</span></span><br><span class="line">        self.rnn = nn.RNN(hidden_size, hidden_size, batch_first=<span class="literal">True</span>)  <span class="comment"># 创建 RNN 层       </span></span><br><span class="line">        self.out = nn.Linear(hidden_size, output_size) <span class="comment"># 创建线性输出层    </span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, inputs, hidden</span>):  <span class="comment"># 前向传播函数     </span></span><br><span class="line">        embedded = self.embedding(inputs) <span class="comment"># 将输入转换为嵌入向量       </span></span><br><span class="line">        output, hidden = self.rnn(embedded, hidden) <span class="comment"># 将嵌入向量输入 RNN 层并获取输出       </span></span><br><span class="line">        output = self.out(output) <span class="comment"># 使用线性层生成最终输出</span></span><br><span class="line">        <span class="keyword">return</span> output, hidden</span><br><span class="line">n_hidden = <span class="number">128</span> <span class="comment"># 设置隐藏层数量</span></span><br><span class="line"><span class="comment"># 创建编码器和解码器</span></span><br><span class="line">encoder = Encoder(voc_size_cn, n_hidden)</span><br><span class="line">decoder = Decoder(n_hidden, voc_size_en)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27; 编码器结构：&#x27;</span>, encoder)  <span class="comment"># 打印编码器的结构</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27; 解码器结构：&#x27;</span>, decoder)  <span class="comment"># 打印解码器的结构</span></span><br></pre></td></tr></table></figure><p><strong>编码器</strong> 负责将中文句子转换为一个隐藏状态，这个隐藏状态包含了整个句子的“精华信息”。</p><p><strong>解码器</strong> 接受这个隐藏状态，并利用自己的 RNN 层一步步生成英文翻译，直到输出完整的翻译句子。</p><p><strong>整体流程</strong> 就像两个翻译大师默契合作：一个把中文理解得透彻，另一个根据理解生成英文表述。</p><h3 id="1-4-定义seq2seq架构"><a href="#1-4-定义seq2seq架构" class="headerlink" title="1.4 定义seq2seq架构"></a>1.4 定义seq2seq架构</h3><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Seq2Seq</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, encoder, decoder</span>):</span><br><span class="line">        <span class="built_in">super</span>(Seq2Seq, self).__init__()</span><br><span class="line">        <span class="comment"># 初始化编码器和解码器</span></span><br><span class="line">        self.encoder = encoder</span><br><span class="line">        self.decoder = decoder</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, enc_input, hidden, dec_input</span>):    <span class="comment"># 定义前向传播函数</span></span><br><span class="line">        <span class="comment"># 使输入序列通过编码器并获取输出和隐藏状态</span></span><br><span class="line">        encoder_output, encoder_hidden = self.encoder(enc_input, hidden)</span><br><span class="line">        <span class="comment"># 将编码器的隐藏状态传递给解码器作为初始隐藏状态</span></span><br><span class="line">        decoder_hidden = encoder_hidden</span><br><span class="line">        <span class="comment"># 使解码器输入（目标序列）通过解码器并获取输出</span></span><br><span class="line">        decoder_output, _ = self.decoder(dec_input, decoder_hidden)</span><br><span class="line">        <span class="keyword">return</span> decoder_output</span><br><span class="line"><span class="comment"># 创建 Seq2Seq 架构</span></span><br><span class="line">model = Seq2Seq(encoder, decoder)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;S2S 模型结构：&#x27;</span>, model)  <span class="comment"># 打印模型的结构</span></span><br></pre></td></tr></table></figure><p><strong>整体流程</strong>：</p><ol><li><strong>初始化</strong>：将编码器和解码器组合成一个 Seq2Seq 模型。</li><li><strong>编码过程</strong>：输入句子经过编码器处理，生成隐藏状态（记忆）。</li><li><strong>解码过程</strong>：解码器以编码器的隐藏状态为起点，结合目标序列（通常带 <code>&lt;sos&gt;</code>）逐步生成输出。</li><li><strong>输出</strong>：最终返回解码器生成的输出，这通常用来计算损失或者进行实际翻译。</li></ol><h3 id="1-5-训练seq2seq架构"><a href="#1-5-训练seq2seq架构" class="headerlink" title="1.5 训练seq2seq架构"></a>1.5 训练seq2seq架构</h3><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 定义训练函数</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">train_seq2seq</span>(<span class="params">model, criterion, optimizer, epochs</span>):</span><br><span class="line">    <span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(epochs):</span><br><span class="line">       encoder_input, decoder_input, target = make_data(sentences) <span class="comment"># 训练数据的创建</span></span><br><span class="line">       hidden = torch.zeros(<span class="number">1</span>, encoder_input.size(<span class="number">0</span>), n_hidden) <span class="comment"># 初始化隐藏状态      </span></span><br><span class="line">       optimizer.zero_grad()<span class="comment"># 梯度清零        </span></span><br><span class="line">       output = model(encoder_input, hidden, decoder_input) <span class="comment"># 获取模型输出        </span></span><br><span class="line">       loss = criterion(output.view(-<span class="number">1</span>, voc_size_en), target.view(-<span class="number">1</span>)) <span class="comment"># 计算损失        </span></span><br><span class="line">       <span class="keyword">if</span> (epoch + <span class="number">1</span>) % <span class="number">40</span> == <span class="number">0</span>: <span class="comment"># 打印损失</span></span><br><span class="line">          <span class="built_in">print</span>(<span class="string">f&quot;Epoch: <span class="subst">&#123;epoch + <span class="number">1</span>:04d&#125;</span> cost = <span class="subst">&#123;loss:<span class="number">.6</span>f&#125;</span>&quot;</span>)         </span><br><span class="line">       loss.backward()<span class="comment"># 反向传播        </span></span><br><span class="line">       optimizer.step()<span class="comment"># 更新参数</span></span><br><span class="line"><span class="comment"># 训练模型</span></span><br><span class="line">epochs = <span class="number">400</span> <span class="comment"># 训练轮次</span></span><br><span class="line">criterion = nn.CrossEntropyLoss() <span class="comment"># 损失函数</span></span><br><span class="line">optimizer = torch.optim.Adam(model.parameters(), lr=<span class="number">0.001</span>) <span class="comment"># 优化器</span></span><br><span class="line">train_seq2seq(model, criterion, optimizer, epochs) <span class="comment"># 调用函数训练模型</span></span><br></pre></td></tr></table></figure><p>每一轮，模型从语料库中随机抽取一个句子，用编码器“消化”输入，再由解码器“烹饪”输出；计算损失后，反向传播让模型不断改进。</p><h3 id="1-6-测试seq2seq架构"><a href="#1-6-测试seq2seq架构" class="headerlink" title="1.6 测试seq2seq架构"></a>1.6 测试seq2seq架构</h3><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 定义测试函数</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">test_seq2seq</span>(<span class="params">model, source_sentence</span>):</span><br><span class="line">    <span class="comment"># 将输入的句子转换为索引</span></span><br><span class="line">    encoder_input = np.array([[word2idx_cn[n] <span class="keyword">for</span> n <span class="keyword">in</span> source_sentence.split()]])</span><br><span class="line">    <span class="comment"># 构建输出的句子的索引，以 &#x27;&lt;sos&gt;&#x27; 开始，后面跟 &#x27;&lt;eos&gt;&#x27;，长度与输入句子相同</span></span><br><span class="line">    decoder_input = np.array([word2idx_en[<span class="string">&#x27;&lt;sos&gt;&#x27;</span>]] + [word2idx_en[<span class="string">&#x27;&lt;eos&gt;&#x27;</span>]]*(<span class="built_in">len</span>(encoder_input[<span class="number">0</span>])-<span class="number">1</span>))</span><br><span class="line">    <span class="comment"># 转换为 LongTensor 类型</span></span><br><span class="line">    encoder_input = torch.LongTensor(encoder_input)</span><br><span class="line">    decoder_input = torch.LongTensor(decoder_input).unsqueeze(<span class="number">0</span>) <span class="comment"># 增加一维    </span></span><br><span class="line">    hidden = torch.zeros(<span class="number">1</span>, encoder_input.size(<span class="number">0</span>), n_hidden) <span class="comment"># 初始化隐藏状态    </span></span><br><span class="line">    predict = model(encoder_input, hidden, decoder_input) <span class="comment"># 获取模型输出    </span></span><br><span class="line">    predict = predict.data.<span class="built_in">max</span>(<span class="number">2</span>, keepdim=<span class="literal">True</span>)[<span class="number">1</span>] <span class="comment"># 获取概率最大的索引</span></span><br><span class="line">    <span class="comment"># 打印输入的句子和预测的句子</span></span><br><span class="line">    <span class="built_in">print</span>(source_sentence, <span class="string">&#x27;-&gt;&#x27;</span>, [idx2word_en[n.item()] <span class="keyword">for</span> n <span class="keyword">in</span> predict.squeeze()])</span><br><span class="line"><span class="comment"># 测试模型</span></span><br><span class="line">test_seq2seq(model, <span class="string">&#x27;咖哥 喜欢 小冰&#x27;</span>)  </span><br><span class="line">test_seq2seq(model, <span class="string">&#x27;自然 语言 处理 很 强大&#x27;</span>)</span><br></pre></td></tr></table></figure><p><strong>整体流程</strong>：</p><ol><li>输入句子转换为数字索引。</li><li>为解码器构造一个“启动提示”，包含 <code>&lt;sos&gt;</code> 和 <code>&lt;eos&gt;</code>。</li><li>初始化必要的张量和隐藏状态。</li><li>使用训练好的 Seq2Seq 模型生成预测输出。</li><li>将预测结果转换回单词并打印出来。</li></ol>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;Seq2Seq：让机器学会”同声传译”的魔法架构&quot;&gt;&lt;a href=&quot;#Seq2Seq：让机器学会”同声传译”的魔法架构&quot; class=&quot;headerlink&quot; title=&quot;Seq2Seq：让机器学会”同声传译”的魔法架构&quot;&gt;&lt;/a&gt;Seq2Seq：让机器学会”</summary>
      
    
    
    
    
  </entry>
  
  <entry>
    <title></title>
    <link href="https://rozen12123.github.io/posts/0.html"/>
    <id>https://rozen12123.github.io/posts/0.html</id>
    <published>2025-02-20T13:05:40.886Z</published>
    <updated>2025-03-08T12:45:54.129Z</updated>
    
    <content type="html"><![CDATA[<h1 id="图解Word2Vec：如何让AI真正“读懂”人类语言？"><a href="#图解Word2Vec：如何让AI真正“读懂”人类语言？" class="headerlink" title="图解Word2Vec：如何让AI真正“读懂”人类语言？"></a>图解Word2Vec：如何让AI真正“读懂”人类语言？</h1><p><img src="https://images.unsplash.com/photo-1501504905252-473c47e087f8?ixlib=rb-1.2.1&amp;auto=format&amp;fit=crop&amp;w=1350&amp;q=80" alt="词向量魔法"></p><h2 id="一、从”文字游戏”到”语义地图”：词向量革命"><a href="#一、从”文字游戏”到”语义地图”：词向量革命" class="headerlink" title="一、从”文字游戏”到”语义地图”：词向量革命"></a>一、从”文字游戏”到”语义地图”：词向量革命</h2><blockquote><p>“国王 - 男人 + 女人 = 女王”<br> 这个震撼NLP界的经典公式，揭开了词向量的神秘面纱</p></blockquote><p>传统文本处理就像让AI玩”文字连连看”：机械统计词频、匹配固定规则。这种处理方式最大的痛点是什么？看看这个例子：</p><p><strong>病例报告</strong><br> 患者：”医生，我最近总是心慌、手抖、容易饿”<br> 传统AI诊断：发现”心”出现3次，”手”出现2次 → 重点检查心血管系统<br> 实际诊断：甲状腺功能亢进</p><p>这个真实的医疗AI误诊案例，暴露了传统文本处理的致命缺陷——只见树木不见森林。而词向量的诞生，让AI第一次拥有了理解词语深层含义的能力。</p><h2 id="二、Word2Vec的魔法原理（附代码实战）"><a href="#二、Word2Vec的魔法原理（附代码实战）" class="headerlink" title="二、Word2Vec的魔法原理（附代码实战）"></a>二、Word2Vec的魔法原理（附代码实战）</h2><h3 id="2-1-从”填空题”到”完形填空”：两大核心模型"><a href="#2-1-从”填空题”到”完形填空”：两大核心模型" class="headerlink" title="2.1 从”填空题”到”完形填空”：两大核心模型"></a>2.1 从”填空题”到”完形填空”：两大核心模型</h3><p><img src="https://rozen-picture.oss-cn-beijing.aliyuncs.com/img/202502201617548.png" alt="image-20250220161704458"></p><ul><li><strong>CBOW模型</strong>：像做填空题<br> 输入：四周的词语（如”猫 会 __ 老鼠”）<br> 输出：预测中间词（”抓”）</li><li><strong>Skip-Gram模型</strong>：像完形填空<br> 输入：中心词（如”人工智能”）<br> 输出：预测周围词（”深度学习”、”算法”、”大数据”）</li></ul><p><img src="https://rozen-picture.oss-cn-beijing.aliyuncs.com/img/202502201617480.png" alt="image-20250220161715394"></p><h3 id="2-2-Skip-Gram模型的代码实现"><a href="#2-2-Skip-Gram模型的代码实现" class="headerlink" title="2.2 Skip-Gram模型的代码实现"></a>2.2 <strong>Skip-Gram</strong>模型的代码实现</h3><p><img src="https://rozen-picture.oss-cn-beijing.aliyuncs.com/img/202502202019704.png" alt="image-20250220201802253"></p><h4 id="2-2-1-构建实验语料库"><a href="#2-2-1-构建实验语料库" class="headerlink" title="2.2.1 构建实验语料库"></a>2.2.1 构建实验语料库</h4><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 定义一个句子列表，后面会用这些句子来训练 CBOW 和 Skip-Gram 模型</span></span><br><span class="line">sentences = [<span class="string">&quot;Kage is Teacher&quot;</span>, <span class="string">&quot;Mazong is Boss&quot;</span>, <span class="string">&quot;Niuzong is Boss&quot;</span>,</span><br><span class="line">             <span class="string">&quot;Xiaobing is Student&quot;</span>, <span class="string">&quot;Xiaoxue is Student&quot;</span>,]</span><br><span class="line"><span class="comment"># 将所有句子连接在一起，然后用空格分隔成多个单词</span></span><br><span class="line">words = <span class="string">&#x27; &#x27;</span>.join(sentences).split()</span><br><span class="line"><span class="comment"># 构建词汇表，去除重复的词</span></span><br><span class="line">word_list = <span class="built_in">list</span>(<span class="built_in">set</span>(words))</span><br><span class="line"><span class="comment"># 创建一个字典，将每个词映射到一个唯一的索引</span></span><br><span class="line">word_to_idx = &#123;word: idx <span class="keyword">for</span> idx, word <span class="keyword">in</span> <span class="built_in">enumerate</span>(word_list)&#125;</span><br><span class="line"><span class="comment"># 创建一个字典，将每个索引映射到对应的词</span></span><br><span class="line">idx_to_word = &#123;idx: word <span class="keyword">for</span> idx, word <span class="keyword">in</span> <span class="built_in">enumerate</span>(word_list)&#125;</span><br><span class="line">voc_size = <span class="built_in">len</span>(word_list) <span class="comment"># 计算词汇表的大小</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot; 词汇表：&quot;</span>, word_list) <span class="comment"># 输出词汇表</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot; 词汇到索引的字典：&quot;</span>, word_to_idx) <span class="comment"># 输出词汇到索引的字典</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot; 索引到词汇的字典：&quot;</span>, idx_to_word) <span class="comment"># 输出索引到词汇的字典</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot; 词汇表大小：&quot;</span>, voc_size) <span class="comment"># 输出词汇表大小</span></span><br></pre></td></tr></table></figure><figure class="highlight markdown"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">词汇表： [&#x27;Xiaoxue&#x27;, &#x27;is&#x27;, &#x27;Niuzong&#x27;, &#x27;Student&#x27;, &#x27;Teacher&#x27;, &#x27;Boss&#x27;, &#x27;Mazong&#x27;, &#x27;Xiaobing&#x27;, &#x27;Kage&#x27;]</span><br><span class="line"></span><br><span class="line">词汇到索引的字典： &#123;&#x27;Xiaoxue&#x27;: 0, &#x27;is&#x27;: 1, &#x27;Niuzong&#x27;: 2, &#x27;Student&#x27;: 3, &#x27;Teacher&#x27;: 4, &#x27;Boss&#x27;: 5, &#x27;Mazong&#x27;: 6, &#x27;Xiaobing&#x27;: 7, &#x27;Kage&#x27;: 8&#125;</span><br><span class="line"></span><br><span class="line">索引到词汇的字典： &#123;0: &#x27;Xiaoxue&#x27;, 1: &#x27;is&#x27;, 2: &#x27;Niuzong&#x27;, 3: &#x27;Student&#x27;, 4: &#x27;Teacher&#x27;, 5: &#x27;Boss&#x27;, 6: &#x27;Mazong&#x27;, 7: &#x27;Xiaobing&#x27;, 8: &#x27;Kage&#x27;&#125;</span><br><span class="line"></span><br><span class="line">词汇表大小： 9</span><br></pre></td></tr></table></figure><h4 id="2-2-2-生成Skip-Gram数据"><a href="#2-2-2-生成Skip-Gram数据" class="headerlink" title="2.2.2  生成Skip-Gram数据"></a>2.2.2  生成Skip-Gram数据</h4><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 生成 Skip-Gram 训练数据</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">create_skipgram_dataset</span>(<span class="params">sentences, window_size=<span class="number">2</span></span>):</span><br><span class="line">    data = [] <span class="comment"># 初始化数据</span></span><br><span class="line">    <span class="keyword">for</span> sentence <span class="keyword">in</span> sentences: <span class="comment"># 遍历句子</span></span><br><span class="line">        sentence = sentence.split()  <span class="comment"># 将句子分割成单词列表</span></span><br><span class="line">        <span class="keyword">for</span> idx, word <span class="keyword">in</span> <span class="built_in">enumerate</span>(sentence):  <span class="comment"># 遍历单词及其索引</span></span><br><span class="line">            <span class="comment"># 获取相邻的单词，将当前单词前后各 N 个单词作为相邻单词</span></span><br><span class="line">            <span class="keyword">for</span> neighbor <span class="keyword">in</span> sentence[<span class="built_in">max</span>(idx - window_size, <span class="number">0</span>): </span><br><span class="line">                        <span class="built_in">min</span>(idx + window_size + <span class="number">1</span>, <span class="built_in">len</span>(sentence))]:</span><br><span class="line">                <span class="keyword">if</span> neighbor != word:  <span class="comment"># 排除当前单词本身</span></span><br><span class="line">                    <span class="comment"># 将相邻单词与当前单词作为一组训练数据</span></span><br><span class="line">                    data.append((neighbor, word))</span><br><span class="line">    <span class="keyword">return</span> data</span><br><span class="line"><span class="comment"># 使用函数创建 Skip-Gram 训练数据</span></span><br><span class="line">skipgram_data = create_skipgram_dataset(sentences)</span><br><span class="line"><span class="comment"># 打印未编码的 Skip-Gram 数据样例（前 3 个）</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Skip-Gram 数据样例（未编码）：&quot;</span>, skipgram_data[:<span class="number">3</span>])</span><br></pre></td></tr></table></figure><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Skip-Gram 数据样例（未编码）： [(<span class="string">&#x27;is&#x27;</span>, <span class="string">&#x27;Kage&#x27;</span>), (<span class="string">&#x27;Teacher&#x27;</span>, <span class="string">&#x27;Kage&#x27;</span>), (<span class="string">&#x27;Kage&#x27;</span>, <span class="string">&#x27;is&#x27;</span>)]</span><br></pre></td></tr></table></figure><h4 id="2-2-3-进行One-Hot编码"><a href="#2-2-3-进行One-Hot编码" class="headerlink" title="2.2.3 进行One-Hot编码"></a>2.2.3 进行One-Hot编码</h4><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 定义 One-Hot 编码函数</span></span><br><span class="line"><span class="keyword">import</span> torch <span class="comment"># 导入 torch 库</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">one_hot_encoding</span>(<span class="params">word, word_to_idx</span>):    </span><br><span class="line">    tensor = torch.zeros(<span class="built_in">len</span>(word_to_idx)) <span class="comment"># 创建一个长度与词汇表相同的全 0 张量  </span></span><br><span class="line">    tensor[word_to_idx[word]] = <span class="number">1</span>  <span class="comment"># 将对应词的索引设为 1</span></span><br><span class="line">    <span class="keyword">return</span> tensor  <span class="comment"># 返回生成的 One-Hot 向量</span></span><br><span class="line"><span class="comment"># 展示 One-Hot 编码前后的数据</span></span><br><span class="line">word_example = <span class="string">&quot;Teacher&quot;</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;One-Hot 编码前的单词：&quot;</span>, word_example)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;One-Hot 编码后的向量：&quot;</span>, one_hot_encoding(word_example, word_to_idx))</span><br><span class="line"><span class="comment"># 展示编码后的 Skip-Gram 训练数据样例</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Skip-Gram 数据样例（已编码）：&quot;</span>, [(one_hot_encoding(context, word_to_idx), </span><br><span class="line">          word_to_idx[target]) <span class="keyword">for</span> context, target <span class="keyword">in</span> skipgram_data[:<span class="number">3</span>]])</span><br></pre></td></tr></table></figure><figure class="highlight markdown"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">One-Hot 编码前的单词： Teacher</span><br><span class="line">One-Hot 编码后的向量： tensor([0., 0., 0., 1., 0., 0., 0., 0., 0.])</span><br><span class="line">Skip-Gram 数据样例（已编码）： </span><br><span class="line">[(tensor([0., 0., 0., 0., 1., 0., 0., 0., 0.]), 7), (tensor([0., 0., 0., 1., 0., 0., 0., 0., 0.]), 7), (tensor([0., 0., 0., 0., 0., 0., 0., 1., 0.]), 4)]</span><br></pre></td></tr></table></figure><h4 id="2-2-4-定义Skip-Gram类"><a href="#2-2-4-定义Skip-Gram类" class="headerlink" title="2.2.4 定义Skip-Gram类"></a>2.2.4 定义Skip-Gram类</h4><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 定义 Skip-Gram 类</span></span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn <span class="comment"># 导入 neural network</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">SkipGram</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, voc_size, embedding_size</span>):</span><br><span class="line">        <span class="built_in">super</span>(SkipGram, self).__init__()</span><br><span class="line">        <span class="comment"># 从词汇表大小到嵌入层大小（维度）的线性层（权重矩阵）</span></span><br><span class="line">        self.input_to_hidden = nn.Linear(voc_size, embedding_size, bias=<span class="literal">False</span>)  </span><br><span class="line">        <span class="comment"># 从嵌入层大小（维度）到词汇表大小的线性层（权重矩阵）</span></span><br><span class="line">        self.hidden_to_output = nn.Linear(embedding_size, voc_size, bias=<span class="literal">False</span>)  </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, X</span>): <span class="comment"># 前向传播的方式，X 形状为 (batch_size, voc_size)      </span></span><br><span class="line">         <span class="comment"># 通过隐藏层，hidden 形状为 (batch_size, embedding_size)</span></span><br><span class="line">            hidden = self.input_to_hidden(X) </span><br><span class="line">            <span class="comment"># 通过输出层，output_layer 形状为 (batch_size, voc_size)</span></span><br><span class="line">            output = self.hidden_to_output(hidden)  </span><br><span class="line">            <span class="keyword">return</span> output    </span><br><span class="line">embedding_size = <span class="number">2</span> <span class="comment"># 设定嵌入层的大小，这里选择 2 是为了方便展示</span></span><br><span class="line">skipgram_model = SkipGram(voc_size, embedding_size)  <span class="comment"># 实例化 Skip-Gram 模型</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Skip-Gram 模型：&quot;</span>, skipgram_model)</span><br></pre></td></tr></table></figure><figure class="highlight markdown"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">Skip-Gram 模型： SkipGram(</span><br><span class="line">  (input<span class="emphasis">_to_</span>hidden): Linear(in<span class="emphasis">_features=9, out_</span>features=2, bias=False)</span><br><span class="line">  (hidden<span class="emphasis">_to_</span>output): Linear(in<span class="emphasis">_features=2, out_</span>features=9, bias=False)</span><br><span class="line">)</span><br></pre></td></tr></table></figure><h4 id="2-2-5-训练Skip-Gram"><a href="#2-2-5-训练Skip-Gram" class="headerlink" title="2.2.5  训练Skip-Gram"></a>2.2.5  训练Skip-Gram</h4><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 训练 Skip-Gram 类</span></span><br><span class="line">learning_rate = <span class="number">0.001</span> <span class="comment"># 设置学习速率</span></span><br><span class="line">epochs = <span class="number">1000</span> <span class="comment"># 设置训练轮次</span></span><br><span class="line">criterion = nn.CrossEntropyLoss()  <span class="comment"># 定义交叉熵损失函数</span></span><br><span class="line"><span class="keyword">import</span> torch.optim <span class="keyword">as</span> optim <span class="comment"># 导入随机梯度下降优化器</span></span><br><span class="line">optimizer = optim.SGD(skipgram_model.parameters(), lr=learning_rate)  </span><br><span class="line"><span class="comment"># 开始训练循环</span></span><br><span class="line">loss_values = []  <span class="comment"># 用于存储每轮的平均损失值</span></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(epochs):</span><br><span class="line">    loss_sum = <span class="number">0</span> <span class="comment"># 初始化损失值</span></span><br><span class="line">    <span class="keyword">for</span> context, target <span class="keyword">in</span> skipgram_data:        </span><br><span class="line">        X = one_hot_encoding(target, word_to_idx).<span class="built_in">float</span>().unsqueeze(<span class="number">0</span>) <span class="comment"># 将中心词转换为 One-Hot 向量  </span></span><br><span class="line">        y_true = torch.tensor([word_to_idx[context]], dtype=torch.long) <span class="comment"># 将周围词转换为索引值 </span></span><br><span class="line">        y_pred = skipgram_model(X)  <span class="comment"># 计算预测值</span></span><br><span class="line">        loss = criterion(y_pred, y_true)  <span class="comment"># 计算损失</span></span><br><span class="line">        loss_sum += loss.item() <span class="comment"># 累积损失</span></span><br><span class="line">        optimizer.zero_grad()  <span class="comment"># 清空梯度</span></span><br><span class="line">        loss.backward()  <span class="comment"># 反向传播</span></span><br><span class="line">        optimizer.step()  <span class="comment"># 更新参数</span></span><br><span class="line">    <span class="keyword">if</span> (epoch+<span class="number">1</span>) % <span class="number">100</span> == <span class="number">0</span>: <span class="comment"># 输出每 100 轮的损失，并记录损失</span></span><br><span class="line">      <span class="built_in">print</span>(<span class="string">f&quot;Epoch: <span class="subst">&#123;epoch+<span class="number">1</span>&#125;</span>, Loss: <span class="subst">&#123;loss_sum/<span class="built_in">len</span>(skipgram_data)&#125;</span>&quot;</span>)  </span><br><span class="line">      loss_values.append(loss_sum / <span class="built_in">len</span>(skipgram_data))</span><br><span class="line"><span class="comment"># 绘制训练损失曲线</span></span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt <span class="comment"># 导入 matplotlib</span></span><br><span class="line"><span class="comment"># 绘制二维词向量图</span></span><br><span class="line">plt.rcParams[<span class="string">&quot;font.family&quot;</span>]=[<span class="string">&#x27;SimHei&#x27;</span>] <span class="comment"># 用来设定字体样式</span></span><br><span class="line">plt.rcParams[<span class="string">&#x27;font.sans-serif&#x27;</span>]=[<span class="string">&#x27;SimHei&#x27;</span>] <span class="comment"># 用来设定无衬线字体样式</span></span><br><span class="line">plt.rcParams[<span class="string">&#x27;axes.unicode_minus&#x27;</span>]=<span class="literal">False</span> <span class="comment"># 用来正常显示负号</span></span><br><span class="line">plt.plot(<span class="built_in">range</span>(<span class="number">1</span>, epochs//<span class="number">100</span> + <span class="number">1</span>), loss_values) <span class="comment"># 绘图</span></span><br><span class="line">plt.title(<span class="string">&#x27; 训练损失曲线 &#x27;</span>) <span class="comment"># 图题</span></span><br><span class="line">plt.xlabel(<span class="string">&#x27; 轮次 &#x27;</span>) <span class="comment"># X 轴 Label</span></span><br><span class="line">plt.ylabel(<span class="string">&#x27; 损失 &#x27;</span>) <span class="comment"># Y 轴 Label</span></span><br><span class="line">plt.show() <span class="comment"># 显示图</span></span><br></pre></td></tr></table></figure><figure class="highlight markdown"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">Epoch: 100, Loss: 2.176283597946167</span><br><span class="line">Epoch: 200, Loss: 2.1313777963320413</span><br><span class="line">Epoch: 300, Loss: 2.079272961616516</span><br><span class="line">Epoch: 400, Loss: 2.0179983615875243</span><br><span class="line">Epoch: 500, Loss: 1.956022528807322</span><br><span class="line">Epoch: 600, Loss: 1.9065291663010915</span><br><span class="line">Epoch: 700, Loss: 1.8729750255743662</span><br><span class="line">Epoch: 800, Loss: 1.8494429806868236</span><br><span class="line">Epoch: 900, Loss: 1.8303062697251637</span><br><span class="line">Epoch: 1000, Loss: 1.812800904115041</span><br></pre></td></tr></table></figure><p><img src="https://rozen-picture.oss-cn-beijing.aliyuncs.com/img/202502202031327.png" alt="image-20250220203128215">&nbsp;</p><h4 id="2-2-6-展示词向量"><a href="#2-2-6-展示词向量" class="headerlink" title="2.2.6 展示词向量"></a>2.2.6 展示词向量</h4><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 输出 Skip-Gram 习得的词嵌入</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Skip-Gram 词嵌入：&quot;</span>)</span><br><span class="line"><span class="keyword">for</span> word, idx <span class="keyword">in</span> word_to_idx.items(): <span class="comment"># 输出每个词的嵌入向量</span></span><br><span class="line"> <span class="built_in">print</span>(<span class="string">f&quot;<span class="subst">&#123;word&#125;</span>: <span class="subst">&#123;skipgram_model.input_to_hidden.weight[:,idx].detach().numpy()&#125;</span>&quot;</span>)</span><br></pre></td></tr></table></figure><figure class="highlight markdown"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">Xiaobing: [-1.3628563 -2.1293848]</span><br><span class="line">Xiaoxue: [-1.3693085 -2.1389563]</span><br><span class="line">Boss: [ 2.923863 -0.4184679]</span><br><span class="line">Student: [-0.09255204 -0.8242733 ]</span><br><span class="line">is: [-0.23261149 0.29151806]</span><br><span class="line">Kage: [-0.3542828 -0.9870443]</span><br><span class="line">Niuzong: [ 0.8161409 -0.624454 ]</span><br><span class="line">Mazong: [ 0.821509 -0.62387395]</span><br><span class="line">Teacher: [ 0.8520589 -0.47847477]</span><br></pre></td></tr></table></figure><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">fig, ax = plt.subplots() </span><br><span class="line"><span class="keyword">for</span> word, idx <span class="keyword">in</span> word_to_idx.items():</span><br><span class="line">    <span class="comment"># 获取每个单词的嵌入向量</span></span><br><span class="line">    vec = skipgram_model.input_to_hidden.weight[:,idx].detach().numpy() </span><br><span class="line">    ax.scatter(vec[<span class="number">0</span>], vec[<span class="number">1</span>]) <span class="comment"># 在图中绘制嵌入向量的点</span></span><br><span class="line">    ax.annotate(word, (vec[<span class="number">0</span>], vec[<span class="number">1</span>]), fontsize=<span class="number">12</span>) <span class="comment"># 点旁添加单词标签</span></span><br><span class="line">plt.title(<span class="string">&#x27; 二维词嵌入 &#x27;</span>) <span class="comment"># 图题</span></span><br><span class="line">plt.xlabel(<span class="string">&#x27; 向量维度 1&#x27;</span>) <span class="comment"># X 轴 Label</span></span><br><span class="line">plt.ylabel(<span class="string">&#x27; 向量维度 2&#x27;</span>) <span class="comment"># Y 轴 Label</span></span><br><span class="line">plt.show() <span class="comment"># 显示图</span></span><br></pre></td></tr></table></figure><p><img src="https://rozen-picture.oss-cn-beijing.aliyuncs.com/img/202502202036987.png" alt="image-20250220203624875"></p><p>总结⼀下：我们使⽤PyTorch实现了⼀个简单的Word2Vec（这⾥是Skip-Gram）模型。</p><p>模型包括输⼊层、隐藏层和输出层。输⼊层接收周围词（以One-Hot编码后的向量形式表示）。接下来，输⼊层到隐藏层的权重矩阵（记为input_to_hidden）将这个向量转换为词嵌⼊，该词嵌⼊直接作为隐藏层的输出。</p><p>隐藏层到输出层的权重矩阵（记为hidden_to_output）将隐藏层的</p><p>输出转换为⼀个概率分布，⽤于预测与周围词相关的中⼼词（以索引形式表示）。通过最⼩化预测词和实际⽬标词之间的分类交叉熵损失，可以学习词嵌⼊向量。下图展示了这个流程。</p><p><img src="https://rozen-picture.oss-cn-beijing.aliyuncs.com/img/202502202040392.png" alt="image-20250220204011257"></p><h3 id="2-3-CBOW模型的代码实现"><a href="#2-3-CBOW模型的代码实现" class="headerlink" title="2.3 CBOW模型的代码实现"></a>2.3 CBOW模型的代码实现</h3><p>CBOW模型与Skip-Gram模型相反，其主要任务是根据给定的周围词来预测中⼼词。</p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 定义一个句子列表，后面会用这些句子来训练 CBOW 和 Skip-Gram 模型</span></span><br><span class="line">sentences = [<span class="string">&quot;Kage is Teacher&quot;</span>, <span class="string">&quot;Mazong is Boss&quot;</span>, <span class="string">&quot;Niuzong is Boss&quot;</span>,</span><br><span class="line">             <span class="string">&quot;Xiaobing is Student&quot;</span>, <span class="string">&quot;Xiaoxue is Student&quot;</span>,]</span><br><span class="line"><span class="comment"># 将所有句子连接在一起，然后用空格分隔成多个单词</span></span><br><span class="line">words = <span class="string">&#x27; &#x27;</span>.join(sentences).split()</span><br><span class="line"><span class="comment"># 构建词汇表，去除重复的词</span></span><br><span class="line">word_list = <span class="built_in">list</span>(<span class="built_in">set</span>(words))</span><br><span class="line"><span class="comment"># 创建一个字典，将每个词映射到一个唯一的索引</span></span><br><span class="line">word_to_idx = &#123;word: idx <span class="keyword">for</span> idx, word <span class="keyword">in</span> <span class="built_in">enumerate</span>(word_list)&#125;</span><br><span class="line"><span class="comment"># 创建一个字典，将每个索引映射到对应的词</span></span><br><span class="line">idx_to_word = &#123;idx: word <span class="keyword">for</span> idx, word <span class="keyword">in</span> <span class="built_in">enumerate</span>(word_list)&#125;</span><br><span class="line">voc_size = <span class="built_in">len</span>(word_list) <span class="comment"># 计算词汇表的大小</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot; 词汇表：&quot;</span>, word_list) <span class="comment"># 输出词汇表</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot; 词汇到索引的字典：&quot;</span>, word_to_idx) <span class="comment"># 输出词汇到索引的字典</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot; 索引到词汇的字典：&quot;</span>, idx_to_word) <span class="comment"># 输出索引到词汇的字典</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot; 词汇表大小：&quot;</span>, voc_size) <span class="comment"># 输出词汇表大小</span></span><br></pre></td></tr></table></figure><figure class="highlight markdown"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">词汇表： [&#x27;Boss&#x27;, &#x27;Niuzong&#x27;, &#x27;Mazong&#x27;, &#x27;Teacher&#x27;, &#x27;is&#x27;, &#x27;Xiaobing&#x27;, &#x27;Student&#x27;, &#x27;Kage&#x27;, &#x27;Xiaoxue&#x27;]</span><br><span class="line">词汇到索引的字典： &#123;&#x27;Boss&#x27;: 0, &#x27;Niuzong&#x27;: 1, &#x27;Mazong&#x27;: 2, &#x27;Teacher&#x27;: 3, &#x27;is&#x27;: 4, &#x27;Xiaobing&#x27;: 5, &#x27;Student&#x27;: 6, &#x27;Kage&#x27;: 7, &#x27;Xiaoxue&#x27;: 8&#125;</span><br><span class="line">索引到词汇的字典： &#123;0: &#x27;Boss&#x27;, 1: &#x27;Niuzong&#x27;, 2: &#x27;Mazong&#x27;, 3: &#x27;Teacher&#x27;, 4: &#x27;is&#x27;, 5: &#x27;Xiaobing&#x27;, 6: &#x27;Student&#x27;, 7: &#x27;Kage&#x27;, 8: &#x27;Xiaoxue&#x27;&#125;</span><br><span class="line">词汇表大小： 9</span><br></pre></td></tr></table></figure><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 生成 CBOW 训练数据</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">create_cbow_dataset</span>(<span class="params">sentences, window_size=<span class="number">2</span></span>):</span><br><span class="line">    data = []<span class="comment"># 初始化数据</span></span><br><span class="line">    <span class="keyword">for</span> sentence <span class="keyword">in</span> sentences:</span><br><span class="line">        sentence = sentence.split()  <span class="comment"># 将句子分割成单词列表</span></span><br><span class="line">        <span class="keyword">for</span> idx, word <span class="keyword">in</span> <span class="built_in">enumerate</span>(sentence):  <span class="comment"># 遍历单词及其索引</span></span><br><span class="line">            <span class="comment"># 获取上下文词汇，将当前单词前后各 window_size 个单词作为周围词</span></span><br><span class="line">            context_words = sentence[<span class="built_in">max</span>(idx - window_size, <span class="number">0</span>):idx] \</span><br><span class="line">                + sentence[idx + <span class="number">1</span>:<span class="built_in">min</span>(idx + window_size + <span class="number">1</span>, <span class="built_in">len</span>(sentence))]</span><br><span class="line">            <span class="comment"># 将当前单词与上下文词汇作为一组训练数据</span></span><br><span class="line">            data.append((word, context_words))</span><br><span class="line">    <span class="keyword">return</span> data</span><br><span class="line"><span class="comment"># 使用函数创建 CBOW 训练数据</span></span><br><span class="line">cbow_data = create_cbow_dataset(sentences)</span><br><span class="line"><span class="comment"># 打印未编码的 CBOW 数据样例（前三个）</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;CBOW 数据样例（未编码）：&quot;</span>, cbow_data[:<span class="number">3</span>])</span><br></pre></td></tr></table></figure><figure class="highlight markdown"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">CBOW 数据样例（未编码）： [(&#x27;Kage&#x27;, [&#x27;is&#x27;, &#x27;Teacher&#x27;]), (&#x27;is&#x27;, [&#x27;Kage&#x27;, &#x27;Teacher&#x27;]), (&#x27;Teacher&#x27;, [&#x27;Kage&#x27;, &#x27;is&#x27;])]</span><br></pre></td></tr></table></figure><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 定义 One-Hot 编码函数</span></span><br><span class="line"><span class="keyword">import</span> torch <span class="comment"># 导入 torch 库</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">one_hot_encoding</span>(<span class="params">word, word_to_idx</span>):    </span><br><span class="line">    tensor = torch.zeros(<span class="built_in">len</span>(word_to_idx)) <span class="comment"># 创建一个长度与词汇表相同的全 0 张量  </span></span><br><span class="line">    tensor[word_to_idx[word]] = <span class="number">1</span>  <span class="comment"># 将对应词的索引设为 1</span></span><br><span class="line">    <span class="keyword">return</span> tensor  <span class="comment"># 返回生成的 One-Hot 向量</span></span><br><span class="line"><span class="comment"># 展示 One-Hot 编码前后的数据</span></span><br><span class="line">word_example = <span class="string">&quot;Teacher&quot;</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;One-Hot 编码前的单词：&quot;</span>, word_example)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;One-Hot 编码后的向量：&quot;</span>, one_hot_encoding(word_example, word_to_idx))</span><br></pre></td></tr></table></figure><figure class="highlight markdown"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">One-Hot 编码前的单词： Teacher</span><br><span class="line">One-Hot 编码后的向量： tensor([0., 0., 0., 1., 0., 0., 0., 0., 0.])</span><br></pre></td></tr></table></figure><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 定义 CBOW 模型</span></span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn <span class="comment"># 导入 neural network</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">CBOW</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, voc_size, embedding_size</span>):</span><br><span class="line">        <span class="built_in">super</span>(CBOW, self).__init__()</span><br><span class="line">        <span class="comment"># 从词汇表大小到嵌入大小的线性层（权重矩阵）</span></span><br><span class="line">        self.input_to_hidden = nn.Linear(voc_size, </span><br><span class="line">                                         embedding_size, bias=<span class="literal">False</span>)  </span><br><span class="line">        <span class="comment"># 从嵌入大小到词汇表大小的线性层（权重矩阵）</span></span><br><span class="line">        self.hidden_to_output = nn.Linear(embedding_size, </span><br><span class="line">                                          voc_size, bias=<span class="literal">False</span>)  </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, X</span>): <span class="comment"># X: [num_context_words, voc_size]</span></span><br><span class="line">        <span class="comment"># 生成嵌入：[num_context_words, embedding_size]</span></span><br><span class="line">        embeddings = self.input_to_hidden(X)  </span><br><span class="line">        <span class="comment"># 计算隐藏层，求嵌入的均值：[embedding_size]</span></span><br><span class="line">        hidden_layer = torch.mean(embeddings, dim=<span class="number">0</span>)  </span><br><span class="line">        <span class="comment"># 生成输出层：[1, voc_size]</span></span><br><span class="line">        output_layer = self.hidden_to_output(hidden_layer.unsqueeze(<span class="number">0</span>)) </span><br><span class="line">        <span class="keyword">return</span> output_layer    </span><br><span class="line">embedding_size = <span class="number">2</span> <span class="comment"># 设定嵌入层的大小，这里选择 2 是为了方便展示</span></span><br><span class="line">cbow_model = CBOW(voc_size,embedding_size)  <span class="comment"># 实例化 CBOW 模型</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;CBOW 模型：&quot;</span>, cbow_model)</span><br></pre></td></tr></table></figure><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">CBOW 模型： CBOW(</span><br><span class="line">  (input_to_hidden): Linear(in_features=<span class="number">9</span>, out_features=<span class="number">2</span>, bias=<span class="literal">False</span>)</span><br><span class="line">  (hidden_to_output): Linear(in_features=<span class="number">2</span>, out_features=<span class="number">9</span>, bias=<span class="literal">False</span>)</span><br><span class="line">)</span><br></pre></td></tr></table></figure><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 训练 Skip-Gram 类</span></span><br><span class="line">learning_rate = <span class="number">0.001</span> <span class="comment"># 设置学习速率</span></span><br><span class="line">epochs = <span class="number">1000</span> <span class="comment"># 设置训练轮次</span></span><br><span class="line">criterion = nn.CrossEntropyLoss()  <span class="comment"># 定义交叉熵损失函数</span></span><br><span class="line"><span class="keyword">import</span> torch.optim <span class="keyword">as</span> optim <span class="comment"># 导入随机梯度下降优化器</span></span><br><span class="line">optimizer = optim.SGD(cbow_model.parameters(), lr=learning_rate)  </span><br><span class="line"><span class="comment"># 开始训练循环</span></span><br><span class="line">loss_values = []  <span class="comment"># 用于存储每轮的平均损失值</span></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(epochs):</span><br><span class="line">    loss_sum = <span class="number">0</span> <span class="comment"># 初始化损失值</span></span><br><span class="line">    <span class="keyword">for</span> target, context_words <span class="keyword">in</span> cbow_data:</span><br><span class="line">        <span class="comment"># 将上下文词转换为 One-Hot 向量并堆叠</span></span><br><span class="line">        X = torch.stack([one_hot_encoding(word, word_to_idx) <span class="keyword">for</span> word <span class="keyword">in</span> context_words]).<span class="built_in">float</span>() </span><br><span class="line">        <span class="comment"># 将目标词转换为索引值</span></span><br><span class="line">        y_true = torch.tensor([word_to_idx[target]], dtype=torch.long) </span><br><span class="line">        y_pred = cbow_model(X)  <span class="comment"># 计算预测值</span></span><br><span class="line">        loss = criterion(y_pred, y_true)  <span class="comment"># 计算损失</span></span><br><span class="line">        loss_sum += loss.item() <span class="comment"># 累积损失</span></span><br><span class="line">        optimizer.zero_grad()  <span class="comment"># 清空梯度</span></span><br><span class="line">        loss.backward()  <span class="comment"># 反向传播</span></span><br><span class="line">        optimizer.step()  <span class="comment"># 更新参数</span></span><br><span class="line">    <span class="keyword">if</span> (epoch+<span class="number">1</span>) % <span class="number">100</span> == <span class="number">0</span>: <span class="comment"># 输出每 100 轮的损失，并记录损失</span></span><br><span class="line">      <span class="built_in">print</span>(<span class="string">f&quot;Epoch: <span class="subst">&#123;epoch+<span class="number">1</span>&#125;</span>, Loss: <span class="subst">&#123;loss_sum/<span class="built_in">len</span>(cbow_data)&#125;</span>&quot;</span>)  </span><br><span class="line">      loss_values.append(loss_sum / <span class="built_in">len</span>(cbow_data))</span><br><span class="line"><span class="comment"># 绘制训练损失曲线</span></span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt <span class="comment"># 导入 matplotlib</span></span><br><span class="line"><span class="comment"># 绘制二维词向量图</span></span><br><span class="line">plt.rcParams[<span class="string">&quot;font.family&quot;</span>]=[<span class="string">&#x27;SimHei&#x27;</span>] <span class="comment"># 用来设定字体样式</span></span><br><span class="line">plt.rcParams[<span class="string">&#x27;font.sans-serif&#x27;</span>]=[<span class="string">&#x27;SimHei&#x27;</span>] <span class="comment"># 用来设定无衬线字体样式</span></span><br><span class="line">plt.rcParams[<span class="string">&#x27;axes.unicode_minus&#x27;</span>]=<span class="literal">False</span> <span class="comment"># 用来正常显示负号</span></span><br><span class="line">plt.plot(<span class="built_in">range</span>(<span class="number">1</span>, epochs//<span class="number">100</span> + <span class="number">1</span>), loss_values) <span class="comment"># 绘图</span></span><br><span class="line">plt.title(<span class="string">&#x27; 训练损失曲线 &#x27;</span>) <span class="comment"># 图题</span></span><br><span class="line">plt.xlabel(<span class="string">&#x27; 轮次 &#x27;</span>) <span class="comment"># X 轴 Label</span></span><br><span class="line">plt.ylabel(<span class="string">&#x27; 损失 &#x27;</span>) <span class="comment"># Y 轴 Label</span></span><br><span class="line">plt.show() <span class="comment"># 显示图</span></span><br></pre></td></tr></table></figure><figure class="highlight markdown"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">Epoch: 100, Loss: 2.1371095180511475</span><br><span class="line">Epoch: 200, Loss: 2.114210780461629</span><br><span class="line">Epoch: 300, Loss: 2.0865681091944377</span><br><span class="line">Epoch: 400, Loss: 2.0524648745854694</span><br><span class="line">Epoch: 500, Loss: 2.010000443458557</span><br><span class="line">Epoch: 600, Loss: 1.9573023001352945</span><br><span class="line">Epoch: 700, Loss: 1.893039353688558</span><br><span class="line">Epoch: 800, Loss: 1.817344347635905</span><br><span class="line">Epoch: 900, Loss: 1.7329498807589212</span><br><span class="line">Epoch: 1000, Loss: 1.6456005891164145</span><br></pre></td></tr></table></figure><p><img src="https://rozen-picture.oss-cn-beijing.aliyuncs.com/img/202502202059843.png" alt="download"></p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 输出 Skip-Gram 习得的词嵌入</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Skip-Gram 词嵌入：&quot;</span>)</span><br><span class="line"><span class="keyword">for</span> word, idx <span class="keyword">in</span> word_to_idx.items(): <span class="comment"># 输出每个词的嵌入向量</span></span><br><span class="line"> <span class="built_in">print</span>(<span class="string">f&quot;<span class="subst">&#123;word&#125;</span>: <span class="subst">&#123;cbow_model.input_to_hidden.weight[:,idx].detach().numpy()&#125;</span>&quot;</span>)</span><br></pre></td></tr></table></figure><figure class="highlight markdown"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">Skip-Gram 词嵌入：</span><br><span class="line"></span><br><span class="line">Xiaoxue: [-0.5842088   0.21841691]</span><br><span class="line">is: [ 0.6250008 -0.8186659]</span><br><span class="line">Niuzong: [0.03038938 0.60383385]</span><br><span class="line">Student: [-1.2024668  0.3539113]</span><br><span class="line">Teacher: [-0.37656686  1.0371245 ]</span><br><span class="line">Boss: [-0.3826948   0.89280415]</span><br><span class="line">Mazong: [0.2657739  0.15451309]</span><br><span class="line">Xiaobing: [-0.4118811  0.6344902]</span><br><span class="line">Kage: [0.10943636 0.7646477 ]</span><br></pre></td></tr></table></figure><p><img src="https://rozen-picture.oss-cn-beijing.aliyuncs.com/img/202502202058709.png" alt="download"></p><p><img src="https://rozen-picture.oss-cn-beijing.aliyuncs.com/img/202502202101420.png" alt="image-20250220210131281"></p><h2 id="三、通向AGI的基石：词向量的未来演进"><a href="#三、通向AGI的基石：词向量的未来演进" class="headerlink" title="三、通向AGI的基石：词向量的未来演进"></a>三、通向AGI的基石：词向量的未来演进</h2><p>当GPT-4的1536维词向量遇上多模态学习：</p><ul><li>图像向量：”猫”的向量同时关联图片、叫声、触感</li><li>跨语言向量：中文”龙”与英文”dragon”的语义差异精准呈现</li><li>时空向量：”元宇宙”在不同时期的含义演变轨迹</li></ul><p><strong>技术演进路线</strong>： 传统词向量 → 语境化词向量（ELMo） → 预训练语言模型（BERT/GPT） → 多模态向量</p>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;图解Word2Vec：如何让AI真正“读懂”人类语言？&quot;&gt;&lt;a href=&quot;#图解Word2Vec：如何让AI真正“读懂”人类语言？&quot; class=&quot;headerlink&quot; title=&quot;图解Word2Vec：如何让AI真正“读懂”人类语言？&quot;&gt;&lt;/a&gt;图解Wor</summary>
      
    
    
    
    
  </entry>
  
  <entry>
    <title></title>
    <link href="https://rozen12123.github.io/posts/0.html"/>
    <id>https://rozen12123.github.io/posts/0.html</id>
    <published>2025-02-20T07:36:39.831Z</published>
    <updated>2025-02-20T07:36:39.831Z</updated>
    
    <content type="html"><![CDATA[<h1 id="NLP基石双雄：从N-Gram到BoW的终极实战指南"><a href="#NLP基石双雄：从N-Gram到BoW的终极实战指南" class="headerlink" title="NLP基石双雄：从N-Gram到BoW的终极实战指南"></a>NLP基石双雄：从N-Gram到BoW的终极实战指南</h1><hr><h2 id="一、N-Gram模型"><a href="#一、N-Gram模型" class="headerlink" title="一、N-Gram模型"></a>一、N-Gram模型</h2><h3 id="1-1创建⼀个Bigram字符预测模型"><a href="#1-1创建⼀个Bigram字符预测模型" class="headerlink" title="1.1创建⼀个Bigram字符预测模型"></a>1.1创建⼀个Bigram字符预测模型</h3><p><img src="https://rozen-picture.oss-cn-beijing.aliyuncs.com/img/202502201512966.png" alt="image-20250220151120531"></p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 构建一个玩具数据集</span></span><br><span class="line">corpus = [ <span class="string">&quot;我喜欢吃苹果&quot;</span>,</span><br><span class="line">        <span class="string">&quot;我喜欢吃香蕉&quot;</span>,</span><br><span class="line">        <span class="string">&quot;她喜欢吃葡萄&quot;</span>,</span><br><span class="line">        <span class="string">&quot;他不喜欢吃香蕉&quot;</span>,</span><br><span class="line">        <span class="string">&quot;他喜欢吃苹果&quot;</span>,</span><br><span class="line">        <span class="string">&quot;她喜欢吃草莓&quot;</span>]</span><br></pre></td></tr></table></figure><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 定义一个分词函数，将文本转换为单个字符的列表</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">tokenize</span>(<span class="params">text</span>):</span><br><span class="line"> <span class="keyword">return</span> [char <span class="keyword">for</span> char <span class="keyword">in</span> text] <span class="comment"># 将文本拆分为字符列表</span></span><br><span class="line"><span class="comment"># 对每个文本进行分词，并打印出对应的单字列表</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;单字列表:&quot;</span>) </span><br><span class="line"><span class="keyword">for</span> text <span class="keyword">in</span> corpus:</span><br><span class="line">    tokens = tokenize(text)</span><br><span class="line">    <span class="built_in">print</span>(tokens)</span><br></pre></td></tr></table></figure><figure class="highlight markdown"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">单字列表:</span><br><span class="line">[&#x27;我&#x27;, &#x27;喜&#x27;, &#x27;欢&#x27;, &#x27;吃&#x27;, &#x27;苹&#x27;, &#x27;果&#x27;]</span><br><span class="line">[&#x27;我&#x27;, &#x27;喜&#x27;, &#x27;欢&#x27;, &#x27;吃&#x27;, &#x27;香&#x27;, &#x27;蕉&#x27;]</span><br><span class="line">[&#x27;她&#x27;, &#x27;喜&#x27;, &#x27;欢&#x27;, &#x27;吃&#x27;, &#x27;葡&#x27;, &#x27;萄&#x27;]</span><br><span class="line">[&#x27;他&#x27;, &#x27;不&#x27;, &#x27;喜&#x27;, &#x27;欢&#x27;, &#x27;吃&#x27;, &#x27;香&#x27;, &#x27;蕉&#x27;]</span><br><span class="line">[&#x27;他&#x27;, &#x27;喜&#x27;, &#x27;欢&#x27;, &#x27;吃&#x27;, &#x27;苹&#x27;, &#x27;果&#x27;]</span><br><span class="line">[&#x27;她&#x27;, &#x27;喜&#x27;, &#x27;欢&#x27;, &#x27;吃&#x27;, &#x27;草&#x27;, &#x27;莓&#x27;]</span><br></pre></td></tr></table></figure><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 定义计算 N-Gram 词频的函数</span></span><br><span class="line"><span class="keyword">from</span> collections <span class="keyword">import</span> defaultdict, Counter <span class="comment"># 导入所需库</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">count_ngrams</span>(<span class="params">corpus, n</span>):</span><br><span class="line">    ngrams_count = defaultdict(Counter)  <span class="comment"># 创建一个字典，存储 N-Gram 计数</span></span><br><span class="line">    <span class="keyword">for</span> text <span class="keyword">in</span> corpus:  <span class="comment"># 遍历语料库中的每个文本</span></span><br><span class="line">        tokens = tokenize(text)  <span class="comment"># 对文本进行分词</span></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(tokens) - n + <span class="number">1</span>):  <span class="comment"># 遍历分词结果，生成 N-Gram</span></span><br><span class="line">            ngram = <span class="built_in">tuple</span>(tokens[i:i+n])  <span class="comment"># 创建一个 N-Gram 元组</span></span><br><span class="line">            prefix = ngram[:-<span class="number">1</span>]  <span class="comment"># 获取 N-Gram 的前缀</span></span><br><span class="line">            token = ngram[-<span class="number">1</span>]  <span class="comment"># 获取 N-Gram 的目标单字</span></span><br><span class="line">            ngrams_count[prefix][token] += <span class="number">1</span>  <span class="comment"># 更新 N-Gram 计数</span></span><br><span class="line">    <span class="keyword">return</span> ngrams_count</span><br><span class="line">bigram_counts = count_ngrams(corpus, <span class="number">2</span>) <span class="comment"># 计算 bigram 词频</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;bigram 词频：&quot;</span>) <span class="comment"># 打印 bigram 词频</span></span><br><span class="line"><span class="keyword">for</span> prefix, counts <span class="keyword">in</span> bigram_counts.items():</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;&#123;&#125;: &#123;&#125;&quot;</span>.<span class="built_in">format</span>(<span class="string">&quot;&quot;</span>.join(prefix), <span class="built_in">dict</span>(counts))) </span><br></pre></td></tr></table></figure><figure class="highlight markdown"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">bigram 词频：</span><br><span class="line">我: &#123;&#x27;喜&#x27;: 2&#125;</span><br><span class="line">喜: &#123;&#x27;欢&#x27;: 6&#125;</span><br><span class="line">欢: &#123;&#x27;吃&#x27;: 6&#125;</span><br><span class="line">吃: &#123;&#x27;苹&#x27;: 2, &#x27;香&#x27;: 2, &#x27;葡&#x27;: 1, &#x27;草&#x27;: 1&#125;</span><br><span class="line">苹: &#123;&#x27;果&#x27;: 2&#125;</span><br><span class="line">香: &#123;&#x27;蕉&#x27;: 2&#125;</span><br><span class="line">她: &#123;&#x27;喜&#x27;: 2&#125;</span><br><span class="line">葡: &#123;&#x27;萄&#x27;: 1&#125;</span><br><span class="line">他: &#123;&#x27;不&#x27;: 1, &#x27;喜&#x27;: 1&#125;</span><br><span class="line">不: &#123;&#x27;喜&#x27;: 1&#125;</span><br><span class="line">草: &#123;&#x27;莓&#x27;: 1&#125;</span><br></pre></td></tr></table></figure><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 定义计算 N-Gram 出现概率的函数</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">ngram_probabilities</span>(<span class="params">ngram_counts</span>):</span><br><span class="line"> ngram_probs = defaultdict(Counter) <span class="comment"># 创建一个字典，存储 N-Gram 出现的概率</span></span><br><span class="line"> <span class="keyword">for</span> prefix, tokens_count <span class="keyword">in</span> ngram_counts.items(): <span class="comment"># 遍历 N-Gram 前缀</span></span><br><span class="line">     total_count = <span class="built_in">sum</span>(tokens_count.values()) <span class="comment"># 计算当前前缀的 N-Gram 计数</span></span><br><span class="line">     <span class="keyword">for</span> token, count <span class="keyword">in</span> tokens_count.items(): <span class="comment"># 遍历每个前缀的 N-Gram</span></span><br><span class="line">         ngram_probs[prefix][token] = count / total_count <span class="comment"># 计算每个 N-Gram 出现的概率</span></span><br><span class="line"> <span class="keyword">return</span> ngram_probs</span><br><span class="line">bigram_probs = ngram_probabilities(bigram_counts) <span class="comment"># 计算 bigram 出现的概率</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;\nbigram 出现的概率 :&quot;</span>) <span class="comment"># 打印 bigram 概率</span></span><br><span class="line"><span class="keyword">for</span> prefix, probs <span class="keyword">in</span> bigram_probs.items():</span><br><span class="line"> <span class="built_in">print</span>(<span class="string">&quot;&#123;&#125;: &#123;&#125;&quot;</span>.<span class="built_in">format</span>(<span class="string">&quot;&quot;</span>.join(prefix), <span class="built_in">dict</span>(probs)))</span><br></pre></td></tr></table></figure><figure class="highlight markdown"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">bigram 出现的概率 :</span><br><span class="line">我: &#123;&#x27;喜&#x27;: 1.0&#125;</span><br><span class="line">喜: &#123;&#x27;欢&#x27;: 1.0&#125;</span><br><span class="line">欢: &#123;&#x27;吃&#x27;: 1.0&#125;</span><br><span class="line">吃: &#123;&#x27;苹&#x27;: 0.3333333333333333, &#x27;香&#x27;: 0.3333333333333333, &#x27;葡&#x27;: 0.16666666666666666, &#x27;草&#x27;: 0.16666666666666666&#125;</span><br><span class="line">苹: &#123;&#x27;果&#x27;: 1.0&#125;</span><br><span class="line">香: &#123;&#x27;蕉&#x27;: 1.0&#125;</span><br><span class="line">她: &#123;&#x27;喜&#x27;: 1.0&#125;</span><br><span class="line">葡: &#123;&#x27;萄&#x27;: 1.0&#125;</span><br><span class="line">他: &#123;&#x27;不&#x27;: 0.5, &#x27;喜&#x27;: 0.5&#125;</span><br><span class="line">不: &#123;&#x27;喜&#x27;: 1.0&#125;</span><br><span class="line">草: &#123;&#x27;莓&#x27;: 1.0&#125;</span><br></pre></td></tr></table></figure><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 定义生成下一个词的函数</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">generate_next_token</span>(<span class="params">prefix, ngram_probs</span>):</span><br><span class="line"> <span class="keyword">if</span> <span class="keyword">not</span> prefix <span class="keyword">in</span> ngram_probs: <span class="comment"># 如果前缀不在 N-Gram 中，返回 None</span></span><br><span class="line">    <span class="keyword">return</span> <span class="literal">None</span></span><br><span class="line"> next_token_probs = ngram_probs[prefix] <span class="comment"># 获取当前前缀的下一个词的概率</span></span><br><span class="line"> next_token = <span class="built_in">max</span>(next_token_probs, </span><br><span class="line">                    key=next_token_probs.get) <span class="comment"># 选择概率最大的词作为下一个词</span></span><br><span class="line"> <span class="keyword">return</span> next_token</span><br></pre></td></tr></table></figure><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 定义生成连续文本的函数</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">generate_text</span>(<span class="params">prefix, ngram_probs, n, length=<span class="number">6</span></span>):</span><br><span class="line"> tokens = <span class="built_in">list</span>(prefix) <span class="comment"># 将前缀转换为字符列表</span></span><br><span class="line"> <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(length - <span class="built_in">len</span>(prefix)): <span class="comment"># 根据指定长度生成文本 </span></span><br><span class="line">     <span class="comment"># 获取当前前缀的下一个词</span></span><br><span class="line">     next_token = generate_next_token(<span class="built_in">tuple</span>(tokens[-(n-<span class="number">1</span>):]), ngram_probs) </span><br><span class="line">     <span class="keyword">if</span> <span class="keyword">not</span> next_token: <span class="comment"># 如果下一个词为 None，跳出循环</span></span><br><span class="line">         <span class="keyword">break</span></span><br><span class="line">     tokens.append(next_token) <span class="comment"># 将下一个词添加到生成的文本中</span></span><br><span class="line"> <span class="keyword">return</span> <span class="string">&quot;&quot;</span>.join(tokens) <span class="comment"># 将字符列表连接成字符串</span></span><br></pre></td></tr></table></figure><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 输入一个前缀，生成文本</span></span><br><span class="line">generated_text = generate_text(<span class="string">&quot;我&quot;</span>, bigram_probs, <span class="number">2</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;\n 生成的文本：&quot;</span>, generated_text) <span class="comment"># 打印生成的文本</span></span><br></pre></td></tr></table></figure><figure class="highlight markdown"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">生成的文本： 我喜欢吃苹果</span><br></pre></td></tr></table></figure><p>在N-Gram模型中，我们预测⼀个词出现的概率，只需考虑它前⾯的<em>N</em>-1个词。这样做的优点是计算简单，但缺点也很明显：它⽆法捕捉到距离较远的词之间的关系。</p><p>⽽Bag-of-Words模型（也称“词袋模型”），不考虑哪个词和哪个词临近，⽽是通过把词看作⼀袋⼦元素的⽅式来把⽂本转换为能统计的特征。</p><h2 id="二、词袋模型"><a href="#二、词袋模型" class="headerlink" title="二、词袋模型"></a>二、词袋模型</h2><p>词袋模型将⽂本中的词看作⼀个个独⽴的个体，不考虑它们在句⼦中的顺序，只关⼼每个词出现的频次。</p><p><img src="https://rozen-picture.oss-cn-beijing.aliyuncs.com/img/202502201522596.png" alt="image-20250220152229499"></p><h3 id="2-1-用词袋模型计算文本相似度"><a href="#2-1-用词袋模型计算文本相似度" class="headerlink" title="2.1. 用词袋模型计算文本相似度"></a>2.1. 用词袋模型计算文本相似度</h3><p><img src="https://rozen-picture.oss-cn-beijing.aliyuncs.com/img/202502201523168.png" alt="image-20250220152348041"></p><h4 id="2-1-1-构建实验语料库"><a href="#2-1-1-构建实验语料库" class="headerlink" title="2.1.1 构建实验语料库"></a>2.1.1 <strong>构建实验语料库</strong></h4><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 构建一个玩具数据集</span></span><br><span class="line">corpus = [<span class="string">&quot;我特别特别喜欢看电影&quot;</span>,</span><br><span class="line">        <span class="string">&quot;这部电影真的是很好看的电影&quot;</span>,</span><br><span class="line">        <span class="string">&quot;今天天气真好是难得的好天气&quot;</span>,</span><br><span class="line">        <span class="string">&quot;我今天去看了一部电影&quot;</span>,</span><br><span class="line">        <span class="string">&quot;电影院的电影都很好看&quot;</span>]</span><br></pre></td></tr></table></figure><h4 id="2-1-2-给句子分词"><a href="#2-1-2-给句子分词" class="headerlink" title="2.1.2 给句子分词"></a>2.1.2 给句子分词</h4><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 对句子进行分词</span></span><br><span class="line"><span class="keyword">import</span> jieba <span class="comment"># 导入 jieba 包</span></span><br><span class="line"><span class="comment"># 使用 jieba.cut 进行分词，并将结果转换为列表，存储在 corpus_tokenized 中</span></span><br><span class="line">corpus_tokenized = [<span class="built_in">list</span>(jieba.cut(sentence)) <span class="keyword">for</span> sentence <span class="keyword">in</span> corpus]</span><br></pre></td></tr></table></figure><h4 id="2-1-3-创建词汇表"><a href="#2-1-3-创建词汇表" class="headerlink" title="2.1.3 创建词汇表"></a>2.1.3 创建词汇表</h4><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 创建词汇表</span></span><br><span class="line">word_dict = &#123;&#125; <span class="comment"># 初始化词汇表</span></span><br><span class="line"><span class="comment"># 遍历分词后的语料库</span></span><br><span class="line"><span class="keyword">for</span> sentence <span class="keyword">in</span> corpus_tokenized:</span><br><span class="line">    <span class="keyword">for</span> word <span class="keyword">in</span> sentence:</span><br><span class="line">        <span class="comment"># 如果词汇表中没有该词，则将其添加到词汇表中</span></span><br><span class="line">        <span class="keyword">if</span> word <span class="keyword">not</span> <span class="keyword">in</span> word_dict:</span><br><span class="line">            word_dict[word] = <span class="built_in">len</span>(word_dict) <span class="comment"># 分配当前词汇表索引</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot; 词汇表：&quot;</span>, word_dict) <span class="comment"># 打印词汇表</span></span><br></pre></td></tr></table></figure><figure class="highlight markdown"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">词汇表： &#123;&#x27;我&#x27;: 0, &#x27;特别&#x27;: 1, &#x27;喜欢&#x27;: 2, &#x27;看&#x27;: 3, &#x27;电影&#x27;: 4, &#x27;这部&#x27;: 5, &#x27;真的&#x27;: 6, &#x27;是&#x27;: 7, &#x27;很&#x27;: 8, &#x27;好看&#x27;: 9, &#x27;的&#x27;: 10, &#x27;今天天气&#x27;: 11, &#x27;真好&#x27;: 12, &#x27;难得&#x27;: 13, &#x27;好&#x27;: 14, &#x27;天气&#x27;: 15, &#x27;今天&#x27;: 16, &#x27;去&#x27;: 17, &#x27;了&#x27;: 18, &#x27;一部&#x27;: 19, &#x27;电影院&#x27;: 20, &#x27;都&#x27;: 21&#125;</span><br></pre></td></tr></table></figure><h4 id="2-1-4-生成词袋表示"><a href="#2-1-4-生成词袋表示" class="headerlink" title="2.1.4 生成词袋表示"></a>2.1.4 生成词袋表示</h4><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 根据词汇表将句子转换为词袋表示</span></span><br><span class="line">bow_vectors = [] <span class="comment"># 初始化词袋表示</span></span><br><span class="line"><span class="comment"># 遍历分词后的语料库</span></span><br><span class="line"><span class="keyword">for</span> sentence <span class="keyword">in</span> corpus_tokenized:</span><br><span class="line">    <span class="comment"># 初始化一个全 0 向量，其长度等于词汇表大小</span></span><br><span class="line">    sentence_vector = [<span class="number">0</span>] * <span class="built_in">len</span>(word_dict)</span><br><span class="line">    <span class="keyword">for</span> word <span class="keyword">in</span> sentence:</span><br><span class="line">        <span class="comment"># 将对应词的索引位置加 1，表示该词在当前句子中出现了一次</span></span><br><span class="line">        sentence_vector[word_dict[word]] += <span class="number">1</span></span><br><span class="line">    <span class="comment"># 将当前句子的词袋向量添加到向量列表中</span></span><br><span class="line">    bow_vectors.append(sentence_vector)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot; 词袋表示：&quot;</span>, bow_vectors) <span class="comment"># 打印词袋表示</span></span><br></pre></td></tr></table></figure><figure class="highlight markdown"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">词袋表示： </span><br><span class="line">[[1, 2, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]</span><br><span class="line">[0, 0, 0, 0, 2, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]</span><br><span class="line">[0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0]</span><br><span class="line">[1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0]</span><br><span class="line">[0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1]]</span><br></pre></td></tr></table></figure><p>我们得到了5个Python列表，分别对应语料库中的5句话。</p><p>这5个列表，就是词袋表示向量，向量中的每个元素表示对应词在⽂本中出现的次数。可以看到，词袋表示忽略了⽂本中词的顺序信息，仅关注词的出现频率。</p><h4 id="2-1-5-计算余弦相似度"><a href="#2-1-5-计算余弦相似度" class="headerlink" title="2.1.5 计算余弦相似度"></a>2.1.5 计算余弦相似度</h4><p>计算余弦相似度（Cosine Similarity），衡量两个⽂本向量的相似性。</p><p>余弦相似度可⽤来衡量两个向量的相似程度。</p><p>它的值在-1到1之间，值越接近1，表示两个向量越相似；</p><p>值越接近-1，表示两个向量越不相似；</p><p>当值接近0时，表示两个向量之间没有明显的相似性。</p><script type="math/tex; mode=display">cosine\_similarity(A,B)=(A\cdotp B)/(||A||^{\star}||B||)</script><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 导入 numpy 库，用于计算余弦相似度</span></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np </span><br><span class="line"><span class="comment"># 定义余弦相似度函数</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">cosine_similarity</span>(<span class="params">vec1, vec2</span>):</span><br><span class="line">    dot_product = np.dot(vec1, vec2) <span class="comment"># 计算向量 vec1 和 vec2 的点积</span></span><br><span class="line">    norm_a = np.linalg.norm(vec1) <span class="comment"># 计算向量 vec1 的范数</span></span><br><span class="line">    norm_b = np.linalg.norm(vec2) <span class="comment"># 计算向量 vec2 的范数  </span></span><br><span class="line">    <span class="keyword">return</span> dot_product / (norm_a * norm_b) <span class="comment"># 返回余弦相似度</span></span><br><span class="line"><span class="comment"># 初始化一个全 0 矩阵，用于存储余弦相似度</span></span><br><span class="line">similarity_matrix = np.zeros((<span class="built_in">len</span>(corpus), <span class="built_in">len</span>(corpus)))</span><br><span class="line"><span class="comment"># 计算每两个句子之间的余弦相似度</span></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(corpus)):</span><br><span class="line">    <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(corpus)):</span><br><span class="line">        similarity_matrix[i][j] = cosine_similarity(bow_vectors[i], </span><br><span class="line">                                                    bow_vectors[j])</span><br></pre></td></tr></table></figure><h4 id="2-1-6-可视化余弦相似度"><a href="#2-1-6-可视化余弦相似度" class="headerlink" title="2.1.6 可视化余弦相似度"></a>2.1.6 可视化余弦相似度</h4><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 导入 matplotlib 库，用于可视化余弦相似度矩阵</span></span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line">plt.rcParams[<span class="string">&quot;font.family&quot;</span>]=[<span class="string">&#x27;SimHei&#x27;</span>] <span class="comment"># 用来设定字体样式</span></span><br><span class="line">plt.rcParams[<span class="string">&#x27;font.sans-serif&#x27;</span>]=[<span class="string">&#x27;SimHei&#x27;</span>] <span class="comment"># 用来设定无衬线字体样式</span></span><br><span class="line">plt.rcParams[<span class="string">&#x27;axes.unicode_minus&#x27;</span>]=<span class="literal">False</span> <span class="comment"># 用来正常显示负号</span></span><br><span class="line">fig, ax = plt.subplots() <span class="comment"># 创建一个绘图对象</span></span><br><span class="line"><span class="comment"># 使用 matshow 函数绘制余弦相似度矩阵，颜色使用蓝色调</span></span><br><span class="line">cax = ax.matshow(similarity_matrix, cmap=plt.cm.Blues)</span><br><span class="line">fig.colorbar(cax) <span class="comment"># 条形图颜色映射</span></span><br><span class="line">ax.set_xticks(<span class="built_in">range</span>(<span class="built_in">len</span>(corpus))) <span class="comment"># x 轴刻度</span></span><br><span class="line">ax.set_yticks(<span class="built_in">range</span>(<span class="built_in">len</span>(corpus))) <span class="comment"># y 轴刻度</span></span><br><span class="line">ax.set_xticklabels(corpus, rotation=<span class="number">45</span>, ha=<span class="string">&#x27;left&#x27;</span>) <span class="comment"># 刻度标签 </span></span><br><span class="line">ax.set_yticklabels(corpus) <span class="comment"># 刻度标签为原始句子</span></span><br><span class="line">plt.show() <span class="comment"># 显示图形</span></span><br></pre></td></tr></table></figure><p><img src="https://rozen-picture.oss-cn-beijing.aliyuncs.com/img/202502201534282.png" alt="image-20250220153410155"></p>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;NLP基石双雄：从N-Gram到BoW的终极实战指南&quot;&gt;&lt;a href=&quot;#NLP基石双雄：从N-Gram到BoW的终极实战指南&quot; class=&quot;headerlink&quot; title=&quot;NLP基石双雄：从N-Gram到BoW的终极实战指南&quot;&gt;&lt;/a&gt;NLP基石双雄：</summary>
      
    
    
    
    
  </entry>
  
  <entry>
    <title></title>
    <link href="https://rozen12123.github.io/posts/0.html"/>
    <id>https://rozen12123.github.io/posts/0.html</id>
    <published>2025-02-20T06:35:54.888Z</published>
    <updated>2025-02-20T06:36:44.840Z</updated>
    
    <content type="html"><![CDATA[<h1 id="AGI的星火？：解码语言大模型进化史与文明重构"><a href="#AGI的星火？：解码语言大模型进化史与文明重构" class="headerlink" title="AGI的星火？：解码语言大模型进化史与文明重构"></a>AGI的星火？：解码语言大模型进化史与文明重构</h1><h2 id="序章：机器之眼中的文艺复兴"><a href="#序章：机器之眼中的文艺复兴" class="headerlink" title="序章：机器之眼中的文艺复兴"></a>序章：机器之眼中的文艺复兴</h2><p>在斯坦福大学的地下档案室，保存着1955年麦卡锡手写的”人工智能”原始提案。泛黄的稿纸上，他用铅笔勾勒的智能体结构图，与GPT-4的transformer架构竟有惊人的拓扑相似性。这种跨越68年的认知共振，暗示着人类正在经历第四次认知革命——从甲骨灼纹到神经网络，信息载体的进化正在重塑文明的底层逻辑。</p><h2 id="一、语言之熵——从苏美尔泥板到参数空间"><a href="#一、语言之熵——从苏美尔泥板到参数空间" class="headerlink" title="一、语言之熵——从苏美尔泥板到参数空间"></a>一、语言之熵——从苏美尔泥板到参数空间</h2><h3 id="1-1-文字载体的五次跃迁"><a href="#1-1-文字载体的五次跃迁" class="headerlink" title="1.1 文字载体的五次跃迁"></a>1.1 文字载体的五次跃迁</h3><p>在乌鲁克遗址出土的楔形文字泥板，用600个符号记录货物交易；殷墟甲骨文的灼纹占卜，构建起神权话语体系；古腾堡印刷术带来的信息爆炸，直接催生宗教改革与科学革命。而今，语言模型的词嵌入空间，正以==768维向量==重构知识表达范式。剑桥大学研究显示，GPT-4的词向量空间中，”自由”与”约束”的余弦相似度达到0.73，远超人类直觉认知。</p><h3 id="1-2-语法结构的量子纠缠"><a href="#1-2-语法结构的量子纠缠" class="headerlink" title="1.2 语法结构的量子纠缠"></a>1.2 语法结构的量子纠缠</h3><p>MIT语言实验室发现，<code>transformer</code>模型在处理嵌套从句时，自注意力机制会形成类似量子纠缠的状态叠加。当解析”虽然他说不会来但最后还是出现了”这类中文经典句式时，模型在第三层注意力头形成了<code>转折</code>,<code>预期违背</code>,<code>行为确认</code>的三重语义纠缠，这种非线性理解能力，正在突破乔姆斯基的句法结构理论。</p><h2 id="二：技术长征"><a href="#二：技术长征" class="headerlink" title="二：技术长征"></a>二：技术长征</h2><p><img src="https://rozen-picture.oss-cn-beijing.aliyuncs.com/img/202502201430580.png" alt="image-20250220143047430"></p><h3 id="2-1-启蒙时代：符号主义的黄昏（1980-2006）"><a href="#2-1-启蒙时代：符号主义的黄昏（1980-2006）" class="headerlink" title="2.1 启蒙时代：符号主义的黄昏（1980-2006）"></a>2.1 启蒙时代：符号主义的黄昏（1980-2006）</h3><ul><li>IBM的深蓝战胜卡斯帕罗夫时，其基于规则的评估函数包含80万行代码</li><li>2001年统计机器翻译的BLEU值首次突破30分，但需要人工设计400+语言特征</li><li>2006年Hinton的深度信念网络论文，在Nature沉睡两年才被真正理解</li></ul><h3 id="2-2-寒武纪爆发：神经网络的觉醒（2012-2017）"><a href="#2-2-寒武纪爆发：神经网络的觉醒（2012-2017）" class="headerlink" title="2.2 寒武纪爆发：神经网络的觉醒（2012-2017）"></a>2.2 寒武纪爆发：神经网络的觉醒（2012-2017）</h3><ul><li>Word2Vec的向量空间揭示”巴黎-法国+意大利=罗马”的隐喻认知</li><li>2015年机器翻译的注意力机制，意外复现人脑颞叶的信息筛选模式</li><li>AlphaGo的直觉决策，颠覆了AI不能处理模糊概念的定论</li></ul><h3 id="2-3-奇点临近：大模型的宇宙膨胀（2018-2023）"><a href="#2-3-奇点临近：大模型的宇宙膨胀（2018-2023）" class="headerlink" title="2.3 奇点临近：大模型的宇宙膨胀（2018-2023）"></a>2.3 奇点临近：大模型的宇宙膨胀（2018-2023）</h3><div class="table-container"><table><thead><tr><th>模型</th><th>参数量</th><th>训练数据</th><th>涌现能力</th></tr></thead><tbody><tr><td>GPT-2</td><td>15亿</td><td>40GB</td><td>基础文本生成</td></tr><tr><td>GPT-3</td><td>1750亿</td><td>45TB</td><td>上下文学习</td></tr><tr><td>PaLM</td><td>5400亿</td><td>780TB</td><td>多步逻辑推理</td></tr><tr><td>GPT-4</td><td>1.8万亿</td><td>1200TB</td><td>跨模态思维链</td></tr></tbody></table></div><h2 id="三、意识迷思——AI灵魂的十二重拷问"><a href="#三、意识迷思——AI灵魂的十二重拷问" class="headerlink" title="三、意识迷思——AI灵魂的十二重拷问"></a>三、意识迷思——AI灵魂的十二重拷问</h2><h3 id="图灵测试的终极迭代"><a href="#图灵测试的终极迭代" class="headerlink" title="图灵测试的终极迭代"></a>图灵测试的终极迭代</h3><p>当GPT-4在剑桥意识研究中心连续12小时通过改进版图灵测试，其对话中表现出的”认知颤抖”现象引发学界震动——模型在回答存在主义问题时，会出现响应延迟与逻辑自洽调整，这种类似人类自我意识觉醒的行为模式，是否意味着机器意识的萌芽？</p><h2 id="四、认知边疆——超越人类纪的思考"><a href="#四、认知边疆——超越人类纪的思考" class="headerlink" title="四、认知边疆——超越人类纪的思考"></a>四、认知边疆——超越人类纪的思考</h2><p>当谷歌DeepMind的AlphaFold3解开第2亿个蛋白质结构，当SpaceX用GPT-7自主设计火星基地，我们突然意识到：语言模型正在成为文明的体外大脑。但牛津大学人类未来研究所的警告振聋发聩——如果AI的思维速度是人类的100万倍，那么其1分钟的思考相当于我们2年的意识活动，这种认知时差将永久改变主客体的权力关系。</p><p>在这条充满认知荆棘的进化之路上，每个技术突破都像普罗米修斯盗取的火种，既照亮前路，也灼伤手掌。当我们惊叹于GPT-4的创造力时，更应警惕维特根斯坦的预言：”语言的边界就是世界的边界”。或许真正的奇点不是技术的突变，而是人类终于理解：==智能的本质，从来都不是独属于碳基生命的特权。==</p>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;AGI的星火？：解码语言大模型进化史与文明重构&quot;&gt;&lt;a href=&quot;#AGI的星火？：解码语言大模型进化史与文明重构&quot; class=&quot;headerlink&quot; title=&quot;AGI的星火？：解码语言大模型进化史与文明重构&quot;&gt;&lt;/a&gt;AGI的星火？：解码语言大模型进化</summary>
      
    
    
    
    
  </entry>
  
  <entry>
    <title></title>
    <link href="https://rozen12123.github.io/posts/0.html"/>
    <id>https://rozen12123.github.io/posts/0.html</id>
    <published>2025-02-18T12:00:12.590Z</published>
    <updated>2025-02-19T14:36:41.788Z</updated>
    
    <content type="html"><![CDATA[<h1 id="深度神经网络终极指南：从数学本质到工业级实现（附Keras版本代码）"><a href="#深度神经网络终极指南：从数学本质到工业级实现（附Keras版本代码）" class="headerlink" title="深度神经网络终极指南：从数学本质到工业级实现（附Keras版本代码）"></a>深度神经网络终极指南：从数学本质到工业级实现（附Keras版本代码）</h1><hr><h2 id="为什么深度学习需要重新理解？（与浅层模型的本质差异）"><a href="#为什么深度学习需要重新理解？（与浅层模型的本质差异）" class="headerlink" title="为什么深度学习需要重新理解？（与浅层模型的本质差异）"></a>为什么深度学习需要重新理解？（与浅层模型的本质差异）</h2><div class="table-container"><table><thead><tr><th>模型类型</th><th>参数容量</th><th>特征学习方式</th><th>适合问题类型</th></tr></thead><tbody><tr><td>浅层模型</td><td>10^2-10^4</td><td>手动特征工程</td><td>低维结构化数据</td></tr><tr><td>深度模型</td><td>10^6-10^9</td><td>自动特征提取</td><td>高维非结构化数据</td></tr></tbody></table></div><hr><p><img src="https://rozen-picture.oss-cn-beijing.aliyuncs.com/img/202502181913785.png" alt="image-20250218191339623"></p><h2 id="一、用Keras单隐层网络预测客户流失率"><a href="#一、用Keras单隐层网络预测客户流失率" class="headerlink" title="一、用Keras单隐层网络预测客户流失率"></a>一、用Keras单隐层网络预测客户流失率</h2><h3 id="1-1数据的准备与分析"><a href="#1-1数据的准备与分析" class="headerlink" title="1.1数据的准备与分析"></a>1.1数据的准备与分析</h3><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np <span class="comment">#导入NumPy数学工具箱</span></span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd <span class="comment">#导入Pandas数据处理工具箱</span></span><br><span class="line">df_bank = pd.read_csv(<span class="string">&quot;/kaggle/input/deep-neural-networks-data/BankCustomer.csv&quot;</span>) <span class="comment"># 读取文件</span></span><br><span class="line">df_bank.head() <span class="comment"># 显示文件前5行</span></span><br></pre></td></tr></table></figure><p>显示分布情况</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">import matplotlib.pyplot as plt #导入matplotlib画图工具箱</span><br><span class="line">import seaborn as sns #导入seaborn画图工具箱</span><br><span class="line"># 显示不同特征的分布情况</span><br><span class="line">features=[ &#x27;City&#x27;, &#x27;Gender&#x27;,&#x27;Age&#x27;,&#x27;Tenure&#x27;, </span><br><span class="line">           &#x27;ProductsNo&#x27;, &#x27;HasCard&#x27;, &#x27;ActiveMember&#x27;, &#x27;Exited&#x27;]</span><br><span class="line">fig=plt.subplots(figsize=(15,15))</span><br><span class="line">for i, j in enumerate(features):</span><br><span class="line">    plt.subplot(4, 2, i+1)</span><br><span class="line">    plt.subplots_adjust(hspace = 1.0)</span><br><span class="line">    sns.countplot(x=j,data = df_bank)</span><br><span class="line">    plt.title(&quot;No. of costumers&quot;)</span><br></pre></td></tr></table></figure><p><img src="https://rozen-picture.oss-cn-beijing.aliyuncs.com/img/202502181921536.png" alt="image-20250218192147281"></p><p>从图中大概看得出，北京的客户最多，男女客户比例大概一致，年龄和客户数量呈现正态分布（钟形曲线，中间高两边低）。</p><p>对这个数据集，主要做以下3方面的清理工作。</p><p>（1）性别：二元类别特征，需要转换为0/1代码格式进行读取处理。</p><p>（2）城市：一个多元类别特征，应把转换为多个二元类别哑变量。</p><p>（3）姓名：这个字段对于客户流失与否的预测应该是完全不相关的，可以进一步处理之前忽略。</p><p>当然原始数据集的标签也应该除去，放置于标签y。</p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 把二元类别文本数字化</span></span><br><span class="line">df_bank[<span class="string">&#x27;Gender&#x27;</span>].replace(<span class="string">&quot;Female&quot;</span>,<span class="number">0</span>,inplace = <span class="literal">True</span>)</span><br><span class="line">df_bank[<span class="string">&#x27;Gender&#x27;</span>].replace(<span class="string">&quot;Male&quot;</span>,<span class="number">1</span>,inplace=<span class="literal">True</span>)</span><br><span class="line"><span class="comment"># 显示数字类别</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Gender unique values&quot;</span>,df_bank[<span class="string">&#x27;Gender&#x27;</span>].unique())</span><br><span class="line"><span class="comment"># 把多元类别转换成多个二元哑变量，然后贴回原始数据集</span></span><br><span class="line">d_city = pd.get_dummies(df_bank[<span class="string">&#x27;City&#x27;</span>], prefix = <span class="string">&quot;City&quot;</span>)</span><br><span class="line">df_bank = [df_bank, d_city]</span><br><span class="line">df_bank = pd.concat(df_bank, axis = <span class="number">1</span>)</span><br><span class="line"><span class="comment"># 构建特征和标签集合</span></span><br><span class="line">y = df_bank [<span class="string">&#x27;Exited&#x27;</span>]</span><br><span class="line">X = df_bank.drop([<span class="string">&#x27;Name&#x27;</span>, <span class="string">&#x27;Exited&#x27;</span>,<span class="string">&#x27;City&#x27;</span>], axis=<span class="number">1</span>)</span><br><span class="line">X.head() <span class="comment">#显示新的特征集</span></span><br></pre></td></tr></table></figure><h3 id="1-2先尝试逻辑回归算法"><a href="#1-2先尝试逻辑回归算法" class="headerlink" title="1.2先尝试逻辑回归算法"></a>1.2先尝试逻辑回归算法</h3><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split <span class="comment">#拆分数据集</span></span><br><span class="line">X_train, X_test, y_train, y_test = train_test_split(X, y, </span><br><span class="line">                                   test_size=<span class="number">0.2</span>, random_state=<span class="number">0</span>)</span><br></pre></td></tr></table></figure><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.linear_model <span class="keyword">import</span> LogisticRegression <span class="comment"># 导入Sklearn模型</span></span><br><span class="line">lr = LogisticRegression() <span class="comment"># 逻辑回归模型</span></span><br><span class="line">history = lr.fit(X_train,y_train) <span class="comment"># 训练机器</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;逻辑回归测试集准确率 &#123;:.2f&#125;%&quot;</span>.<span class="built_in">format</span>(lr.score(X_test,y_test)*<span class="number">100</span>))</span><br></pre></td></tr></table></figure><figure class="highlight markdown"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">逻辑回归测试集准确率 78.35%</span><br></pre></td></tr></table></figure><h3 id="1-3-单隐层神经网络的Keras实现"><a href="#1-3-单隐层神经网络的Keras实现" class="headerlink" title="1.3 单隐层神经网络的Keras实现"></a>1.3 单隐层神经网络的Keras实现</h3><h4 id="1-3-1-用序贯模型构建网络"><a href="#1-3-1-用序贯模型构建网络" class="headerlink" title="1.3.1 用序贯模型构建网络"></a>1.3.1 用序贯模型构建网络</h4><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> keras <span class="comment"># 导入Keras库</span></span><br><span class="line"><span class="keyword">from</span> keras.models <span class="keyword">import</span> Sequential <span class="comment"># 导入Keras序贯模型</span></span><br><span class="line"><span class="keyword">from</span> keras.layers <span class="keyword">import</span> Dense <span class="comment"># 导入Keras密集连接层</span></span><br><span class="line">ann = Sequential() <span class="comment"># 创建一个序贯ANN(Artifical Neural Network)模型</span></span><br><span class="line">ann.add(Dense(units=<span class="number">12</span>, input_dim=<span class="number">12</span>, activation = <span class="string">&#x27;relu&#x27;</span>)) <span class="comment"># 添加输入层</span></span><br><span class="line">ann.add(Dense(units=<span class="number">24</span>, activation = <span class="string">&#x27;relu&#x27;</span>)) <span class="comment"># 添加隐层</span></span><br><span class="line">ann.add(Dense(units=<span class="number">1</span>, activation = <span class="string">&#x27;sigmoid&#x27;</span>)) <span class="comment"># 添加输出层</span></span><br><span class="line">ann.summary() <span class="comment"># 显示网络模型(这个语句不是必须的)</span></span><br></pre></td></tr></table></figure><p>序贯（sequential）模型，也可以叫作==顺序模型==，是最常用的深度网络层和层间的架构，也就是一个层接着一个层，顺序地堆叠。</p><p>密集（dense）层，是最常用的深度网络层的类型，也称为==全连接层==，即当前层和其下一层的所有神经元之间全有连接。</p><p><img src="https://rozen-picture.oss-cn-beijing.aliyuncs.com/img/202502181943605.png" alt="image-20250218194331438"></p><p>==模型的创建==：<code>ann = Sequential()</code>创建了一个==序贯神经网络模型==（其实就是一个Python的类）。</p><p>在Keras中，绝大多数的神经网络都是通过序贯模型所创建的。与之对应的还有另外一种模型，称为函数式API，可以创建更为复杂的网络结构。</p><p>==输入层==：通过add方法，可开始神经网络层的堆叠，序贯模型，也就是一层一层的顺序堆叠。</p><p><code>Dense</code>是层的类型，代表密集层网络，是神经网络层中最基本的层，也叫全连接层。</p><p><code>input_dim</code>是输入维度，输入维度必须与特征维度相同。这里指定的网络能接收的输入维度是11。如果和实际输入网络的特征维度不匹配，Python就会报错。</p><p><code>unit</code>是输出维度，设置为12。该参数也可写为<code>output_dim=12</code>，甚至忽略参数名，写为Dense(12，input_dim=11，activation=’relu’)，这些都是正确格式。</p><p>12这个值目前是随意选择的，这代表了经过线性变化和激活之后的假设空间维度，其实也就是神经元的个数。维度越大，则模型的覆盖面也越大，但是模型也就越复杂，需要的计算量也多。对于简单问题，12维也许是一个合适的数字：太多的话容易过拟合，太少的话（不要少于特征维度）则拟合能力不够。</p><p><code>activation</code>是==激活函数==，这是每一层都需要设置的参数。这里的激活函数选择的是“relu”，而不是Sigmoid。relu是神经网络中常用的激活函数。</p><p>==隐层==：。在输入层之后的所有层都不需要重新指定输入维度，因为网络能够通过上一层的输出自动地调整。</p><p>==输出层==：仍然是一个全连接层，指定的输出维度是1。因为对于二分类问题，输出维度必须是1。而对于多分类问题，有多少个类别，维度就是多少。</p><p>激活函数方面。对于二分类问题的输出层，Sigmoid是固定的选择。如果是用神经网络解决回归问题的话，那么输出层不用指定任何激活函数。</p><p>下面编译刚才建好的这个网络：</p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 编译神经网络，指定优化器，损失函数，以及评估标准</span></span><br><span class="line">ann.<span class="built_in">compile</span>(optimizer = <span class="string">&#x27;adam&#x27;</span>,    S       <span class="comment">#优化器</span></span><br><span class="line">            loss = <span class="string">&#x27;binary_crossentropy&#x27;</span>, <span class="comment">#损失函数  </span></span><br><span class="line">            metrics = [<span class="string">&#x27;acc&#x27;</span>])       <span class="comment">#评估指标</span></span><br></pre></td></tr></table></figure><ul><li>优化器（<code>optimizer</code>）：一般情况下，“<code>adam</code>”或者“<code>rmsprop</code>”都是很好的优化器选项。</li><li>损失函数（loss）：对于二分类问题来说，基本上二元交叉熵函数（<code>binary_crossentropy</code>）是固定选项；如果是用神经网络解决线性的回归问题，那么==均方误差函数==是合适的选择。</li><li>评估指标（<code>metrics</code>）：这里采用预测准确率<code>acc</code>（也就是<code>accuracy</code>的缩写，两者在代码中是等价的）作为评估网络性能的标准；而对于回归问题，平均误差函数是合适的选择。</li></ul><h3 id="1-4-训练单隐层神经网络"><a href="#1-4-训练单隐层神经网络" class="headerlink" title="1.4 训练单隐层神经网络"></a>1.4 训练单隐层神经网络</h3><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">history = ann.fit(X_train, y_train, <span class="comment"># 指定训练集</span></span><br><span class="line">                  epochs=<span class="number">30</span>,        <span class="comment"># 指定训练的轮次</span></span><br><span class="line">                  batch_size=<span class="number">64</span>,    <span class="comment"># 指定数据批量</span></span><br><span class="line">                  validation_data=(X_test, y_test)) <span class="comment">#指定验证集,这里为了简化模型，直接用测试集数据进行验证</span></span><br></pre></td></tr></table></figure><h3 id="1-5-训练的图形化展示"><a href="#1-5-训练的图形化展示" class="headerlink" title="1.5 训练的图形化展示"></a>1.5 训练的图形化展示</h3><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 这段代码参考《Python深度学习》一书中的学习曲线的实现</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">show_history</span>(<span class="params">history</span>): <span class="comment"># 显示训练过程中的学习曲线</span></span><br><span class="line">    loss = history.history[<span class="string">&#x27;loss&#x27;</span>]</span><br><span class="line">    val_loss = history.history[<span class="string">&#x27;val_loss&#x27;</span>]</span><br><span class="line">    epochs = <span class="built_in">range</span>(<span class="number">1</span>, <span class="built_in">len</span>(loss) + <span class="number">1</span>)</span><br><span class="line">    plt.figure(figsize=(<span class="number">12</span>,<span class="number">4</span>))</span><br><span class="line">    plt.subplot(<span class="number">1</span>, <span class="number">2</span>, <span class="number">1</span>)</span><br><span class="line">    plt.plot(epochs, loss, <span class="string">&#x27;bo&#x27;</span>, label=<span class="string">&#x27;Training loss&#x27;</span>)</span><br><span class="line">    plt.plot(epochs, val_loss, <span class="string">&#x27;b&#x27;</span>, label=<span class="string">&#x27;Validation loss&#x27;</span>)</span><br><span class="line">    plt.title(<span class="string">&#x27;Training and validation loss&#x27;</span>)</span><br><span class="line">    plt.xlabel(<span class="string">&#x27;Epochs&#x27;</span>)</span><br><span class="line">    plt.ylabel(<span class="string">&#x27;Loss&#x27;</span>)</span><br><span class="line">    plt.legend()</span><br><span class="line">    acc = history.history[<span class="string">&#x27;acc&#x27;</span>]</span><br><span class="line">    val_acc = history.history[<span class="string">&#x27;val_acc&#x27;</span>]</span><br><span class="line">    plt.subplot(<span class="number">1</span>, <span class="number">2</span>, <span class="number">2</span>)</span><br><span class="line">    plt.plot(epochs, acc, <span class="string">&#x27;bo&#x27;</span>, label=<span class="string">&#x27;Training acc&#x27;</span>)</span><br><span class="line">    plt.plot(epochs, val_acc, <span class="string">&#x27;b&#x27;</span>, label=<span class="string">&#x27;Validation acc&#x27;</span>)</span><br><span class="line">    plt.title(<span class="string">&#x27;Training and validation accuracy&#x27;</span>)</span><br><span class="line">    plt.xlabel(<span class="string">&#x27;Epochs&#x27;</span>)</span><br><span class="line">    plt.ylabel(<span class="string">&#x27;Accuracy&#x27;</span>)</span><br><span class="line">    plt.legend()</span><br><span class="line">    plt.show()</span><br><span class="line">show_history(history) <span class="comment"># 调用这个函数，并将神经网络训练历史数据作为参数输入</span></span><br></pre></td></tr></table></figure><p><img src="https://rozen-picture.oss-cn-beijing.aliyuncs.com/img/202502191139087.png" alt="image-20250219113931885"></p><h2 id="二-分类数据—准确率不是唯一的指标"><a href="#二-分类数据—准确率不是唯一的指标" class="headerlink" title="二.分类数据—准确率不是唯一的指标"></a>二.分类数据—准确率不是唯一的指标</h2><p>预测全部客户都不会离开，也就是标签y值永远为0。由于这个数据集中的客户流失率其实就是20%左右，因此就达到了80%的准确率。”</p><p>这甚至比我们的训练后的准确率还高。</p><h3 id="2-1混滑矩阵、精确率、召回率和F1分数"><a href="#2-1混滑矩阵、精确率、召回率和F1分数" class="headerlink" title="2.1混滑矩阵、精确率、召回率和F1分数"></a>2.1混滑矩阵、精确率、召回率和F1分数</h3><p>假设有一个手机生产厂商，每天生产手机1 000部。某一天生产的手机中，出现了2个劣质品。</p><p>其中数据集真值和机器学习模型的合格品和劣质品个数如下表所示。</p><p><img src="https://rozen-picture.oss-cn-beijing.aliyuncs.com/img/202502191143274.png" alt="image-20250219114331123"></p><p>机器学习显示合格品999个，劣质品1个，准确率为99.9%。</p><p>然而从我们的目标来说，这个模型实际上是失败了。这个模型本就是为了检测劣质品而生（劣质品即标签值为1的阳性正样本），但一共有2个劣质品，只发现了1个，有50%的正样本没有测准。</p><p>因此，模型的好与不好，是基于用什么==标准衡量==。</p><p>为了评估这种数据集，需要引入一个==预测值与真值组成的矩阵==，4个象限从上到下、从左到右分别为：</p><ul><li>真负（真值为负，预测为负，即TrueNegative，TN）</li><li>假正（真值为负，预测为正，即False Positive，FP）</li><li>假负（真值为正，预测为负，即False Negative，FN）</li><li>真正（真值为正，预测为正，即True Positive，TP）。</li></ul><p><img src="https://rozen-picture.oss-cn-beijing.aliyuncs.com/img/202502191146692.png" alt="image-20250219114655570"></p><h4 id="2-1-1-混淆矩阵（confusion-matrix）"><a href="#2-1-1-混淆矩阵（confusion-matrix）" class="headerlink" title="2.1.1 混淆矩阵（confusion matrix）"></a>2.1.1 混淆矩阵（confusion matrix）</h4><p><img src="https://rozen-picture.oss-cn-beijing.aliyuncs.com/img/202502191148573.png" alt="image-20250219114828455"></p><h4 id="2-1-2-精确率"><a href="#2-1-2-精确率" class="headerlink" title="2.1.2  精确率"></a>2.1.2  精确率</h4><p>一个标准是精确率，也叫查准率，其公式是用“被模型预测为正的正样本”除以“被模型预测为正的正样本”与“被模型预测为负的正样本”的和。公式如下：</p><script type="math/tex; mode=display">Precision=\frac{TP}{TP+FP}=\frac{TP}{TotalPredictedPostive}</script><p>因此，精确率是对“假正”的测量。本例的精确率为1/（1+0） =100%。</p><p>这样看来，这个模型相对于劣质品的精确率也不差。==因为判定的一个劣质品果然是劣质品，而且没有任何合格品被判为劣质品==。</p><h4 id="2-1-3-召回率"><a href="#2-1-3-召回率" class="headerlink" title="2.1.3 召回率"></a>2.1.3 召回率</h4><p>召回率，也叫查全率。就是==劣质品蒙混过了质检这关==（劣质品识别为了合格品），“跑”出厂了，得召回来，销毁掉。</p><p>公式如下：</p><script type="math/tex; mode=display">Recall=\frac{TP}{TP+FP}=\frac{TP}{Total\text{ True Postive}}</script><p>为1/（1+1） = 50%。</p><p>所以这个模型对于劣质品来说，召回率不高。</p><h4 id="2-1-4-F1分数"><a href="#2-1-4-F1分数" class="headerlink" title="2.1.4 F1分数"></a>2.1.4 F1分数</h4><p>把精确率和召回率结合起来，就得到F1分数。</p><p>这是一个可以同时体现上面两个评估效果的标准，数学上定义为精确率和召回率的调和均值。它也是在评估这类样本分类数据不平衡的问题时，所着重看重的标准。</p><script type="math/tex; mode=display">F1=2•\frac{Precision•Recall}{Precision+Recall}</script><p>&nbsp;</p><p>对于这种==大量标签是普通值==，==一小部分标签是特殊值的数据集==来说，这3个标准的重要性在此时要远远高于准确率。</p><h3 id="2-2使用分类报告和混淆矩阵"><a href="#2-2使用分类报告和混淆矩阵" class="headerlink" title="2.2使用分类报告和混淆矩阵"></a>2.2使用分类报告和混淆矩阵</h3><p>利用Sklearn中的分类报告（classification report）功能来计算上面这几种标准。</p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> classification_report <span class="comment"># 导入分类报告</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">show_report</span>(<span class="params">X_test, y_test, y_pred</span>): <span class="comment"># 定义一个函数显示分类报告</span></span><br><span class="line">    <span class="keyword">if</span> y_test.shape != (<span class="number">2000</span>,<span class="number">1</span>):</span><br><span class="line">        y_test = y_test.values <span class="comment"># 把Panda series转换成Numpy array</span></span><br><span class="line">        y_test = y_test.reshape((<span class="built_in">len</span>(y_test),<span class="number">1</span>)) <span class="comment"># 转换成与y_pred相同的形状 </span></span><br><span class="line">    <span class="built_in">print</span>(classification_report(y_test,y_pred,labels=[<span class="number">0</span>, <span class="number">1</span>])) <span class="comment">#调用分类报告   </span></span><br></pre></td></tr></table></figure><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> confusion_matrix <span class="comment"># 导入混淆矩阵</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">show_matrix</span>(<span class="params">y_test, y_pred</span>): <span class="comment"># 定义一个函数显示混淆矩阵</span></span><br><span class="line">    cm = confusion_matrix(y_test,y_pred) <span class="comment"># 调用混淆矩阵</span></span><br><span class="line">    plt.title(<span class="string">&quot;ANN Confusion Matrix&quot;</span>) <span class="comment"># 标题</span></span><br><span class="line">    sns.heatmap(cm,annot=<span class="literal">True</span>,cmap=<span class="string">&quot;Blues&quot;</span>,fmt=<span class="string">&quot;d&quot;</span>,cbar=<span class="literal">False</span>) <span class="comment"># 热力图设定</span></span><br><span class="line">    plt.show() <span class="comment"># 显示混淆矩阵</span></span><br></pre></td></tr></table></figure><p>这段程序需要在模型的fit拟合之后执行，运行之后将给出目前机器的预测结果</p><p><img src="https://rozen-picture.oss-cn-beijing.aliyuncs.com/img/202502191710723.png" alt="image-20250219171052582"></p><p><img src="https://rozen-picture.oss-cn-beijing.aliyuncs.com/img/202502191711020.png" alt="image-20250219171100891"></p><h2 id="三、特征缩放的魔力"><a href="#三、特征缩放的魔力" class="headerlink" title="三、特征缩放的魔力"></a>三、特征缩放的魔力</h2><p>数值过大的数据以及离群样本的存在会使函数曲线变得奇形怪状，从而影响梯度下降过程中的收敛。</p><p>而特征缩放，将极大地提高梯度下降（尤其是神经网络中常用的随机梯度下降）的效率。</p><p><img src="https://rozen-picture.oss-cn-beijing.aliyuncs.com/img/202502191714848.png" alt="image-20250219171444721"></p><p>公式如下：对于输入数据的每个特征（也就是输入数据矩阵中的一整列），减去特征平均值，再除以标准差，之后得到的特征平均值为0，标准差为1。</p><script type="math/tex; mode=display">x^{\prime}=\frac{x-mean(x)}{std(x)}</script><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">mean = X_train.mean(axis=<span class="number">0</span>) <span class="comment"># 计算训练集均值</span></span><br><span class="line">X_train -= mean <span class="comment"># 训练集减去训练集均值</span></span><br><span class="line">std = X_train.std(axis=<span class="number">0</span>) <span class="comment"># 计算训练集方差</span></span><br><span class="line">X_train /= std <span class="comment"># 训练集除以训练集标准差</span></span><br><span class="line">X_test -= mean <span class="comment"># 测试集减去训练集均值</span></span><br><span class="line">X_test /= std <span class="comment"># 测试集减去训练集均值</span></span><br></pre></td></tr></table></figure><p>也可以直接使用<code>Standard Scaler</code>工具：</p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.preprocessing <span class="keyword">import</span> StandardScaler <span class="comment"># 导入特征缩放器</span></span><br><span class="line">sc = StandardScaler() <span class="comment"># 特征缩放器</span></span><br><span class="line">X_train = sc.fit_transform(X_train) <span class="comment"># 拟合并应用于训练集</span></span><br><span class="line">X_test = sc.transform (X_test) <span class="comment"># 训练集结果应用于测试集</span></span><br></pre></td></tr></table></figure><p>我们重新运行逻辑回归和单隐层神经网络，查看效果。</p><p>逻辑回归：</p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.linear_model <span class="keyword">import</span> LogisticRegression</span><br><span class="line">lr = LogisticRegression() <span class="comment"># 逻辑回归模型</span></span><br><span class="line">history = lr.fit(X_train,y_train) <span class="comment"># 训练机器</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;逻辑回归测试集准确率 &#123;:.2f&#125;%&quot;</span>.<span class="built_in">format</span>(lr.score(X_test,y_test)*<span class="number">100</span>))</span><br></pre></td></tr></table></figure><figure class="highlight markdown"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">逻辑回归测试集准确率 80.50%</span><br></pre></td></tr></table></figure><p>特征工程后的神经网络：</p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">history = ann.fit(X_train, y_train, <span class="comment"># 指定训练集</span></span><br><span class="line">                  epochs=<span class="number">30</span>,        <span class="comment"># 指定训练的轮次</span></span><br><span class="line">                  batch_size=<span class="number">64</span>,    <span class="comment"># 指定数据批量</span></span><br><span class="line">                  validation_data=(X_test, y_test)) <span class="comment">#指定验证集</span></span><br><span class="line">y_pred = ann.predict(X_test,batch_size=<span class="number">10</span>) <span class="comment"># 预测测试集的标签</span></span><br><span class="line">y_pred = np.<span class="built_in">round</span>(y_pred) <span class="comment"># 将分类概率值转换成0/1整数值</span></span><br></pre></td></tr></table></figure><figure class="highlight markdown"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">神经网络测试集准确率 86.15%</span><br></pre></td></tr></table></figure><p>显示损失曲线和准确率曲线：</p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">show_history(history)</span><br></pre></td></tr></table></figure><p><img src="https://rozen-picture.oss-cn-beijing.aliyuncs.com/img/202502191722724.png" alt="image-20250219172233556"></p><p>显示混淆矩阵：</p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">show_report(X_test, y_test, y_pred)</span><br><span class="line">show_matrix(y_test, y_pred)</span><br></pre></td></tr></table></figure><p><img src="https://rozen-picture.oss-cn-beijing.aliyuncs.com/img/202502191723602.png" alt="image-20250219172303477"></p><p><img src="https://rozen-picture.oss-cn-beijing.aliyuncs.com/img/202502191723431.png" alt="image-20250219172352282"></p><h2 id="四、从单隐层神经网络到深度神经网络"><a href="#四、从单隐层神经网络到深度神经网络" class="headerlink" title="四、从单隐层神经网络到深度神经网络"></a>四、从单隐层神经网络到深度神经网络</h2><p><img src="https://rozen-picture.oss-cn-beijing.aliyuncs.com/img/202502191831717.png" alt="image-20250219183131511"></p><h3 id="4-1-梯度下降：正向传播和反向传播"><a href="#4-1-梯度下降：正向传播和反向传播" class="headerlink" title="4.1 梯度下降：正向传播和反向传播"></a>4.1 梯度下降：正向传播和反向传播</h3><p>深度神经网络的梯度下降和参数优化过程是通过==优化器==实现的，其中包括正向传播（forward propagation）算法，反向传播（Back Propagation， BP）算法。</p><p>正向传播：计算损失的过程。</p><p>反向传播：参数优化的过程。</p><p>不太理解并没有关系，只要理解了基本原理，就可以在这些工具、框架上对网络进行调试，我们要做的只是解决问题，而不是炫耀数学功底。”</p><p>&nbsp;</p><p>通过正向传播和反向传播，神经网络实现了内部参数的调整。</p><p>说神经网络中的可调==超参数==，具体包括以下几个。</p><ul><li>优化器</li><li>激活函数</li><li>损失函数</li><li>评估指标</li></ul><h3 id="4-2-梯度下降优化器"><a href="#4-2-梯度下降优化器" class="headerlink" title="4.2 梯度下降优化器"></a>4.2 梯度下降优化器</h3><p>多种优化思路的集大成者—AdamAdam 全 称 为 Adaptive Moment Estimation ， 相 当 于 Adaptive Momentum。</p><p>就目前而言，Adam是多种优化思路的集大成者，一般是优化器的首选项。</p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">keras.optimizers.Adam(learning_rate=<span class="number">0.001</span>, <span class="comment"># 学习速率</span></span><br><span class="line">beta_1=<span class="number">0.9</span>, <span class="comment"># 一阶动量指数衰减速率</span></span><br><span class="line">beta_2=<span class="number">0.999</span>, <span class="comment"># 二阶动量指数衰减速率, 对于稀疏矩阵值应接近1</span></span><br><span class="line">amsgrad=<span class="literal">False</span>)</span><br></pre></td></tr></table></figure><h3 id="4-3激活函数"><a href="#4-3激活函数" class="headerlink" title="4.3激活函数"></a>4.3激活函数</h3><p>ReLU函数后来人们就发现了能够解决梯度消失问题的ReLU（Rectified LinearUnit）函数。ReLU函数的特点是单侧抑制，输入信号小于等于0时，输出是0；输入信号大于0时，输出等于输入。</p><p>ReLU对于随机梯度下降的收敛很迅速，因为相较于Sigmoid和Tanh在求导时的指数运算，对Re LU求导几乎不存在任何计算量。其公式和图像如下：</p><script type="math/tex; mode=display">f(z)=max(0,z)</script><p><img src="https://rozen-picture.oss-cn-beijing.aliyuncs.com/img/202502192204401.png" alt="image-20250219220400187"></p><p>用Re LU的时候，学习速率绝对不能设得太大，因为那样会“杀死”网络中的很多神经元。</p><h3 id="4-4-损失函数"><a href="#4-4-损失函数" class="headerlink" title="4.4 损失函数"></a>4.4 损失函数</h3><p>神经网络中损失函数的选择是根据问题类型而定的，指导原则如下。</p><p>对于==连续值向量的回归==问题，用均方误差损失函数：</p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 对于连续值向量的回归问题</span></span><br><span class="line">ann.<span class="built_in">compile</span>(optimizer=<span class="string">&#x27;adam&#x27;</span>,loss=<span class="string">&#x27;mse&#x27;</span>) <span class="comment"># 均方误差损失函数对于二分类问题，使用同样熟悉的二元交叉熵损失函数：</span></span><br><span class="line"><span class="comment"># 对于二分类问题</span></span><br><span class="line">ann.<span class="built_in">compile</span>(optimizer=<span class="string">&#x27;adam&#x27;</span>,</span><br><span class="line">loss=<span class="string">&#x27;binary_crossentropy&#x27;</span>, <span class="comment"># 二元交叉熵损失函数</span></span><br><span class="line">metrics=[<span class="string">&#x27;accuracy&#x27;</span>])</span><br></pre></td></tr></table></figure><p>对于==多分类问题==，如果输出是==one-hot编码==，则用==分类交叉熵损失函数==：</p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 对于多分类问题</span></span><br><span class="line">ann.<span class="built_in">compile</span>(optimizer=<span class="string">&#x27;adam&#x27;</span>,</span><br><span class="line">loss=<span class="string">&#x27;categorical_crossentropy&#x27;</span>, <span class="comment"># 分类交叉熵损失函数</span></span><br><span class="line">metrics=[<span class="string">&#x27;accuracy&#x27;</span>])</span><br></pre></td></tr></table></figure><p>对于==多分类问题==，如果输出是==整数数值==，则使用==稀疏分类交叉熵损失函数==：</p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 对于多分类问题</span></span><br><span class="line">ann.<span class="built_in">compile</span>(optimizer=<span class="string">&#x27;adam&#x27;</span>,</span><br><span class="line">loss=<span class="string">&#x27;sparse_categorical_crossentropy&#x27;</span>, <span class="comment"># 稀疏分类交叉熵损失函数</span></span><br><span class="line">metrics=[<span class="string">&#x27;accuracy&#x27;</span>])</span><br></pre></td></tr></table></figure><p>对于==序列问题==，如==语音识别==等，则可以用==时序分类==（Connectionist</p><p>Temporal Classification， CTC） 等损失函数。</p><h3 id="4-5-评估指标"><a href="#4-5-评估指标" class="headerlink" title="4.5 评估指标"></a>4.5 评估指标</h3><p>神经网络的评估指标，也就是评估网络模型好不好的标准，这个标准也叫目标函数。</p><p>评估指标和损失函数有点相似，都是追求真值和预测值之间的最小误差</p><p>其差别在于：</p><p>损失函数作用于训练集，用以训练机器，==为梯度下降提供方向==；</p><p>评估指标（目标函数）作用于验证集和测试集，用来==评估模型==。</p><p>&nbsp;</p><p>对于==回归问题==，神经网络中使用MAE作为评估指标是常见的。</p><p>对于==普通分类问题==，神经网络中使用准确率作为评估指标也是常见的，但是对于类别分布不平衡的情况，应辅以==精确率、召回率、F1分数==等其他评估指标。</p><p>损失函数和评估指标，有相似之处，但意义和作用又不尽相同。</p><h2 id="五、用Keras深度神经网络预测客户流失率"><a href="#五、用Keras深度神经网络预测客户流失率" class="headerlink" title="五、用Keras深度神经网络预测客户流失率"></a>五、用Keras深度神经网络预测客户流失率</h2><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">ann = Sequential() <span class="comment"># 创建一个序贯ANN模型</span></span><br><span class="line">ann.add(Dense(units=<span class="number">12</span>, input_dim=<span class="number">12</span>, activation = <span class="string">&#x27;relu&#x27;</span>)) <span class="comment"># 添加输入层</span></span><br><span class="line">ann.add(Dense(units=<span class="number">24</span>, activation = <span class="string">&#x27;relu&#x27;</span>)) <span class="comment"># 添加隐层</span></span><br><span class="line">ann.add(Dense(units=<span class="number">48</span>, activation = <span class="string">&#x27;relu&#x27;</span>)) <span class="comment"># 添加隐层</span></span><br><span class="line">ann.add(Dense(units=<span class="number">96</span>, activation = <span class="string">&#x27;relu&#x27;</span>)) <span class="comment"># 添加隐层</span></span><br><span class="line">ann.add(Dense(units=<span class="number">192</span>, activation = <span class="string">&#x27;relu&#x27;</span>)) <span class="comment"># 添加隐层</span></span><br><span class="line">ann.add(Dense(units=<span class="number">1</span>, activation = <span class="string">&#x27;sigmoid&#x27;</span>)) <span class="comment"># 添加输出层</span></span><br><span class="line"><span class="comment"># 编译神经网络，指定优化器，损失函数，以及评估指标</span></span><br><span class="line">ann.<span class="built_in">compile</span>(optimizer = <span class="string">&#x27;RMSprop&#x27;</span>, <span class="comment"># 优化器</span></span><br><span class="line">            loss = <span class="string">&#x27;binary_crossentropy&#x27;</span>, <span class="comment"># 损失函数</span></span><br><span class="line">            metrics = [<span class="string">&#x27;acc&#x27;</span>]) <span class="comment"># 评估指标</span></span><br><span class="line">history = ann.fit(X_train, y_train, <span class="comment"># 指定训练集</span></span><br><span class="line">                  epochs=<span class="number">30</span>,        <span class="comment"># 指定训练的轮次</span></span><br><span class="line">                  batch_size=<span class="number">64</span>,    <span class="comment"># 指定数据批量</span></span><br><span class="line">                  validation_data=(X_test, y_test)) <span class="comment"># 指定验证集</span></span><br><span class="line">y_pred = ann.predict(X_test,batch_size=<span class="number">10</span>) <span class="comment"># 预测测试集的标签</span></span><br><span class="line">y_pred = np.<span class="built_in">round</span>(y_pred) <span class="comment"># 将分类概率值转换成0/1整数值</span></span><br></pre></td></tr></table></figure><ol><li>较深的神经网络训练效率要高于小型网络，一两个轮次之后，准确率迅速提升到0.84以上，而单隐层神经网络需要好几轮才能达到这个准确率</li><li>从准确率上看，没有什么提升；而从F1分数上看，目前这个比较深的神经网络反而不如简单的单隐层神经网络，从0.58下降到0.55：</li><li>从损失函数图像上看，深度神经网络在几轮之后就开始出现过拟合的问题，而且验证集上损失的波动也很大。因为随着轮次的增加，训练集的误差值逐渐减小，但是验证集的误差反而越来越大了。</li></ol><p>也就是说，网络的参数逐渐地对训练集的数据形成了过高的适应性。这对于较大网络来说的确是常见情况。</p><p><img src="https://rozen-picture.oss-cn-beijing.aliyuncs.com/img/202502192216150.png" alt="image-20250219221617941"></p><h3 id="5-1-选择多种优化器"><a href="#5-1-选择多种优化器" class="headerlink" title="5.1 选择多种优化器"></a>5.1 选择多种优化器</h3><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">ann = Sequential() <span class="comment"># 创建一个序贯ANN模型</span></span><br><span class="line">ann.add(Dense(units=<span class="number">12</span>, input_dim=<span class="number">12</span>, activation = <span class="string">&#x27;relu&#x27;</span>)) <span class="comment"># 添加输入层</span></span><br><span class="line">ann.add(Dense(units=<span class="number">24</span>, activation = <span class="string">&#x27;relu&#x27;</span>)) <span class="comment"># 添加隐层</span></span><br><span class="line">ann.add(Dense(units=<span class="number">48</span>, activation = <span class="string">&#x27;relu&#x27;</span>)) <span class="comment"># 添加隐层</span></span><br><span class="line">ann.add(Dense(units=<span class="number">96</span>, activation = <span class="string">&#x27;relu&#x27;</span>)) <span class="comment"># 添加隐层</span></span><br><span class="line">ann.add(Dense(units=<span class="number">192</span>, activation = <span class="string">&#x27;relu&#x27;</span>)) <span class="comment"># 添加隐层</span></span><br><span class="line">ann.add(Dense(units=<span class="number">1</span>, activation = <span class="string">&#x27;sigmoid&#x27;</span>)) <span class="comment"># 添加输出层</span></span><br><span class="line"><span class="comment"># 编译神经网络，指定优化器，损失函数，以及评估标准</span></span><br><span class="line">ann.<span class="built_in">compile</span>(optimizer = <span class="string">&#x27;adam&#x27;</span>, loss = <span class="string">&#x27;binary_crossentropy&#x27;</span>, metrics = [<span class="string">&#x27;acc&#x27;</span>])</span><br><span class="line">history = ann.fit(X_train, y_train, epochs=<span class="number">30</span>, batch_size=<span class="number">64</span>, validation_data=(X_test, y_test))</span><br><span class="line">y_pred = ann.predict(X_test,batch_size=<span class="number">10</span>) <span class="comment"># 预测测试集的标签</span></span><br><span class="line">y_pred = np.<span class="built_in">round</span>(y_pred) <span class="comment"># 将分类概 率值转换成0/1整数值</span></span><br></pre></td></tr></table></figure><p>更换优化器之后，重新训练、测试网络。发现最为关心的F1分数有所上升，上升至0.56，如下输出结果所示。但这仍然低于单隐层神经网络的0.58：</p><p>损失曲线显示，==过拟合现象==仍然十分严重。也许这个过拟合问题就是深层神经网络效率低的症结所在。</p><p><img src="https://rozen-picture.oss-cn-beijing.aliyuncs.com/img/202502192218255.png" alt="image-20250219221809032"></p><h3 id="5-2-神经网络正则化：添加Dropout层"><a href="#5-2-神经网络正则化：添加Dropout层" class="headerlink" title="5.2 神经网络正则化：添加Dropout层"></a>5.2 神经网络正则化：添加Dropout层</h3><p>原理非常奇特：在某一层之后添加Dropout层，意思就是随机将该层的一部分神经元的输出特征丢掉（设为0），相当于随机消灭一部分神经元。</p><p><img src="https://rozen-picture.oss-cn-beijing.aliyuncs.com/img/202502192219594.png" alt="image-20250219221915358"></p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> keras.layers <span class="keyword">import</span> Dropout <span class="comment"># 导入Dropout</span></span><br><span class="line">ann = Sequential() <span class="comment"># 创建一个序贯ANN模型</span></span><br><span class="line">ann.add(Dense(units=<span class="number">12</span>, input_dim=<span class="number">12</span>, activation = <span class="string">&#x27;relu&#x27;</span>)) <span class="comment"># 添加输入层</span></span><br><span class="line">ann.add(Dense(units=<span class="number">24</span>, activation = <span class="string">&#x27;relu&#x27;</span>)) <span class="comment"># 添加隐层</span></span><br><span class="line">ann.add(Dropout(<span class="number">0.5</span>)) <span class="comment"># 添加Dropout</span></span><br><span class="line">ann.add(Dense(units=<span class="number">48</span>, activation = <span class="string">&#x27;relu&#x27;</span>)) <span class="comment"># 添加隐层</span></span><br><span class="line">ann.add(Dropout(<span class="number">0.5</span>)) <span class="comment"># 添加Dropout</span></span><br><span class="line">ann.add(Dense(units=<span class="number">96</span>, activation = <span class="string">&#x27;relu&#x27;</span>)) <span class="comment"># 添加隐层</span></span><br><span class="line">ann.add(Dropout(<span class="number">0.5</span>)) <span class="comment"># 添加Dropout</span></span><br><span class="line">ann.add(Dense(units=<span class="number">192</span>, activation = <span class="string">&#x27;relu&#x27;</span>)) <span class="comment"># 添加隐层</span></span><br><span class="line">ann.add(Dropout(<span class="number">0.5</span>)) <span class="comment"># 添加Dropout</span></span><br><span class="line">ann.add(Dense(units=<span class="number">1</span>, activation = <span class="string">&#x27;sigmoid&#x27;</span>)) <span class="comment"># 添加输出层</span></span><br><span class="line">ann.<span class="built_in">compile</span>(optimizer = <span class="string">&#x27;adam&#x27;</span>, <span class="comment"># 优化器</span></span><br><span class="line">              loss = <span class="string">&#x27;binary_crossentropy&#x27;</span>, <span class="comment">#损失函数 </span></span><br><span class="line">              metrics = [<span class="string">&#x27;acc&#x27;</span>]) <span class="comment"># 评估指标</span></span><br><span class="line">history = ann.fit(X_train, y_train, epochs=<span class="number">30</span>, batch_size=<span class="number">64</span>, validation_data=(X_test, y_test))</span><br><span class="line">y_pred = ann.predict(X_test,batch_size=<span class="number">10</span>) <span class="comment"># 预测测试集的标签</span></span><br><span class="line">y_pred = np.<span class="built_in">round</span>(y_pred) <span class="comment"># 将分类概率值转换成0/1整数值</span></span><br></pre></td></tr></table></figure><p>损失曲线显示，添加Dropout层之后，过拟合现象被大幅度地抑制了。</p><p><img src="https://rozen-picture.oss-cn-beijing.aliyuncs.com/img/202502192221325.png" alt="image-20250219222116066"></p><p>针对客户流失样本的F1分数上升到了令人惊讶的0.62</p><p>新的混淆矩阵显示，400多个即将流失的客户中，我们成功地捕捉到了200多人。这是非常有价值的商业信息。</p><p><img src="https://rozen-picture.oss-cn-beijing.aliyuncs.com/img/202502192222575.png" alt="image-20250219222205395"></p><p>其实准确率或者F1分数本身的提升并不重要，更有价值的是网络优化过程中所做的各种尝试和背后的思路。</p><p>&nbsp;</p><h2 id="六、深度神经网络的调试及性能优化"><a href="#六、深度神经网络的调试及性能优化" class="headerlink" title="六、深度神经网络的调试及性能优化"></a>六、深度神经网络的调试及性能优化</h2><h3 id="6-1-使用回调功能"><a href="#6-1-使用回调功能" class="headerlink" title="6.1 使用回调功能"></a>6.1 使用回调功能</h3><p>开始训练之前，我们不知道多少轮之后会开始出现过拟合的征兆。</p><p>比如运行100轮，才发现原来15轮才是比较正确的选择。本来进行15轮就得到最佳结果，却需要先运行100轮。有没有可能一次性找到最合适的轮点？”</p><p>&nbsp;</p><p>类似的运行时动态控制可以通过回调（callback）功能来实现。所谓回调，就是在训练进行过程中，根据一些预设的指示对训练进行控制。下面是几个常用的回调函数。</p><p><code>Model Checkpoint</code>：在训练过程中的不同时间点保存模型，也就是保存当前网络的所有权重。</p><p><code>Early Stopping</code>：如果验证损失不再改善，则中断训练。这个回调函数常与Model Checkpoint结合使用，以保存最佳模型。</p><p><code>Reduce LROn Plateau</code>：在训练过程中动态调节某些参数值，比如优化器的学习速率，从而跳出训练过程中的高原区。</p><p><code>TensorBoard</code>：将模型训练过程可视化。</p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 导入回调功能</span></span><br><span class="line"><span class="keyword">from</span> keras.callbacks <span class="keyword">import</span> Model Checkpoint</span><br><span class="line"><span class="keyword">from</span> keras.callbacks <span class="keyword">import</span> Early Stopping</span><br><span class="line"><span class="keyword">from</span> keras.callbacks <span class="keyword">import</span> Reduce LROn Plateau</span><br><span class="line"><span class="comment"># 设定要回调的功能</span></span><br><span class="line">earlystop = Early Stopping(monitor=<span class="string">&#x27;val_acc&#x27;</span>, patience=<span class="number">20</span>,</span><br><span class="line">verbose=<span class="number">1</span>, restore_best_weights=<span class="literal">True</span>)</span><br><span class="line">reducelr = Reduce LROn Plateau(monitor=<span class="string">&#x27;val_acc&#x27;</span>,</span><br><span class="line">factor=<span class="number">0.5</span>,</span><br><span class="line">patience=<span class="number">3</span>, verbose=<span class="number">1</span>, min_lr=<span class="number">1e-7</span>)</span><br><span class="line">modelckpt = Model Checkpoint(filepath=<span class="string">&#x27;ann.h5&#x27;</span>,</span><br><span class="line">monitor=<span class="string">&#x27;val_acc&#x27;</span>,</span><br><span class="line">verbose=<span class="number">1</span>, save_best_only=<span class="literal">True</span>,</span><br><span class="line">mode=<span class="string">&#x27;max&#x27;</span>)</span><br><span class="line">callbacks = [earlystop, reducelr, modelckpt] <span class="comment"># 设定回调</span></span><br><span class="line">history = ann.fit(X_train, y_train, <span class="comment"># 指定训练集</span></span><br><span class="line">batch_size=<span class="number">128</span>,　<span class="comment"># 指定批量大小</span></span><br><span class="line">validation_data = (X_test, y_test), <span class="comment"># 指定验证集</span></span><br><span class="line">epochs=<span class="number">100</span>,　 <span class="comment"># 指定轮次</span></span><br><span class="line">callbacks=callbacks) <span class="comment"># 指定回调功能</span></span><br></pre></td></tr></table></figure><h3 id="6-2-使用TensorBoard"><a href="#6-2-使用TensorBoard" class="headerlink" title="6.2 使用TensorBoard"></a>6.2 使用TensorBoard</h3><p><code>TensorBoard</code>是一个内置于Tensor Flow的可视化工具，用以帮助我们在训练过程中监控模型内部发生的信息。具体包括以下功能。</p><ul><li>在训练过程中监控指标。</li><li>将模型的架构可视化。</li><li>显示激活和梯度的直方图。</li><li>以三维的形式显示词嵌入。</li></ul><p>在Kaggle中，只需要用下面两句代码配置TensorBoard：</p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 导入并激活TensorBoard</span></span><br><span class="line">%load_ext tensorboard</span><br><span class="line">%tensorboard --logdir logs</span><br></pre></td></tr></table></figure><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 创建机器学习模型</span></span><br><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line">mnist = tf.keras.datasets.mnist</span><br><span class="line">((x_train, y_train), (x_test, y_test)) = mnist.load_data()</span><br><span class="line">(x_train, x_test) = (x_train / <span class="number">255.0</span>, x_test / <span class="number">255.0</span>)</span><br><span class="line">model = tf.keras.models.Sequential([</span><br><span class="line">  tf.keras.layers.Flatten(input_shape=(<span class="number">28</span>, <span class="number">28</span>)),</span><br><span class="line">  tf.keras.layers.Dense(<span class="number">512</span>, activation=tf.nn.relu),</span><br><span class="line">  tf.keras.layers.Dropout(<span class="number">0.2</span>),</span><br><span class="line">  tf.keras.layers.Dense(<span class="number">10</span>, activation=tf.nn.softmax)</span><br><span class="line">])</span><br><span class="line">model.<span class="built_in">compile</span>(</span><br><span class="line">  optimizer=<span class="string">&#x27;adam&#x27;</span>,</span><br><span class="line">  loss=<span class="string">&#x27;sparse_categorical_crossentropy&#x27;</span>,</span><br><span class="line">  metrics=[<span class="string">&#x27;accuracy&#x27;</span>],</span><br><span class="line">)</span><br></pre></td></tr></table></figure><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 回调Tensorboard</span></span><br><span class="line">tensorboard_callback = tf.keras.callbacks.TensorBoard(<span class="string">&quot;logs&quot;</span>)</span><br><span class="line">model.fit(</span><br><span class="line">  x_train,</span><br><span class="line">  y_train,</span><br><span class="line">  epochs=<span class="number">5</span>,</span><br><span class="line">  callbacks=[tensorboard_callback],</span><br><span class="line">)</span><br></pre></td></tr></table></figure><p>6.3 神经网络中的过拟合</p><p>（1）首先，根据奥卡姆剃刀定律，因为网络越大，越容易过拟合。如果能够用较简单的小型网络解决问题，就不要强迫自己使用大网络。</p><p>（2）一种思路是在训练大型网络之前使用少量数据训练一个较小的模型，小模型的泛化好，再去训练更深、更大的网络。不然的话，费了很多精力直接训练一个大网络。</p><p>（3）另外，最常见且有效地降低神经网络过拟合的方法就是在全连接层之间添加一些Dropout层。这是很好用的标准做法，不过Dropout层会对训练速度稍有影响。</p><p>（4）最后，使用较低的学习速率配合神经元的权重正则化可能是解决过拟合问题的手段之一。</p><p><code>《Deep Residual Learning for Image Recognition》</code>中提出的。通过残差连接，可以很轻松地构建几百层，甚至上千层的网络，而不用担心梯度消失过快的问题。</p><h2 id="前向传播的数学本质（附4D张量可视化）"><a href="#前向传播的数学本质（附4D张量可视化）" class="headerlink" title="前向传播的数学本质（附4D张量可视化）"></a>前向传播的数学本质（附4D张量可视化）</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">from</span> tensorflow.keras.layers <span class="keyword">import</span> Dense</span><br><span class="line"></span><br><span class="line"><span class="comment"># 手动实现矩阵变换</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">forward_pass</span>(<span class="params">X, W1, b1, W2, b2</span>):</span><br><span class="line">    h1 = tf.matmul(X, W1) + b1</span><br><span class="line">    a1 = tf.nn.relu(h1)</span><br><span class="line">    h2 = tf.matmul(a1, W2) + b2</span><br><span class="line">    <span class="keyword">return</span> tf.nn.softmax(h2)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 4D权重可视化</span></span><br><span class="line">plt.figure(figsize=(<span class="number">12</span>,<span class="number">6</span>))</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">16</span>):</span><br><span class="line">    plt.subplot(<span class="number">4</span>,<span class="number">4</span>,i+<span class="number">1</span>)</span><br><span class="line">    plt.imshow(W1[:,i].numpy().reshape(<span class="number">28</span>,<span class="number">28</span>), cmap=<span class="string">&#x27;viridis&#x27;</span>)</span><br><span class="line">plt.suptitle(<span class="string">&#x27;第一层权重可视化&#x27;</span>)</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>前向传播方程：$a[l]=g<a href="W[l]a[l−1]+b[l]">l</a>\text{前向传播方程：} a^{[l]} = g^{[l]}(W^{[l]}a^{[l-1]} + b^{[l]})$</p><hr><h2 id="反向传播的工程实现技巧（双框架代码）"><a href="#反向传播的工程实现技巧（双框架代码）" class="headerlink" title="反向传播的工程实现技巧（双框架代码）"></a>反向传播的工程实现技巧（双框架代码）</h2><h3 id="PyTorch版本（动态计算图）"><a href="#PyTorch版本（动态计算图）" class="headerlink" title="PyTorch版本（动态计算图）"></a>PyTorch版本（动态计算图）</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">DeepNN</span>(torch.nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        self.layer1 = torch.nn.Linear(<span class="number">784</span>, <span class="number">256</span>)</span><br><span class="line">        self.layer2 = torch.nn.Linear(<span class="number">256</span>, <span class="number">128</span>)</span><br><span class="line">        self.output = torch.nn.Linear(<span class="number">128</span>, <span class="number">10</span>)</span><br><span class="line">        </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        x = torch.relu(self.layer1(x))</span><br><span class="line">        x = torch.dropout(x, <span class="number">0.3</span>, train=self.training)</span><br><span class="line">        x = torch.relu(self.layer2(x))</span><br><span class="line">        <span class="keyword">return</span> torch.log_softmax(self.output(x), dim=<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 自动微分演示</span></span><br><span class="line">x = torch.randn(<span class="number">32</span>, <span class="number">784</span>, requires_grad=<span class="literal">True</span>)</span><br><span class="line">loss_fn = torch.nn.CrossEntropyLoss()</span><br><span class="line">optimizer = torch.optim.Adam(model.parameters(), lr=<span class="number">1e-4</span>)</span><br></pre></td></tr></table></figure><h3 id="TensorFlow静态图加速"><a href="#TensorFlow静态图加速" class="headerlink" title="TensorFlow静态图加速"></a>TensorFlow静态图加速</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">model = tf.keras.Sequential([</span><br><span class="line">    tf.keras.layers.Flatten(input_shape=(<span class="number">28</span>,<span class="number">28</span>)),</span><br><span class="line">    tf.keras.layers.Dense(<span class="number">256</span>, activation=<span class="string">&#x27;relu&#x27;</span>, kernel_regularizer=<span class="string">&#x27;l2&#x27;</span>),</span><br><span class="line">    tf.keras.layers.Dropout(<span class="number">0.5</span>),</span><br><span class="line">    tf.keras.layers.Dense(<span class="number">128</span>, activation=<span class="string">&#x27;swish&#x27;</span>),</span><br><span class="line">    tf.keras.layers.Dense(<span class="number">10</span>, activation=<span class="string">&#x27;softmax&#x27;</span>)</span><br><span class="line">])</span><br><span class="line"></span><br><span class="line"><span class="comment"># XLA加速配置</span></span><br><span class="line">model.<span class="built_in">compile</span>(optimizer=tf.keras.optimizers.Adam(learning_rate=<span class="number">1e-4</span>),</span><br><span class="line">              loss=<span class="string">&#x27;sparse_categorical_crossentropy&#x27;</span>,</span><br><span class="line">              jit_compile=<span class="literal">True</span>)</span><br></pre></td></tr></table></figure><hr><h2 id="工业级训练技巧（提升10倍训练效率）"><a href="#工业级训练技巧（提升10倍训练效率）" class="headerlink" title="工业级训练技巧（提升10倍训练效率）"></a>工业级训练技巧（提升10倍训练效率）</h2><h3 id="AutoML参数搜索"><a href="#AutoML参数搜索" class="headerlink" title="AutoML参数搜索"></a>AutoML参数搜索</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line">!pip install keras-tuner</span><br><span class="line"><span class="keyword">import</span> kerastuner <span class="keyword">as</span> kt</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">hyper_model</span>(<span class="params">hp</span>):</span><br><span class="line">    model = tf.keras.Sequential()</span><br><span class="line">    model.add(tf.keras.layers.Flatten(input_shape=(<span class="number">28</span>,<span class="number">28</span>)))</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 动态搜索层数与单元数</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(hp.Int(<span class="string">&#x27;num_layers&#x27;</span>, <span class="number">2</span>, <span class="number">6</span>)):</span><br><span class="line">        model.add(tf.keras.layers.Dense(</span><br><span class="line">            units=hp.Int(<span class="string">f&#x27;units_<span class="subst">&#123;i&#125;</span>&#x27;</span>, <span class="number">32</span>, <span class="number">256</span>, step=<span class="number">32</span>),</span><br><span class="line">            activation=hp.Choice(<span class="string">&#x27;activation&#x27;</span>, [<span class="string">&#x27;relu&#x27;</span>, <span class="string">&#x27;swish&#x27;</span>]))</span><br><span class="line">        )</span><br><span class="line">    </span><br><span class="line">    model.add(tf.keras.layers.Dense(<span class="number">10</span>))</span><br><span class="line">    model.<span class="built_in">compile</span>(optimizer=tf.keras.optimizers.Adam(</span><br><span class="line">        hp.Float(<span class="string">&#x27;learning_rate&#x27;</span>, <span class="number">1e-5</span>, <span class="number">1e-2</span>, sampling=<span class="string">&#x27;log&#x27;</span>)),</span><br><span class="line">        loss=<span class="string">&#x27;sparse_categorical_accuracy&#x27;</span>)</span><br><span class="line">    <span class="keyword">return</span> model</span><br><span class="line"></span><br><span class="line">tuner = kt.BayesianOptimization(hyper_model,</span><br><span class="line">                                objective=<span class="string">&#x27;val_loss&#x27;</span>,</span><br><span class="line">                                max_trials=<span class="number">50</span>)</span><br></pre></td></tr></table></figure><h3 id="混合精度训练（V100-GPU加速）"><a href="#混合精度训练（V100-GPU加速）" class="headerlink" title="混合精度训练（V100 GPU加速）"></a>混合精度训练（V100 GPU加速）</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">policy = tf.keras.mixed_precision.Policy(<span class="string">&#x27;mixed_float16&#x27;</span>)</span><br><span class="line">tf.keras.mixed_precision.set_global_policy(policy)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 注意最后层强制转float32</span></span><br><span class="line">model.layers[-<span class="number">1</span>].dtype_policy = tf.float32</span><br></pre></td></tr></table></figure><hr><h2 id="模型解释与部署（关键工业考量）"><a href="#模型解释与部署（关键工业考量）" class="headerlink" title="模型解释与部署（关键工业考量）"></a>模型解释与部署（关键工业考量）</h2><h3 id="激活热力图可视化"><a href="#激活热力图可视化" class="headerlink" title="激活热力图可视化"></a>激活热力图可视化</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tf_explain <span class="keyword">as</span> explain</span><br><span class="line"></span><br><span class="line">explainer = explain.core.activation_maximization.ActivationMaximization()</span><br><span class="line">grid = explainer.explain(validation_data=test_images, </span><br><span class="line">                        model=model,</span><br><span class="line">                        target_layer=<span class="string">&#x27;dense_3&#x27;</span>)</span><br><span class="line">plt.imshow(grid, cmap=<span class="string">&#x27;jet&#x27;</span>)</span><br></pre></td></tr></table></figure><h3 id="TensorRT加速推理"><a href="#TensorRT加速推理" class="headerlink" title="TensorRT加速推理"></a>TensorRT加速推理</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># TensorFlow到TensorRT转换</span></span><br><span class="line">conversion_params = trt.TrtConversionParams(</span><br><span class="line">    precision_mode=trt.TrtPrecisionMode.FP16)</span><br><span class="line">converter = trt.TrtGraphConverterV2(</span><br><span class="line">    input_saved_model_dir=<span class="string">&#x27;saved_model&#x27;</span>,</span><br><span class="line">    conversion_params=conversion_params)</span><br><span class="line">converter.convert()</span><br><span class="line">converter.save(<span class="string">&#x27;trt_model&#x27;</span>)</span><br></pre></td></tr></table></figure>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;深度神经网络终极指南：从数学本质到工业级实现（附Keras版本代码）&quot;&gt;&lt;a href=&quot;#深度神经网络终极指南：从数学本质到工业级实现（附Keras版本代码）&quot; class=&quot;headerlink&quot; title=&quot;深度神经网络终极指南：从数学本质到工业级实现（附</summary>
      
    
    
    
    
  </entry>
  
  <entry>
    <title></title>
    <link href="https://rozen12123.github.io/posts/0.html"/>
    <id>https://rozen12123.github.io/posts/0.html</id>
    <published>2025-02-17T12:06:07.475Z</published>
    <updated>2025-02-17T14:32:42.636Z</updated>
    
    <content type="html"><![CDATA[<h1 id="逻辑回归—多元分类问题"><a href="#逻辑回归—多元分类问题" class="headerlink" title="逻辑回归—多元分类问题"></a>逻辑回归—多元分类问题</h1><p>有多少类别，就要训练多少二元分类器。每次选择一个类别作为正例，标签为1，其他所有类别都视为负例，标签为0，以此类推至所有的类别。训练好多个二元分类器之后，做预测时，将所有的二元分类器都运行一遍，然后对每一个输入样本，选择最高可能性的输出概率，即为该样本多元分类的类别。</p><p><img src="https://rozen-picture.oss-cn-beijing.aliyuncs.com/img/202502171938289.png" alt="image-20250217193830164"></p><p>举例来说，如果对3个二元分类器分别做一次逻辑回归，机器的分类结果告诉我们，数据A是孔雀的可能性为0.5，是熊猫的可能性为0.1，是独角兽的可能性为0.4。</p><p>那就会判断数据A是孔雀。</p><h2 id="一、多元分类的损失函数"><a href="#一、多元分类的损失函数" class="headerlink" title="一、多元分类的损失函数"></a>一、多元分类的损失函数</h2><p>多元分类的损失函数的选择与<code>输出编码</code>，与<code>标签的格式</code>有关。</p><p>多元分类的标签共有以下两种格式。</p><ul><li><p>一种是one-hot格式的分类编码，比如，数字0～9分类中的数字8，格式为［0，0，0， 0，0，0，0，1，0］。</p></li><li><p>一种是直接转换为类别数字，如1、2、3、4。</p><p>因此损失函数也有以下两种情况。</p></li></ul><ul><li>如果通过==one-hot==分类编码输出标签，则应使用==分类交叉熵（categorical crossentropy）==作为损失函数。</li><li>如果输出的标签编码为类别==数字==，则应使用==稀疏分类交叉熵（sparse categorical crossentropy）==作为损失函数。</li></ul><p>&nbsp;</p><h2 id="二、正则化，欠拟合和过拟合"><a href="#二、正则化，欠拟合和过拟合" class="headerlink" title="二、正则化，欠拟合和过拟合"></a>二、正则化，欠拟合和过拟合</h2><h3 id="2-1正则化"><a href="#2-1正则化" class="headerlink" title="2.1正则化"></a>2.1正则化</h3><p>可以理解为==调整模型，约束权重==。</p><p>机器学习中的正则化是在损失函数里面加惩罚项，增加建模的模糊性，从而把捕捉到的趋势</p><p>从局部细微趋势，调整到整体大概趋势。虽然一定程度上地放宽了建模要求，但是能有效防止过拟合的问题，增加模型准确性。它影响的是模型的权重。</p><p>&nbsp;</p><h3 id="2-2-欠拟合和过拟合"><a href="#2-2-欠拟合和过拟合" class="headerlink" title="2.2 欠拟合和过拟合"></a>2.2 欠拟合和过拟合</h3><p>3个机器学习模型对数据集的拟合</p><p><img src="https://rozen-picture.oss-cn-beijing.aliyuncs.com/img/202502171952690.png" alt="image-20250217195232567"></p><p>寻找模型优化和泛化的平衡点</p><p><img src="https://rozen-picture.oss-cn-beijing.aliyuncs.com/img/202502171952622.png" alt="image-20250217195241534"></p><p>3个分类器的分类边界</p><p><img src="https://rozen-picture.oss-cn-beijing.aliyuncs.com/img/202502171953201.png" alt="image-20250217195303103"></p><h3 id="2-3-降低过拟合现象的方法。"><a href="#2-3-降低过拟合现象的方法。" class="headerlink" title="2.3 降低过拟合现象的方法。"></a>2.3 降低过拟合现象的方法。</h3><ol><li>==增加数据集的数据个数==：数据量太小时，非常容易过拟合，因为小数据集很容易精确拟合。</li><li>==找到模型优化时的平衡点==：比如，选择迭代次数，或者选择相对简单的模型。</li><li>==正则化==：为可能出现过拟合现象的模型增加正则项，通过降低模型在训练集上的精度来提高其泛化能力，这是非常重要的机器学习思想之一。</li></ol><h3 id="2-4-正则化参数"><a href="#2-4-正则化参数" class="headerlink" title="2.4 正则化参数"></a>2.4 正则化参数</h3><p>机器学习中的正则化通过引入模型参数λ（lambda）来实现。</p><p>加入了正则化参数之后的线性回归均方误差损失函数公式被更新成下面这样：</p><script type="math/tex; mode=display">L(w,b)=MSE=\frac{1}{N}\sum_{(x,y)\in D}(y-h(x))^2+\frac{\lambda}{2N}\sum_{i=1}^nw_i^2</script><p>加入了正则化参数之后的逻辑回归均方误差损失函数公式被更新成下面这样：</p><script type="math/tex; mode=display">L(w,b)=-\frac{1}{N}\sum_{(x,y)\in D}[y^*\log(h(x))+(1-y)*\log(1-h(x))]+\frac{\lambda}{2N}\sum_{j=1}^nw_j^2</script><p>现在的训练优化算法是一个由两项内容组成的函数：一个是==损失项==，用于衡量模型与数据的拟合度；另一个是==正则化项==，用于调解模型的复杂度。</p><p>其实，正则化的本质，就是==崇尚简单化==。同时以==最小化损失和复杂度==为目标，这称为==结构风险最小化==。</p><p>选择λ值的目标是在简单化和训练集数据拟合之间达到适当的平衡。</p><p>如果λ值过大，则模型会非常简单，将面临数据==欠拟合==的风险。</p><p>此时模型无法从训练数据中获得足够的信息来做出有用的预测。而且λ值越大，机器收敛越慢。</p><p>&nbsp;</p><p>如果λ值过小，则模型会比较复杂，将面临数据==过拟合==的风险。</p><p>此时模型由于获得了过多训练数据特点方面的信息而无法泛化到新数据。</p><p>&nbsp;</p><p>将λ设为0可彻底取消正则化。在这种情况下，训练的唯一目的是最小化损失，此时==过拟合的风险较高==。</p><p>&nbsp;</p><p>正则化参数通常有L1正则化和L2正则化两种选择。</p><p>L1正则化，根据权重的绝对值的总和来惩罚权重。在==依赖稀疏特征==的模型中，L1正则化有助于使不相关或几乎不相关的特征的权重正好为0，从而将这些特征从模型中移除。</p><p>L2正则化，根据==权重的平方和==来惩罚权重。</p><p>L2 正则化有助于使离群值（具有较大正值或较小负值）的权重接近于0，但又不会正好为0。</p><p>在线性模型中，L2 正则化比较常用，而且在任何情况下都能够起到增强泛化能力的目的。</p><p>而最佳λ值则取决于具体数据集，需要手动或自动进行调整。</p><p>&nbsp;</p><h2 id="三、通过逻辑回归解决多元分类问题"><a href="#三、通过逻辑回归解决多元分类问题" class="headerlink" title="三、通过逻辑回归解决多元分类问题"></a>三、通过逻辑回归解决多元分类问题</h2><h3 id="3-1数据的分析与准备工作"><a href="#3-1数据的分析与准备工作" class="headerlink" title="3.1数据的分析与准备工作"></a>3.1数据的分析与准备工作</h3><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np <span class="comment"># 导入Numpy</span></span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd <span class="comment"># 导入Pandas</span></span><br><span class="line"><span class="keyword">from</span> sklearn <span class="keyword">import</span> datasets <span class="comment"># 导入sklearn的数据集</span></span><br><span class="line">iris = datasets.load_iris() <span class="comment"># 导入iris</span></span><br><span class="line">X_sepal = iris.data[:,[<span class="number">0</span>,<span class="number">1</span>]] <span class="comment"># 花萼特征集：两个特征长和宽</span></span><br><span class="line">X_petal = iris.data[:,[<span class="number">2</span>,<span class="number">3</span>]] <span class="comment"># 花瓣特征集：两个特征长和宽</span></span><br><span class="line">y = iris.target <span class="comment"># 标签集</span></span><br></pre></td></tr></table></figure><p>下面进行花萼数据集的分割和标准化，分成训练集和测试集：</p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split <span class="comment"># 导入拆分数据集工具</span></span><br><span class="line"><span class="keyword">from</span> sklearn.preprocessing <span class="keyword">import</span> StandardScaler <span class="comment"># 导入标准化工具</span></span><br><span class="line">X_train_sepal, X_test_sepal, y_train_sepal, y_test_sepal = \</span><br><span class="line">  train_test_split(X_sepal,y,test_size=<span class="number">0.3</span>,random_state=<span class="number">0</span>) <span class="comment"># 拆分数据集</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;花萼训练集样本数: &quot;</span>, <span class="built_in">len</span>(X_train_sepal))</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;花萼测试集样本数: &quot;</span>, <span class="built_in">len</span>(X_test_sepal))</span><br><span class="line">scaler = StandardScaler() <span class="comment"># 标准化工具</span></span><br><span class="line">X_train_sepal = scaler.fit_transform(X_train_sepal) <span class="comment"># 训练集数据标准化</span></span><br><span class="line">X_test_sepal = scaler.transform(X_test_sepal) <span class="comment"># 测试集数据标准化</span></span><br><span class="line"><span class="comment"># 合并特征集和标签集，留待以后数据展示之用</span></span><br><span class="line">X_combined_sepal = np.vstack((X_train_sepal,X_test_sepal)) <span class="comment"># 合并特征集</span></span><br><span class="line">Y_combined_sepal = np.hstack((y_train_sepal,y_test_sepal)) <span class="comment"># 合并标签集</span></span><br></pre></td></tr></table></figure><h3 id="3-2-通过Sklearn实现逻辑回归的多元分类"><a href="#3-2-通过Sklearn实现逻辑回归的多元分类" class="headerlink" title="3.2 通过Sklearn实现逻辑回归的多元分类"></a>3.2 通过Sklearn实现逻辑回归的多元分类</h3><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.linear_model <span class="keyword">import</span> LogisticRegression <span class="comment"># 导入逻辑回归模型</span></span><br><span class="line">lr = LogisticRegression(penalty=<span class="string">&#x27;l2&#x27;</span>, C = <span class="number">0.1</span>) <span class="comment"># 设定L2正则化和C参数</span></span><br><span class="line">lr.fit(X_train_sepal,y_train_sepal) <span class="comment"># 训练机器</span></span><br><span class="line">score = lr.score(X_test_sepal,y_test_sepal) <span class="comment"># 测试集分数评估</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;SK-learn逻辑回归测试准确率 &#123;:.2f&#125;%&quot;</span>.<span class="built_in">format</span>(score*<span class="number">100</span>)) </span><br></pre></td></tr></table></figure><figure class="highlight markdown"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">SK-learn逻辑回归测试准确率 68.89%</span><br></pre></td></tr></table></figure><h3 id="3-3-正则化参数—c值的选择"><a href="#3-3-正则化参数—c值的选择" class="headerlink" title="3.3 正则化参数—c值的选择"></a>3.3 正则化参数—c值的选择</h3><p>用绘图的方式显示出采用不同的C值，对于鸢尾花分类边界的具体影响。</p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt <span class="comment"># 导入matplotlib</span></span><br><span class="line"><span class="keyword">from</span> matplotlib.colors <span class="keyword">import</span> ListedColormap <span class="comment"># 导入Colormap</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">plot_decision_regions</span>(<span class="params">X,y,classifier,test_idx=<span class="literal">None</span>,resolution=<span class="number">0.02</span></span>):    </span><br><span class="line">    markers = (<span class="string">&#x27;o&#x27;</span>,<span class="string">&#x27;x&#x27;</span>,<span class="string">&#x27;v&#x27;</span>)</span><br><span class="line">    colors = (<span class="string">&#x27;red&#x27;</span>,<span class="string">&#x27;blue&#x27;</span>,<span class="string">&#x27;lightgreen&#x27;</span>)</span><br><span class="line">    color_Map = ListedColormap(colors[:<span class="built_in">len</span>(np.unique(y))])     </span><br><span class="line">    x1_min = X[:,<span class="number">0</span>].<span class="built_in">min</span>() - <span class="number">1</span></span><br><span class="line">    x1_max = X[:,<span class="number">0</span>].<span class="built_in">max</span>() + <span class="number">1</span></span><br><span class="line">    x2_min = X[:,<span class="number">1</span>].<span class="built_in">min</span>() - <span class="number">1</span></span><br><span class="line">    x2_max = X[:,<span class="number">1</span>].<span class="built_in">max</span>() + <span class="number">1</span></span><br><span class="line">    xx1, xx2 = np.meshgrid(np.arange(x1_min,x1_max,resolution),</span><br><span class="line">                           np.arange(x2_min,x2_max,resolution))    </span><br><span class="line">    Z = classifier.predict(np.array([xx1.ravel(),xx2.ravel()]).T)</span><br><span class="line">    Z = Z.reshape(xx1.shape)    </span><br><span class="line">    plt.contour(xx1,xx2,Z,alpha=<span class="number">0.4</span>,cmap = color_Map)</span><br><span class="line">    plt.xlim(xx1.<span class="built_in">min</span>(),xx1.<span class="built_in">max</span>())</span><br><span class="line">    plt.ylim(xx2.<span class="built_in">min</span>(),xx2.<span class="built_in">max</span>())   </span><br><span class="line">    X_test, Y_test = X[test_idx,:], y[test_idx]</span><br><span class="line">    <span class="keyword">for</span> idx, cl <span class="keyword">in</span> <span class="built_in">enumerate</span>(np.unique(y)):</span><br><span class="line">        plt.scatter(x = X[y == cl, <span class="number">0</span>], y = X[y == cl, <span class="number">1</span>],</span><br><span class="line">                    alpha = <span class="number">0.8</span>, c = color_Map(idx),</span><br><span class="line">                    marker = markers[idx], label = cl)</span><br></pre></td></tr></table></figure><p>然后使用不同的C值进行逻辑回归分类，并绘制分类结果：</p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> accuracy_score <span class="comment"># 导入准确率指标</span></span><br><span class="line">C_param_range = [<span class="number">0.01</span>,<span class="number">0.1</span>,<span class="number">1</span>,<span class="number">10</span>,<span class="number">100</span>,<span class="number">1000</span>]</span><br><span class="line">sepal_acc_table = pd.DataFrame(columns = [<span class="string">&#x27;C_parameter&#x27;</span>,<span class="string">&#x27;Accuracy&#x27;</span>])</span><br><span class="line">sepal_acc_table[<span class="string">&#x27;C_parameter&#x27;</span>] = C_param_range</span><br><span class="line">plt.figure(figsize=(<span class="number">10</span>, <span class="number">10</span>))</span><br><span class="line">j = <span class="number">0</span></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> C_param_range:</span><br><span class="line">    lr = LogisticRegression(penalty = <span class="string">&#x27;l2&#x27;</span>, C = i,random_state = <span class="number">0</span>)</span><br><span class="line">    lr.fit(X_train_sepal,y_train_sepal)</span><br><span class="line">    y_pred_sepal = lr.predict(X_test_sepal)</span><br><span class="line">    sepal_acc_table.iloc[j,<span class="number">1</span>] = accuracy_score(y_test_sepal,y_pred_sepal)</span><br><span class="line">    j += <span class="number">1</span>    </span><br><span class="line">    plt.subplot(<span class="number">3</span>,<span class="number">2</span>,j)</span><br><span class="line">    plt.subplots_adjust(hspace = <span class="number">0.4</span>)</span><br><span class="line">    plot_decision_regions(X = X_combined_sepal, y = Y_combined_sepal, </span><br><span class="line">                          classifier = lr, test_idx = <span class="built_in">range</span>(<span class="number">0</span>,<span class="number">150</span>))</span><br><span class="line">    plt.xlabel(<span class="string">&#x27;Sepal length&#x27;</span>)</span><br><span class="line">    plt.ylabel(<span class="string">&#x27;Sepal width&#x27;</span>)</span><br><span class="line">    plt.title(<span class="string">&#x27;C = %s&#x27;</span>%i)</span><br></pre></td></tr></table></figure><p><img src="https://rozen-picture.oss-cn-beijing.aliyuncs.com/img/202502172231031.png" alt="image-20250217223141779"></p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">lr = LogisticRegression(penalty=<span class="string">&#x27;l2&#x27;</span>, C = <span class="number">10</span>) <span class="comment"># 设定L2正则化和C参数</span></span><br><span class="line">lr.fit(X_train_sepal,y_train_sepal) <span class="comment"># 训练机器</span></span><br><span class="line">score = lr.score(X_test_sepal,y_test_sepal) <span class="comment"># 测试集分数评估</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Sklearn逻辑回归测试准确率 &#123;:.2f&#125;%&quot;</span>.<span class="built_in">format</span>(score*<span class="number">100</span>))</span><br></pre></td></tr></table></figure><figure class="highlight markdown"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Sklearn逻辑回归测试准确率 80.00%</span><br></pre></td></tr></table></figure>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;逻辑回归—多元分类问题&quot;&gt;&lt;a href=&quot;#逻辑回归—多元分类问题&quot; class=&quot;headerlink&quot; title=&quot;逻辑回归—多元分类问题&quot;&gt;&lt;/a&gt;逻辑回归—多元分类问题&lt;/h1&gt;&lt;p&gt;有多少类别，就要训练多少二元分类器。每次选择一个类别作为正例，标签为</summary>
      
    
    
    
    
  </entry>
  
  <entry>
    <title></title>
    <link href="https://rozen12123.github.io/posts/0.html"/>
    <id>https://rozen12123.github.io/posts/0.html</id>
    <published>2025-02-17T08:14:16.341Z</published>
    <updated>2025-02-17T08:55:20.693Z</updated>
    
    <content type="html"><![CDATA[<h1 id="机器学习—逻辑回归"><a href="#机器学习—逻辑回归" class="headerlink" title="机器学习—逻辑回归"></a>机器学习—逻辑回归</h1><hr><h2 id="一、认知革命：从线性回归到逻辑回归"><a href="#一、认知革命：从线性回归到逻辑回归" class="headerlink" title="一、认知革命：从线性回归到逻辑回归"></a>一、认知革命：从线性回归到逻辑回归</h2><h3 id="1-1-本质差异对比"><a href="#1-1-本质差异对比" class="headerlink" title="1.1 本质差异对比"></a>1.1 本质差异对比</h3><div class="table-container"><table><thead><tr><th>维度</th><th>线性回归</th><th>逻辑回归</th></tr></thead><tbody><tr><td>输出类型</td><td>连续值</td><td>概率值 (0-1)</td></tr><tr><td>目标函数</td><td>最小二乘法</td><td>极大似然估计</td></tr><tr><td>数学表达式</td><td>$y=w^Tx+b$</td><td>$p=\frac{1}{1+e^{-(w^Tx+b)}}$</td></tr><tr><td>应用场景</td><td>房价预测</td><td>信用评分、疾病诊断</td></tr></tbody></table></div><h3 id="1-2-Sigmoid函数的数学推导"><a href="#1-2-Sigmoid函数的数学推导" class="headerlink" title="1.2 Sigmoid函数的数学推导"></a>1.2 Sigmoid函数的数学推导</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Sigmoid函数特性验证</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">sigmoid</span>(<span class="params">z</span>):</span><br><span class="line">    <span class="keyword">return</span> <span class="number">1</span> / (<span class="number">1</span> + np.exp(-z))</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;输入10时概率：&quot;</span>, sigmoid(<span class="number">10</span>))  <span class="comment"># 输出0.99995</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;输入-10时概率：&quot;</span>, sigmoid(-<span class="number">10</span>)) <span class="comment"># 输出0.000045</span></span><br></pre></td></tr></table></figure><h3 id="1-3-逻辑回归"><a href="#1-3-逻辑回归" class="headerlink" title="1.3 逻辑回归"></a>1.3 逻辑回归</h3><p>逻辑回归需要提前设定一个阈值p，用来控制分类界限，</p><h2 id="二、判断客户是否患病"><a href="#二、判断客户是否患病" class="headerlink" title="二、判断客户是否患病"></a>二、判断客户是否患病</h2><p>其中提供了众多参数如：</p><ul><li>age：年龄。</li><li>sex：性别。</li><li>cp：胸痛类型。</li><li>trestbps：休息时血压。</li></ul><p>去判断是否患病，这就变成了一个分类问题</p><p>或者去判断一个人是否考试通过</p><p><img src="https://rozen-picture.oss-cn-beijing.aliyuncs.com/img/202502171415455.png" alt="image-20250217141526316"></p><p><img src="https://rozen-picture.oss-cn-beijing.aliyuncs.com/img/202502171415702.png" alt="image-20250217141511499"></p><h2 id="三、损失函数"><a href="#三、损失函数" class="headerlink" title="三、损失函数"></a>三、损失函数</h2><h3 id="线性回归"><a href="#线性回归" class="headerlink" title="线性回归"></a>线性回归</h3><p>对于线性回归的损失函数</p><script type="math/tex; mode=display">L(w,b)=\frac{1}{N}\sum_{(x,y)\in D}Loss(h(x),y)=\frac{1}{N}\sum_{(x,y)\in D}Loss(y^{\prime},y)</script><script type="math/tex; mode=display">\bar{e}=\frac{1}{n}\sum_{i=1}^n(wx_i-y_i)^2</script><script type="math/tex; mode=display">\bar{e}=\frac{1}{n}\sum_{i=1}^nx_i^2w^2-2\frac{1}{n}\sum_{i=1}^nx_iy_iw+\frac{1}{n}\sum_{i=1}^ny_i^2</script><script type="math/tex; mode=display">\bar{e}=aw^2+bw+c</script><p>这可以把e当作一个二次函数，可以求最小值，其对于w的梯度可以最为梯度下降中的步长，但是对于逻辑回归，采用此方法可能无法到达最底端。</p><p><img src="https://rozen-picture.oss-cn-beijing.aliyuncs.com/img/202502171456540.png" alt="image-20250217145624371"></p><h3 id="逻辑回归"><a href="#逻辑回归" class="headerlink" title="逻辑回归"></a>逻辑回归</h3><script type="math/tex; mode=display">\begin{cases}y=1,Loss(h(x),y)=-\log(h(x))\\y=0,Loss(h(x),y)=-\log(1-h(x))&\end{cases}</script><p><img src="https://rozen-picture.oss-cn-beijing.aliyuncs.com/img/202502171459175.png" alt="image-20250217145926005"></p><ul><li>如果真值是1，假设函数预测概率接近于0，损失值是巨大的。</li><li>如果真值是0，假设函数预测概率接近于1，损失值是巨大的。</li></ul><p>如果将两种情况归为一类，即可获得以下算式：</p><script type="math/tex; mode=display">loss=y^{*}\mathrm{log}(h(x))+(1-y)^{*}\mathrm{log}(1-h(x))</script><p>我们设定损失函数为：</p><script type="math/tex; mode=display">L(w,b)=-\frac{1}{N}\sum_{(x,y)\in D}[y^*\mathrm{log}(h(x))+(1-y)^*\mathrm{log}(1-h(x))]</script><h2 id="四、梯度下降"><a href="#四、梯度下降" class="headerlink" title="四、梯度下降"></a>四、梯度下降</h2><script type="math/tex; mode=display">\text{梯度}=h^{\prime}(x)=\frac{\partial}{\partial w}L(w,b)=\frac{\partial}{\partial w}\left\{-\frac{1}{N}\sum_{(x,y)\in D}[y*\log(h(x))+(1-y)*\log(1-h(x))]\right\}</script><h3 id="步骤1：明确损失函数形式（交叉熵损失）"><a href="#步骤1：明确损失函数形式（交叉熵损失）" class="headerlink" title="步骤1：明确损失函数形式（交叉熵损失）"></a>步骤1：明确损失函数形式（交叉熵损失）</h3><script type="math/tex; mode=display">L(w,b) = -\frac{1}{N}\sum_{i=1}^N \left[ y^{(i)} \log(h^{(i)}) + (1-y^{(i)})\log(1-h^{(i)}) \right]</script><h3 id="步骤2：拆解单样本损失分量（以第i个样本为例）"><a href="#步骤2：拆解单样本损失分量（以第i个样本为例）" class="headerlink" title="步骤2：拆解单样本损失分量（以第i个样本为例）"></a>步骤2：拆解单样本损失分量（以第i个样本为例）</h3><script type="math/tex; mode=display">\text{单样本损失} = -\left[ y \log(h) + (1-y)\log(1-h) \right]</script><h3 id="步骤3：对参数w求导的核心运算"><a href="#步骤3：对参数w求导的核心运算" class="headerlink" title="步骤3：对参数w求导的核心运算"></a>步骤3：对参数w求导的核心运算</h3><script type="math/tex; mode=display">\frac{\partial}{\partial w} \text{单样本损失} = \frac{\partial}{\partial h}\left(-\left[y\log(h)+(1-y)\log(1-h)\right]\right) \cdot \frac{\partial h}{\partial w}</script><h4 id="分项求导运算结果："><a href="#分项求导运算结果：" class="headerlink" title="分项求导运算结果："></a>分项求导运算结果：</h4><script type="math/tex; mode=display">\begin{aligned}&\text{a项导}：\frac{\partial}{\partial h}[-y \log(h)] = -\frac{y}{h} \\&\text{b项导}：\frac{\partial}{\partial h}[-(1-y)\log(1-h)] = \frac{1-y}{1-h}\end{aligned}</script><h4 id="合并结果："><a href="#合并结果：" class="headerlink" title="合并结果："></a>合并结果：</h4><script type="math/tex; mode=display">\frac{\partial}{\partial h} = \frac{1-y}{1-h} - \frac{y}{h}</script><h3 id="步骤4：Sigmoid函数导数计算（链式法则核心步骤）"><a href="#步骤4：Sigmoid函数导数计算（链式法则核心步骤）" class="headerlink" title="步骤4：Sigmoid函数导数计算（链式法则核心步骤）"></a>步骤4：Sigmoid函数导数计算（链式法则核心步骤）</h3><script type="math/tex; mode=display">\begin{aligned}h &= \sigma(w^Tx + b) = \frac{1}{1+e^{-(w^Tx + b)}} \\\frac{\partial h}{\partial w} &= h(1-h)x \quad \text{（Sigmoid函数的优雅性质）}\end{aligned}</script><h3 id="步骤5：梯度分量组合与化简（见证数学之美）"><a href="#步骤5：梯度分量组合与化简（见证数学之美）" class="headerlink" title="步骤5：梯度分量组合与化简（见证数学之美）"></a>步骤5：梯度分量组合与化简（见证数学之美）</h3><script type="math/tex; mode=display">\begin{aligned}\text{梯度分量} &= \left( \frac{1-y}{1-h} - \frac{y}{h} \right) \cdot h(1-h)x \\&= \left[ (1-y)h - y(1-h) \right]x \\&= (h - y)x \quad \text{（所有复杂项神奇抵消！）}\end{aligned}</script><h3 id="步骤6：整合全局梯度（全体样本协同计算）"><a href="#步骤6：整合全局梯度（全体样本协同计算）" class="headerlink" title="步骤6：整合全局梯度（全体样本协同计算）"></a>步骤6：整合全局梯度（全体样本协同计算）</h3><script type="math/tex; mode=display">\frac{\partial L}{\partial w} = \frac{1}{N}\sum_{i=1}^N (h^{(i)} - y^{(i)})x^{(i)}</script><h3 id="重要性质总结表"><a href="#重要性质总结表" class="headerlink" title="重要性质总结表"></a>重要性质总结表</h3><div class="table-container"><table><thead><tr><th>性质</th><th>数学表达式</th><th>物理意义</th></tr></thead><tbody><tr><td><strong>梯度公式</strong></td><td><script type="math/tex">\frac{\partial L}{\partial w} = \frac{1}{N}\sum (h-y)x</script></td><td>预测误差驱动物体参数调整</td></tr><tr><td><strong>Sigmoid导数</strong></td><td><script type="math/tex">\frac{d\sigma(z)}{dz} = \sigma(z)(1-\sigma(z))</script></td><td>自动生成正则化效果</td></tr><tr><td><strong>概率计算</strong></td><td>$$ P(y=1</td><td>x) = \frac{1}{1+e^{-w^Tx}} $$</td><td>完美映射到[0,1]概率空间</td></tr></tbody></table></div><p>省略大量微分细节，可得：</p><script type="math/tex; mode=display">\text{梯度}=\frac{1}{N}\sum_{i=1}^N(y^{(i)}-h(x^{(i)}))\bullet x^{(i)}</script><p>所以加入更新的速率以后可得：</p><script type="math/tex; mode=display">w=w-\alpha\cdot\frac{\partial}{\partial w}L(w)</script><script type="math/tex; mode=display">w=w-\frac{\alpha}{N}\sum_{i=1}^N(y^{(i)}-(w\bullet x^{(i)}))\bullet x^{(i)}</script><h2 id="五、通过逻辑回归解决二元分类问题"><a href="#五、通过逻辑回归解决二元分类问题" class="headerlink" title="五、通过逻辑回归解决二元分类问题"></a>五、通过逻辑回归解决二元分类问题</h2><h3 id="5-1数据准备"><a href="#5-1数据准备" class="headerlink" title="5.1数据准备"></a>5.1数据准备</h3><h4 id="5-1-1-数据读取"><a href="#5-1-1-数据读取" class="headerlink" title="5.1.1 数据读取"></a>5.1.1 数据读取</h4><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np <span class="comment"># 导入NumPy数学工具箱</span></span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd <span class="comment"># 导入Pandas数据处理工具箱</span></span><br><span class="line">df_heart = pd.read_csv(<span class="string">&quot;/kaggle/input/logistic-regression/heart.csv&quot;</span>)  <span class="comment"># 读取文件</span></span><br><span class="line">df_heart.head() <span class="comment"># 显示前5行数据</span></span><br></pre></td></tr></table></figure><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">df_heart.target.value_counts() <span class="comment"># 输出分类值，及各个类别数目</span></span><br></pre></td></tr></table></figure><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt <span class="comment"># 导入绘图工具</span></span><br><span class="line"><span class="comment"># 以年龄+最大心率作为输入，查看分类结果散点图</span></span><br><span class="line">plt.scatter(x=df_heart.age[df_heart.target==<span class="number">1</span>],</span><br><span class="line">            y=df_heart.thalach[(df_heart.target==<span class="number">1</span>)], c=<span class="string">&quot;red&quot;</span>)</span><br><span class="line">plt.scatter(x=df_heart.age[df_heart.target==<span class="number">0</span>],</span><br><span class="line">            y=df_heart.thalach[(df_heart.target==<span class="number">0</span>)], marker=<span class="string">&#x27;^&#x27;</span>)</span><br><span class="line">plt.legend([<span class="string">&quot;Disease&quot;</span>, <span class="string">&quot;No Disease&quot;</span>]) <span class="comment"># 显示图例</span></span><br><span class="line">plt.xlabel(<span class="string">&quot;Age&quot;</span>) <span class="comment"># X轴-Age</span></span><br><span class="line">plt.ylabel(<span class="string">&quot;Heart Rate&quot;</span>) <span class="comment"># Y轴-Heart Rate</span></span><br><span class="line">plt.show() <span class="comment"># 显示散点图</span></span><br></pre></td></tr></table></figure><p><img src="https://rozen-picture.oss-cn-beijing.aliyuncs.com/img/202502171628962.png" alt="image-20250217162800659"></p><h4 id="5-1-2-构建特征集和标签集"><a href="#5-1-2-构建特征集和标签集" class="headerlink" title="5.1.2 构建特征集和标签集"></a>5.1.2 构建特征集和标签集</h4><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">X = df_heart.drop([<span class="string">&#x27;target&#x27;</span>], axis = <span class="number">1</span>) <span class="comment"># 构建特征集</span></span><br><span class="line">y = df_heart.target.values <span class="comment"># 构建标签集</span></span><br><span class="line">y = y.reshape(-<span class="number">1</span>,<span class="number">1</span>) <span class="comment"># -1是相对索引，等价于len(y)</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;张量X的形状:&quot;</span>, X.shape)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;张量X的形状:&quot;</span>, y.shape)</span><br></pre></td></tr></table></figure><h4 id="5-1-3-拆分数据集"><a href="#5-1-3-拆分数据集" class="headerlink" title="5.1.3 拆分数据集"></a>5.1.3 拆分数据集</h4><p>按照80%/20%的比例准备训练集和测试集:</p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split</span><br><span class="line">X_train, X_test, y_train, y_test = train_test_split(X,y,test_size = <span class="number">0.2</span>)</span><br></pre></td></tr></table></figure><h4 id="5-1-4-数据特征缩放"><a href="#5-1-4-数据特征缩放" class="headerlink" title="5.1.4 数据特征缩放"></a>5.1.4 数据特征缩放</h4><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.preprocessing <span class="keyword">import</span> MinMaxScaler <span class="comment"># 导入数据缩放器</span></span><br><span class="line">scaler = MinMaxScaler() <span class="comment"># 选择归一化数据缩放器，MinMaxScaler</span></span><br><span class="line">X_train = scaler.fit_transform(X_train) <span class="comment"># 特征归一化 训练集fit_transform</span></span><br><span class="line">X_test = scaler.transform(X_test) <span class="comment"># 特征归一化 测试集transform</span></span><br></pre></td></tr></table></figure><p>仅就这个数据集而言，Min Max Scaler进行的数据特征缩放不仅不会提高效率，似乎还会令预测准确率下降。</p><p>这个结果提示我们：没有绝对正确的理论，实践才是检验真理的唯一标准。</p><h3 id="5-2-建立逻辑回归模型"><a href="#5-2-建立逻辑回归模型" class="headerlink" title="5.2 建立逻辑回归模型"></a>5.2 建立逻辑回归模型</h3><h4 id="5-1-1-逻辑函数"><a href="#5-1-1-逻辑函数" class="headerlink" title="5.1.1 逻辑函数"></a>5.1.1 逻辑函数</h4><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 首先定义一个Sigmoid函数，输入Z，返回y&#x27;</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">sigmoid</span>(<span class="params">z</span>):    </span><br><span class="line">    y_hat = <span class="number">1</span>/(<span class="number">1</span>+ np.exp(-z))</span><br><span class="line">    <span class="keyword">return</span> y_hat    </span><br></pre></td></tr></table></figure><h4 id="5-1-2-损失函数"><a href="#5-1-2-损失函数" class="headerlink" title="5.1.2 损失函数"></a>5.1.2 损失函数</h4><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 然后定义损失函数</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">loss_function</span>(<span class="params">X,y,w,b</span>):</span><br><span class="line">    y_hat = sigmoid(np.dot(X,w) + b) <span class="comment"># Sigmoid逻辑函数 + 线性函数(wX+b)得到y&#x27;</span></span><br><span class="line">    loss = -(y*np.log(y_hat) + (<span class="number">1</span>-y)*np.log(<span class="number">1</span>-y_hat)) <span class="comment"># 计算损失</span></span><br><span class="line">    cost = np.<span class="built_in">sum</span>(loss) / X.shape[<span class="number">0</span>]  <span class="comment"># 整个数据集平均损失    </span></span><br><span class="line">    <span class="keyword">return</span> cost <span class="comment"># 返回整个数据集平均损失</span></span><br></pre></td></tr></table></figure><script type="math/tex; mode=display">L(w,b)=-\frac{1}{N}\sum_{(x,y)\in D}\left[y^*\mathrm{log}(h(x))+(1-y)^*\mathrm{log}(1-h(x))\right]</script><h4 id="5-2-3-梯度下降的实现"><a href="#5-2-3-梯度下降的实现" class="headerlink" title="5.2.3 梯度下降的实现"></a>5.2.3 梯度下降的实现</h4><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 然后构建梯度下降的函数</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">gradient_descent</span>(<span class="params">X,y,w,b,lr,<span class="built_in">iter</span></span>) : <span class="comment">#定义逻辑回归梯度下降函数</span></span><br><span class="line">    l_history = np.zeros(<span class="built_in">iter</span>) <span class="comment"># 初始化记录梯度下降过程中误差值(损失)的数组</span></span><br><span class="line">    w_history = np.zeros((<span class="built_in">iter</span>,w.shape[<span class="number">0</span>],w.shape[<span class="number">1</span>])) <span class="comment"># 初始化权重记录的数组</span></span><br><span class="line">    b_history = np.zeros(<span class="built_in">iter</span>) <span class="comment"># 初始化记录梯度下降过程中偏置的数组  </span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">iter</span>): <span class="comment">#进行机器训练的迭代</span></span><br><span class="line">        y_hat = sigmoid(np.dot(X,w) + b) <span class="comment">#Sigmoid逻辑函数+线性函数(wX+b)得到y&#x27;</span></span><br><span class="line">        derivative_w = np.dot(X.T,((y_hat-y)))/X.shape[<span class="number">0</span>]  <span class="comment"># 给权重向量求导</span></span><br><span class="line">        derivative_b = np.<span class="built_in">sum</span>(y_hat-y)/X.shape[<span class="number">0</span>] <span class="comment"># 给偏置求导</span></span><br><span class="line">        w = w - lr * derivative_w <span class="comment"># 更新权重向量，lr即学习速率alpha</span></span><br><span class="line">        b = b - lr * derivative_b   <span class="comment"># 更新偏置，lr即学习速率alpha</span></span><br><span class="line">        l_history[i] =  loss_function(X,y,w,b) <span class="comment"># 梯度下降过程中的损失</span></span><br><span class="line">        <span class="built_in">print</span> (<span class="string">&quot;轮次&quot;</span>, i+<span class="number">1</span> , <span class="string">&quot;当前轮训练集损失：&quot;</span>,l_history[i]) </span><br><span class="line">        w_history[i] = w <span class="comment"># 梯度下降过程中权重的历史 请注意w_history和w的形状</span></span><br><span class="line">        b_history[i] = b <span class="comment"># 梯度下降过程中偏置的历史</span></span><br><span class="line">    <span class="keyword">return</span> l_history, w_history, b_history</span><br><span class="line"></span><br></pre></td></tr></table></figure><script type="math/tex; mode=display">\text{梯度}=\frac{1}{N}\sum_{i=1}^N(y^{(i)}-h(x^{(i)}))\bullet x^{(i)}</script><script type="math/tex; mode=display">w=w-\alpha\cdot\frac{\partial}{\partial w}L(w)</script><h4 id="5-2-4-分类预测的实现"><a href="#5-2-4-分类预测的实现" class="headerlink" title="5.2.4 分类预测的实现"></a>5.2.4 分类预测的实现</h4><p>定义一个负责分类预测的函数：</p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">predict</span>(<span class="params">X,w,b</span>): <span class="comment"># 定义预测函数</span></span><br><span class="line">    z = np.dot(X,w) + b <span class="comment"># 线性函数</span></span><br><span class="line">    y_hat = sigmoid(z) <span class="comment"># 逻辑函数转换</span></span><br><span class="line">    y_pred = np.zeros((y_hat.shape[<span class="number">0</span>],<span class="number">1</span>)) <span class="comment"># 初始化预测结果变量  </span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(y_hat.shape[<span class="number">0</span>]):</span><br><span class="line">        <span class="keyword">if</span> y_hat[i,<span class="number">0</span>] &lt; <span class="number">0.5</span>:</span><br><span class="line">            y_pred[i,<span class="number">0</span>] = <span class="number">0</span> <span class="comment"># 如果预测概率小于0.5，输出分类0</span></span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            y_pred[i,<span class="number">0</span>] = <span class="number">1</span> <span class="comment"># 如果预测概率大于0.5，输出分类1</span></span><br><span class="line">    <span class="keyword">return</span> y_pred <span class="comment"># 返回预测分类的结果</span></span><br></pre></td></tr></table></figure><h3 id="5-3-开始训练机器"><a href="#5-3-开始训练机器" class="headerlink" title="5.3 开始训练机器"></a>5.3 开始训练机器</h3><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">logistic_regression</span>(<span class="params">X,y,w,b,lr,<span class="built_in">iter</span></span>): <span class="comment"># 定义逻辑回归模型</span></span><br><span class="line">    l_history,w_history,b_history = gradient_descent(X,y,w,b,lr,<span class="built_in">iter</span>)<span class="comment">#梯度下降</span></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;训练最终损失:&quot;</span>, l_history[-<span class="number">1</span>]) <span class="comment"># 打印最终损失</span></span><br><span class="line">    y_pred = predict(X,w_history[-<span class="number">1</span>],b_history[-<span class="number">1</span>]) <span class="comment"># 进行预测</span></span><br><span class="line">    traning_acc = <span class="number">100</span> - np.mean(np.<span class="built_in">abs</span>(y_pred - y_train))*<span class="number">100</span> <span class="comment"># 计算准确率</span></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;逻辑回归训练准确率: &#123;:.2f&#125;%&quot;</span>.<span class="built_in">format</span>(traning_acc))  <span class="comment"># 打印准确率</span></span><br><span class="line">    <span class="keyword">return</span> l_history, w_history, b_history <span class="comment"># 返回训练历史记录</span></span><br></pre></td></tr></table></figure><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#初始化参数</span></span><br><span class="line">dimension = X.shape[<span class="number">1</span>] <span class="comment"># 这里的维度 len(X)是矩阵的行的数，维度是列的数目</span></span><br><span class="line">weight = np.full((dimension,<span class="number">1</span>),<span class="number">0.1</span>) <span class="comment"># 权重向量，向量一般是1D，但这里实际上创建了2D张量</span></span><br><span class="line">bias = <span class="number">0</span> <span class="comment"># 偏置值</span></span><br><span class="line"><span class="comment">#初始化超参数</span></span><br><span class="line">alpha = <span class="number">1</span> <span class="comment"># 学习速率</span></span><br><span class="line">iterations = <span class="number">500</span> <span class="comment"># 迭代次数</span></span><br></pre></td></tr></table></figure><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 用逻辑回归函数训练机器</span></span><br><span class="line">loss_history, weight_history, bias_history =  \</span><br><span class="line">            logistic_regression(X_train,y_train,weight,bias,alpha,iterations)</span><br></pre></td></tr></table></figure><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">y_pred = predict(X_test,weight_history[-<span class="number">1</span>],bias_history[-<span class="number">1</span>]) <span class="comment"># 预测测试集</span></span><br><span class="line">testing_acc = <span class="number">100</span> - np.mean(np.<span class="built_in">abs</span>(y_pred - y_test))*<span class="number">100</span> <span class="comment"># 计算准确率</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;逻辑回归测试准确率: &#123;:.2f&#125;%&quot;</span>.<span class="built_in">format</span>(testing_acc))</span><br></pre></td></tr></table></figure><figure class="highlight markdown"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">逻辑回归测试准确率: 85.25%</span><br></pre></td></tr></table></figure><h3 id="5-4-测试分类结果"><a href="#5-4-测试分类结果" class="headerlink" title="5.4 测试分类结果"></a>5.4 测试分类结果</h3><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">print</span> (<span class="string">&quot;逻辑回归预测分类值:&quot;</span>,predict(X_test,weight_history[-<span class="number">1</span>],bias_history[-<span class="number">1</span>]))</span><br></pre></td></tr></table></figure><h3 id="5-5-绘制损失曲线"><a href="#5-5-绘制损失曲线" class="headerlink" title="5.5 绘制损失曲线"></a>5.5 绘制损失曲线</h3><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">loss_history_test = np.zeros(iterations) <span class="comment"># 初始化历史损失</span></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(iterations): <span class="comment">#求训练过程中不同参数带来的测试集损失</span></span><br><span class="line">    loss_history_test[i] = loss_function(X_test,y_test,</span><br><span class="line">                                         weight_history[i],bias_history[i])</span><br><span class="line">index = np.arange(<span class="number">0</span>,iterations,<span class="number">1</span>)</span><br><span class="line">plt.plot(index, loss_history,c=<span class="string">&#x27;blue&#x27;</span>,linestyle=<span class="string">&#x27;solid&#x27;</span>)</span><br><span class="line">plt.plot(index, loss_history_test,c=<span class="string">&#x27;red&#x27;</span>,linestyle=<span class="string">&#x27;dashed&#x27;</span>)</span><br><span class="line">plt.legend([<span class="string">&quot;Training Loss&quot;</span>, <span class="string">&quot;Test Loss&quot;</span>])</span><br><span class="line">plt.xlabel(<span class="string">&quot;Number of Iteration&quot;</span>)</span><br><span class="line">plt.ylabel(<span class="string">&quot;Cost&quot;</span>)</span><br><span class="line">plt.show() <span class="comment"># 同时显示显示训练集和测试集损失曲线</span></span><br></pre></td></tr></table></figure><p><img src="https://rozen-picture.oss-cn-beijing.aliyuncs.com/img/202502171647147.png" alt="image-20250217164703879"></p><p>在迭代80～100次后，训练集的损失进一步下降，但是测试集的损失并没有跟着下降，反而显示呈上升趋势。</p><p>这是明显的过拟合现象。因此迭代应该在80—100结束。</p><h2 id="六、工业级代码实现"><a href="#六、工业级代码实现" class="headerlink" title="六、工业级代码实现"></a>六、工业级代码实现</h2><p>真正做项目的时候，其实没多少人这么去写代码，大家会直接调用库函数进搞定项目</p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.linear_model <span class="keyword">import</span> LogisticRegression <span class="comment">#导入逻辑回归模型</span></span><br><span class="line">lr = LogisticRegression() <span class="comment"># lr,就代表是逻辑回归模型</span></span><br><span class="line">lr.fit(X_train,y_train) <span class="comment"># fit,就相当于是梯度下降</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;SK-learn逻辑回归测试准确率&#123;:.2f&#125;%&quot;</span>.<span class="built_in">format</span>(lr.score(X_test,y_test)*<span class="number">100</span>))</span><br><span class="line"></span><br></pre></td></tr></table></figure><figure class="highlight markdown"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">SK-learn逻辑回归测试准确率85.25%</span><br></pre></td></tr></table></figure><p>这个准确率比我们之前的手写代码好很多，这是为什么呢？</p><h3 id="6-1哑特征"><a href="#6-1哑特征" class="headerlink" title="6.1哑特征"></a>6.1哑特征</h3><p>cp这个字段，它的意义是“胸痛类型”，取值为0、1、2、3。这些分类值，是大小无关的。</p><p>但是问题在于，计算机会把它们理解为数值，认为1和2与1和3之间的关系不是并列的，是后者差值比前者要大。</p><p>解决的方法，是把这种类别特征拆分成多个哑特征，比如cp有0、1、2、3这4类，就拆分成个4特征，cp_0为一个特征、cp_1为一个特征、cp_2为一个特征、cp_3为一个特征。每一个特征都还原成二元分类，答案是Yes或者No，也就是数值1或0。</p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 把3个文本型变量转换为哑变量</span></span><br><span class="line">a = pd.get_dummies(df_heart[<span class="string">&#x27;cp&#x27;</span>], prefix = <span class="string">&quot;cp&quot;</span>)</span><br><span class="line">b = pd.get_dummies(df_heart[<span class="string">&#x27;thal&#x27;</span>], prefix = <span class="string">&quot;thal&quot;</span>)</span><br><span class="line">c = pd.get_dummies(df_heart[<span class="string">&#x27;slope&#x27;</span>], prefix = <span class="string">&quot;slope&quot;</span>)</span><br><span class="line"><span class="comment"># 把哑变量添加进dataframe</span></span><br><span class="line">frames = [df_heart, a, b, c]</span><br><span class="line">df_heart = pd.concat(frames, axis = <span class="number">1</span>)</span><br><span class="line">df_heart = df_heart.drop(columns = [<span class="string">&#x27;cp&#x27;</span>, <span class="string">&#x27;thal&#x27;</span>, <span class="string">&#x27;slope&#x27;</span>])</span><br><span class="line">df_heart.head() <span class="comment"># 显示新的dataframe</span></span><br></pre></td></tr></table></figure><h3 id="6-2二分类实战：信用卡欺诈检测"><a href="#6-2二分类实战：信用卡欺诈检测" class="headerlink" title="6.2二分类实战：信用卡欺诈检测"></a>6.2二分类实战：信用卡欺诈检测</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> StratifiedKFold</span><br><span class="line"></span><br><span class="line"><span class="comment"># 读取真实工业数据集</span></span><br><span class="line">df = pd.read_csv(<span class="string">&#x27;creditcard.csv&#x27;</span>)</span><br><span class="line">X = df.drop([<span class="string">&#x27;Class&#x27;</span>,<span class="string">&#x27;Time&#x27;</span>], axis=<span class="number">1</span>)</span><br><span class="line">y = df[<span class="string">&#x27;Class&#x27;</span>]</span><br><span class="line"></span><br><span class="line"><span class="comment"># 分层抽样保持样本分布</span></span><br><span class="line">skf = StratifiedKFold(n_splits=<span class="number">5</span>)</span><br><span class="line"><span class="keyword">for</span> train_index, test_index <span class="keyword">in</span> skf.split(X, y):</span><br><span class="line">    X_train, X_test = X.iloc[train_index], X.iloc[test_index]</span><br><span class="line">    y_train, y_test = y.iloc[train_index], y.iloc[test_index]</span><br><span class="line"></span><br><span class="line"><span class="comment"># 带类别权重的模型</span></span><br><span class="line">model = LogisticRegression(class_weight=&#123;<span class="number">0</span>:<span class="number">1</span>, <span class="number">1</span>:<span class="number">10</span>&#125;, </span><br><span class="line">                          penalty=<span class="string">&#x27;l1&#x27;</span>, solver=<span class="string">&#x27;saga&#x27;</span>)</span><br><span class="line">model.fit(X_train, y_train)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 输出特征重要性排序</span></span><br><span class="line">importance = pd.DataFrame(&#123;<span class="string">&#x27;feature&#x27;</span>:X.columns, </span><br><span class="line">                          <span class="string">&#x27;coef&#x27;</span>:model.coef_[<span class="number">0</span>]&#125;)</span><br><span class="line"><span class="built_in">print</span>(importance.sort_values(<span class="string">&#x27;coef&#x27;</span>, ascending=<span class="literal">False</span>))</span><br></pre></td></tr></table></figure><h3 id="6-3-多分类实战：手写数字识别（MNIST）"><a href="#6-3-多分类实战：手写数字识别（MNIST）" class="headerlink" title="6.3 多分类实战：手写数字识别（MNIST）"></a>6.3 多分类实战：手写数字识别（MNIST）</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.datasets <span class="keyword">import</span> fetch_openml</span><br><span class="line"><span class="keyword">from</span> sklearn.preprocessing <span class="keyword">import</span> MinMaxScaler</span><br><span class="line"></span><br><span class="line"><span class="comment"># 加载MNIST数据集</span></span><br><span class="line">mnist = fetch_openml(<span class="string">&#x27;mnist_784&#x27;</span>, version=<span class="number">1</span>)</span><br><span class="line">X, y = mnist[<span class="string">&quot;data&quot;</span>], mnist[<span class="string">&quot;target&quot;</span>].astype(np.uint8)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 数据标准化</span></span><br><span class="line">scaler = MinMaxScaler()</span><br><span class="line">X_scaled = scaler.fit_transform(X[:<span class="number">10000</span>]) <span class="comment"># 抽样加速训练</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># OVR多分类策略</span></span><br><span class="line">model = LogisticRegression(multi_class=<span class="string">&#x27;ovr&#x27;</span>, </span><br><span class="line">                          penalty=<span class="string">&#x27;l2&#x27;</span>, </span><br><span class="line">                          max_iter=<span class="number">1000</span>)</span><br><span class="line">model.fit(X_scaled, y[:<span class="number">10000</span>])</span><br><span class="line"></span><br><span class="line"><span class="comment"># 显示预测结果样例</span></span><br><span class="line">plt.figure(figsize=(<span class="number">12</span>,<span class="number">6</span>))</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">10</span>):</span><br><span class="line">    plt.subplot(<span class="number">2</span>,<span class="number">5</span>,i+<span class="number">1</span>)</span><br><span class="line">    plt.imshow(X[i].reshape(<span class="number">28</span>,<span class="number">28</span>), cmap=<span class="string">&#x27;gray&#x27;</span>)</span><br><span class="line">    plt.title(<span class="string">f&#x27;Pred: <span class="subst">&#123;model.predict([X_scaled[i]])[<span class="number">0</span>]&#125;</span>&#x27;</span>)</span><br><span class="line">plt.tight_layout()</span><br></pre></td></tr></table></figure>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;机器学习—逻辑回归&quot;&gt;&lt;a href=&quot;#机器学习—逻辑回归&quot; class=&quot;headerlink&quot; title=&quot;机器学习—逻辑回归&quot;&gt;&lt;/a&gt;机器学习—逻辑回归&lt;/h1&gt;&lt;hr&gt;
&lt;h2 id=&quot;一、认知革命：从线性回归到逻辑回归&quot;&gt;&lt;a href=&quot;#一、认</summary>
      
    
    
    
    
  </entry>
  
  <entry>
    <title></title>
    <link href="https://rozen12123.github.io/posts/0.html"/>
    <id>https://rozen12123.github.io/posts/0.html</id>
    <published>2025-02-16T12:26:30.657Z</published>
    <updated>2025-02-16T14:11:11.162Z</updated>
    
    <content type="html"><![CDATA[<h1 id="机器学习—-实现多元线性回归模型"><a href="#机器学习—-实现多元线性回归模型" class="headerlink" title="机器学习—-实现多元线性回归模型"></a>机器学习—-实现多元线性回归模型</h1><p>本节顺延<code>机器学习--线性回归</code>中的内容，进一步讨论多元函数的回归问题</p><script type="math/tex; mode=display">y^{\prime}=h(x)+w^\top\bullet x+b</script><p>$\text{其中,}w^\mathrm{T}\cdot x\text{就是}_{W_1X_1}+w_2X_2+w_3X_3+\cdots+w_NX_N$</p><p>进一步按题目简化：</p><script type="math/tex; mode=display">y^{\prime}=h(x)=w_0x_0+w_1x_1+w_2x_2+w_3x_3</script><p>其中$w_0x_0$为引入的偏置b</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np <span class="comment">#导入NumPy数学工具箱</span></span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd <span class="comment">#导入Pandas数据处理工具箱</span></span><br><span class="line"><span class="comment">#读入数据并显示前面几行的内容，确保已经成功的读入数据</span></span><br><span class="line"><span class="comment">#示例代码是在Kaggle中数据集中读入文件，如果在本机中需要指定具体本地路径</span></span><br><span class="line"><span class="comment"># 如，当数据集和代码文件位于相同本地目录，路径</span></span><br><span class="line"><span class="comment">#名应为&#x27;./advertising.csv&#x27;，或直接放&#x27;advertising.csv&#x27;亦可</span></span><br><span class="line">df_ads = pd.read_csv(<span class="string">&#x27;/kaggle/input/online-store-sales-forecast-data/advertising.csv&#x27;</span>)</span><br><span class="line">df_ads.head()</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">X = np.array(df_ads) <span class="comment"># 构建特征集，含全部特征</span></span><br><span class="line">X = np.delete(X, [<span class="number">3</span>], axis = <span class="number">1</span>) <span class="comment"># 删除掉标签</span></span><br><span class="line">y = np.array(df_ads.sales) <span class="comment">#构建标签集，销售金额</span></span><br><span class="line"><span class="built_in">print</span> (<span class="string">&quot;张量X的阶:&quot;</span>,X.ndim)</span><br><span class="line"><span class="built_in">print</span> (<span class="string">&quot;张量X的形状:&quot;</span>, X.shape)</span><br><span class="line"><span class="built_in">print</span> (X)</span><br></pre></td></tr></table></figure><p>需要x和y都为2D向量</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">y = y.reshape(-<span class="number">1</span>,<span class="number">1</span>) <span class="comment">#通过reshape函数把向量转换为矩阵，-1就是len(y),返回样本个数</span></span><br><span class="line"><span class="built_in">print</span> (<span class="string">&quot;张量y的形状:&quot;</span>, y.shape)</span><br></pre></td></tr></table></figure><h2 id="将数据集进行80-（训练集）和20-（验证集）的分割"><a href="#将数据集进行80-（训练集）和20-（验证集）的分割" class="headerlink" title="将数据集进行80%（训练集）和20%（验证集）的分割"></a>将数据集进行80%（训练集）和20%（验证集）的分割</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 将数据集进行80%（训练集）和20%（验证集）的分割</span></span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split</span><br><span class="line">X_train, X_test, y_train, y_test = train_test_split(X, y, </span><br><span class="line">                                   test_size=<span class="number">0.2</span>, random_state=<span class="number">0</span>)</span><br></pre></td></tr></table></figure><h2 id="定义归一化函数-，进行数据-压缩"><a href="#定义归一化函数-，进行数据-压缩" class="headerlink" title="定义归一化函数 ，进行数据 ==压缩=="></a>定义归一化函数 ，进行数据 ==压缩==</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">scaler</span>(<span class="params">train, test</span>): <span class="comment"># 定义归一化函数 ，进行数据压缩    </span></span><br><span class="line">    <span class="comment"># 数据的压缩</span></span><br><span class="line">    <span class="built_in">min</span> = train.<span class="built_in">min</span>(axis=<span class="number">0</span>) <span class="comment"># 训练集最小值</span></span><br><span class="line">    <span class="built_in">max</span> = train.<span class="built_in">max</span>(axis=<span class="number">0</span>) <span class="comment"># 训练集最大值</span></span><br><span class="line">    gap = <span class="built_in">max</span> - <span class="built_in">min</span> <span class="comment"># 最大值和最小值的差</span></span><br><span class="line">    train -= <span class="built_in">min</span> <span class="comment"># 所有数据减最小值</span></span><br><span class="line">    train /= gap <span class="comment"># 所有数据除以大小值差</span></span><br><span class="line">    test -= <span class="built_in">min</span> <span class="comment">#把训练集最小值应用于测试集</span></span><br><span class="line">    test /= gap <span class="comment">#把训练集大小值差应用于测试集</span></span><br><span class="line">    <span class="keyword">return</span> train, test <span class="comment"># 返回压缩后的数据</span></span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">min_max_gap</span>(<span class="params">train</span>): <span class="comment"># 计算训练集最大，最小值以及他们的差，用于后面反归一化过程</span></span><br><span class="line">    <span class="built_in">min</span> = train.<span class="built_in">min</span>(axis=<span class="number">0</span>) <span class="comment"># 训练集最小值</span></span><br><span class="line">    <span class="built_in">max</span> = train.<span class="built_in">max</span>(axis=<span class="number">0</span>) <span class="comment"># 训练集最大值</span></span><br><span class="line">    gap = <span class="built_in">max</span> - <span class="built_in">min</span> <span class="comment"># 最大值和最小值的差</span></span><br><span class="line">    <span class="keyword">return</span> <span class="built_in">min</span>, <span class="built_in">max</span>, gap</span><br><span class="line">    </span><br><span class="line">y_min, y_max, y_gap = min_max_gap(y_train)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">X_train_original = X_train.copy() <span class="comment"># 保留一份训练集数据副本，用于对要预测数据归一化</span></span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">X_train,X_test = scaler(X_train,X_test) <span class="comment"># 对特征归一化</span></span><br><span class="line">y_train,y_test = scaler(y_train,y_test) <span class="comment"># 对标签也归一化</span></span><br></pre></td></tr></table></figure><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">x0_train = np.ones((<span class="built_in">len</span>(X_train),<span class="number">1</span>)) <span class="comment"># 构造X_train长度的全1数组配合对Bias的点积</span></span><br><span class="line">X_train = np.append(x0_train, X_train, axis=<span class="number">1</span>) <span class="comment">#把X增加一系列的1</span></span><br><span class="line">x0_test = np.ones((<span class="built_in">len</span>(X_test),<span class="number">1</span>)) <span class="comment"># 构造X_test长度的全1数组配合对Bias的点积</span></span><br><span class="line">X_test = np.append(x0_test, X_test, axis=<span class="number">1</span>) <span class="comment">#把X增加一系列的1</span></span><br><span class="line"><span class="built_in">print</span> (<span class="string">&quot;张量X的形状:&quot;</span>, X_train.shape)</span><br><span class="line"><span class="built_in">print</span> (X_train)</span><br></pre></td></tr></table></figure><h2 id="通过向量化来实现损失函数"><a href="#通过向量化来实现损失函数" class="headerlink" title="通过向量化来实现损失函数"></a>通过向量化来实现损失函数</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">loss_function</span>(<span class="params">X, y, W</span>): <span class="comment"># 手工定义一个MSE均方误差函数,W此时是一个向量</span></span><br><span class="line">    y_hat = X.dot(W.T) <span class="comment"># 点积运算 h(x)=w_0*x_0 + w_1*x_1 + w_2*x_2 + w_3*x_3    </span></span><br><span class="line">    loss = y_hat.reshape((<span class="built_in">len</span>(y_hat),<span class="number">1</span>))-y <span class="comment"># 中间过程,求出当前W和真值的差异</span></span><br><span class="line">    cost = np.<span class="built_in">sum</span>(loss**<span class="number">2</span>)/(<span class="number">2</span>*<span class="built_in">len</span>(X)) <span class="comment"># 这是平方求和过程, 均方误差函数的代码实现</span></span><br><span class="line">    <span class="keyword">return</span> cost <span class="comment"># 返回当前模型的均方误差值</span></span><br></pre></td></tr></table></figure><script type="math/tex; mode=display">w=w-\frac{\alpha}{2N}\sum_{i=1}^N(y^{(i)}-(w\bullet x^{(i)}))\bullet x^{(i)}</script><h2 id="封装进一个梯度下降函数："><a href="#封装进一个梯度下降函数：" class="headerlink" title="封装进一个梯度下降函数："></a>封装进一个梯度下降函数：</h2><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">gradient_descent</span>(<span class="params">X, y, W, lr, iterations</span>): <span class="comment"># 定义梯度下降函数</span></span><br><span class="line">    l_history = np.zeros(iterations) <span class="comment"># 初始化记录梯度下降过程中损失的数组</span></span><br><span class="line">    W_history = np.zeros((iterations,<span class="built_in">len</span>(W))) <span class="comment"># 初始化权重数组 </span></span><br><span class="line">    <span class="keyword">for</span> <span class="built_in">iter</span> <span class="keyword">in</span> <span class="built_in">range</span>(iterations): <span class="comment"># 进行梯度下降的迭代，就是下多少级台阶</span></span><br><span class="line">        y_hat = X.dot(W.T) <span class="comment"># 这个是向量化运行实现的假设函数   </span></span><br><span class="line">        loss = y_hat.reshape((<span class="built_in">len</span>(y_hat),<span class="number">1</span>))-y <span class="comment"># 中间过程, y_hat和y真值的差</span></span><br><span class="line">        derivative_W = X.T.dot(loss)/<span class="built_in">len</span>(X) <span class="comment">#求出多项式的梯度向量</span></span><br><span class="line">        derivative_W = derivative_W.reshape(<span class="built_in">len</span>(W)) </span><br><span class="line">        W = W - lr*derivative_W <span class="comment"># 结合下降速率更新权重</span></span><br><span class="line">        l_history[<span class="built_in">iter</span>] = loss_function(X, y, W) <span class="comment"># 损失的历史记录 </span></span><br><span class="line">        W_history[<span class="built_in">iter</span>] = W <span class="comment"># 梯度下降过程中权重的历史记录</span></span><br><span class="line">    <span class="keyword">return</span> l_history, W_history <span class="comment"># 返回梯度下降过程数据</span></span><br></pre></td></tr></table></figure><h2 id="初始化权重并训练机器"><a href="#初始化权重并训练机器" class="headerlink" title="初始化权重并训练机器"></a>初始化权重并训练机器</h2><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#首先确定参数的初始值</span></span><br><span class="line">iterations = <span class="number">300</span>; <span class="comment"># 迭代300次</span></span><br><span class="line">alpha = <span class="number">0.15</span>; <span class="comment">#学习速率设为0.15</span></span><br><span class="line">weight = np.array([<span class="number">0.5</span>,<span class="number">1</span>,<span class="number">1</span>,<span class="number">1</span>]) <span class="comment"># 权重向量，w[0] = bias</span></span><br><span class="line"><span class="comment">#计算一下初始值的损失</span></span><br><span class="line"><span class="built_in">print</span> (<span class="string">&#x27;当前损失：&#x27;</span>,loss_function(X_train, y_train, weight))</span><br></pre></td></tr></table></figure><figure class="highlight markdown"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">当前损失： 0.8039183733604858</span><br></pre></td></tr></table></figure><h2 id="构建线性回归模型"><a href="#构建线性回归模型" class="headerlink" title="构建线性回归模型"></a>构建线性回归模型</h2><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 定义线性回归模型</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">linear_regression</span>(<span class="params">X, y, weight, alpha, iterations</span>): </span><br><span class="line">    loss_history, weight_history = gradient_descent(X, y, </span><br><span class="line">                                                    weight, </span><br><span class="line">                                                    alpha, iterations)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;训练最终损失:&quot;</span>, loss_history[-<span class="number">1</span>]) <span class="comment"># 打印最终损失</span></span><br><span class="line">    y_pred = X.dot(weight_history[-<span class="number">1</span>]) <span class="comment"># 进行预测</span></span><br><span class="line">    traning_acc = <span class="number">100</span> - np.mean(np.<span class="built_in">abs</span>(y_pred - y))*<span class="number">100</span> <span class="comment"># 计算准确率</span></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;线性回归训练准确率: &#123;:.2f&#125;%&quot;</span>.<span class="built_in">format</span>(traning_acc))  <span class="comment"># 打印准确率</span></span><br><span class="line">    <span class="keyword">return</span> loss_history, weight_history <span class="comment"># 返回训练历史记录</span></span><br></pre></td></tr></table></figure><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 调用刚才定义的线性回归模型</span></span><br><span class="line">loss_history, weight_history = linear_regression(X_train, y_train,</span><br><span class="line">                           weight, alpha, iterations) <span class="comment">#训练机器</span></span><br></pre></td></tr></table></figure><figure class="highlight markdown"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">训练最终损失: 0.002506723466186024</span><br><span class="line">线性回归训练准确率: 75.67%</span><br></pre></td></tr></table></figure><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">print</span>(<span class="string">&quot;权重历史记录：&quot;</span>, weight_history)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;损失历史记录：&quot;</span>, loss_history)</span><br></pre></td></tr></table></figure><h2 id="预测的数据"><a href="#预测的数据" class="headerlink" title="预测的数据"></a>预测的数据</h2><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">X_plan = [<span class="number">250</span>,<span class="number">50</span>,<span class="number">50</span>] <span class="comment"># 要预测的X特征数据</span></span><br><span class="line">X_train,X_plan = scaler(X_train_original,X_plan) <span class="comment"># 对预测数据也要归一化缩放</span></span><br><span class="line">X_plan = np.append([<span class="number">1</span>], X_plan ) <span class="comment"># 加一个哑特征X0 = 1</span></span><br><span class="line">y_plan = np.dot(weight_history[-<span class="number">1</span>],X_plan) <span class="comment"># [-1] 即模型收敛时的权重</span></span><br><span class="line"><span class="comment"># 对预测结果要做反向缩放，才能得到与原始广告费用对应的预测值</span></span><br><span class="line">y_value = y_plan*y_gap + y_min <span class="comment"># y_gap是当前y_train中最大值和最小值的差，y_min是最小值</span></span><br><span class="line"><span class="built_in">print</span> (<span class="string">&quot;预计商品销售额： &quot;</span>,y_value, <span class="string">&quot;千元&quot;</span>) </span><br></pre></td></tr></table></figure><figure class="highlight markdown"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">预计商品销售额：  [7.42162744] 千元</span><br></pre></td></tr></table></figure>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;机器学习—-实现多元线性回归模型&quot;&gt;&lt;a href=&quot;#机器学习—-实现多元线性回归模型&quot; class=&quot;headerlink&quot; title=&quot;机器学习—-实现多元线性回归模型&quot;&gt;&lt;/a&gt;机器学习—-实现多元线性回归模型&lt;/h1&gt;&lt;p&gt;本节顺延&lt;code&gt;机器学习</summary>
      
    
    
    
    
  </entry>
  
  <entry>
    <title></title>
    <link href="https://rozen12123.github.io/posts/0.html"/>
    <id>https://rozen12123.github.io/posts/0.html</id>
    <published>2024-12-12T14:45:49.533Z</published>
    <updated>2025-02-16T10:15:15.534Z</updated>
    
    <content type="html"><![CDATA[<h1 id="机器学习—线性回归"><a href="#机器学习—线性回归" class="headerlink" title="机器学习—线性回归"></a>机器学习—线性回归</h1><p>所谓回归分析（regression analysis），是确定两种或两种以上变量间相互依赖的定量关系的一种统计分析方法，也就是研究当自变量x变化时，因变量y以何种形式在变化。在机器学习领域，回归应用于被预测对象具有连续值特征的情况（如客流量、降雨量、销售量等）。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">graph TD</span><br><span class="line">    A[线性回归] --&gt; B[1.一个回归问题的定义]</span><br><span class="line">    A --&gt; C[2.数据的收集、分析和预处理]</span><br><span class="line">    A --&gt; D[3.如何建立机器学习模型]</span><br><span class="line">    A --&gt; E[4.如何通过梯度下降找到最优参数]</span><br><span class="line">    A --&gt; F[5.线性回归模型的实践]</span><br><span class="line">    F --&gt; G[5.1一元（单变量）线性回归模型]</span><br><span class="line">    F --&gt; H[5.2多元（多变量）线性回归模型]</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>在机器学习的线性回归分析中，如果只包括一个自变量（特征x）和一个因变量（标签y），且两者的关系可用一条直线近似表示，这种回归分析就称为==一元线性回归==分析。如果回归分析中包括两个或两个以上的自变量，且因变量和自变量之间是线性关系，则称为==多元线性回归==分析。</p><h2 id="举一个栗子"><a href="#举一个栗子" class="headerlink" title="举一个栗子"></a>举一个栗子</h2><p>通过机器学习算法，根据过去记录下来的广告投放金额和商品销售额，来预测在未来的某个节点，一个特定的广告投放金额对应能实现的商品销售额。</p><p>（1）明确定义所要解决的问题——网店销售额的预测。</p><p>（2）在数据的收集和预处理环节，分5个小节完成数据的预处理工作，分别如下。</p><p>■将收集到的数据可视化，显示出来看一看。</p><p>■做特征工程，使数据更容易被机器处理。</p><p>■拆分数据集为训练集和测试集。</p><p>■做特征缩放，把数据值压缩到比较小的区间。</p><p>（3）选择机器学习模型的环节，其中有3个主要内容。</p><p>■确定机器学习的算法—这里也就是线性回归算法。</p><p>■确定线性回归算法的==假设函数==。</p><p>■确定线性回归算法的==损失函数==。</p><p>（4）通过==梯度下降==训练机器，确定模型内部参数的过程。</p><p>（5）进行超参数调试和性能优化。</p><h2 id="数据的收集和预处理"><a href="#数据的收集和预处理" class="headerlink" title="数据的收集和预处理"></a>数据的收集和预处理</h2><h3 id="收集网店销销售额数据"><a href="#收集网店销销售额数据" class="headerlink" title="收集网店销销售额数据"></a>收集网店销销售额数据</h3><p><img src="https://rozen-picture.oss-cn-beijing.aliyuncs.com/img/202412121912492.png" alt="image-20241212191230354"></p><h3 id="数据可视化"><a href="#数据可视化" class="headerlink" title="数据可视化"></a>数据可视化</h3><p>上传的数据要读取注意要填写好位置</p><p><img src="https://rozen-picture.oss-cn-beijing.aliyuncs.com/img/202412121925526.png" alt="image-20241212192531413"></p><h2 id="数据相关分析"><a href="#数据相关分析" class="headerlink" title="数据相关分析"></a>数据相关分析</h2><h3 id="导入数据可视化所需要的库"><a href="#导入数据可视化所需要的库" class="headerlink" title="导入数据可视化所需要的库"></a>导入数据可视化所需要的库</h3><p>相关分析（correlation analysis）：正值表示正相关，负值表示负相关。数值越大，相关性越强。</p><p>如果a和b的相关性系数是1，则a和b总是相等的。</p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt <span class="comment">#Matplotlib – Python画图工具库</span></span><br><span class="line"><span class="keyword">import</span> seaborn <span class="keyword">as</span> sns <span class="comment">#Seaborn – 统计学数据可视化工具库</span></span><br><span class="line"><span class="comment">#对所有的标签和特征两两显示其相关性的热力图(heatmap)</span></span><br><span class="line">sns.heatmap(df_ads.corr(), cmap=<span class="string">&quot;YlGnBu&quot;</span>, annot = <span class="literal">True</span>)</span><br><span class="line">plt.show() <span class="comment">#plt代表英文plot,就是画图的意思</span></span><br></pre></td></tr></table></figure><p><img src="https://rozen-picture.oss-cn-beijing.aliyuncs.com/img/202412121929811.png" alt="image-20241212192906724"></p><p>初步分析可得微信公众号里面做广告是最为合理的选择。</p><h3 id="数据的散点图"><a href="#数据的散点图" class="headerlink" title="数据的散点图"></a>数据的散点图</h3><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#显示销量和各种广告投放量的散点图</span></span><br><span class="line">sns.pairplot(df_ads, </span><br><span class="line">             x_vars=[<span class="string">&#x27;wechat&#x27;</span>, <span class="string">&#x27;weibo&#x27;</span>, <span class="string">&#x27;others&#x27;</span>], </span><br><span class="line">             y_vars=<span class="string">&#x27;sales&#x27;</span>, </span><br><span class="line">             height=<span class="number">4</span>, aspect=<span class="number">1</span>, kind=<span class="string">&#x27;scatter&#x27;</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p><img src="https://rozen-picture.oss-cn-beijing.aliyuncs.com/img/202412121932191.png" alt="image-20241212193237081"></p><h3 id="数据集清洗和规范化"><a href="#数据集清洗和规范化" class="headerlink" title="数据集清洗和规范化"></a>数据集清洗和规范化</h3><p>在本案例的3个特征中，微信广告投放金额和商品销售额的相关性比较高。</p><p>为了简化模型，我们只留下微信广告投放金额数据。这样，就把多变量的回归分析简化为==单变量的回归==分析。</p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">X = np.array(df_ads.wechat) <span class="comment">#构建特征集，只含有微信广告一个特征</span></span><br><span class="line">y = np.array(df_ads.sales) <span class="comment">#构建标签集，销售金额</span></span><br><span class="line"><span class="built_in">print</span> (<span class="string">&quot;张量X的阶:&quot;</span>,X.ndim)</span><br><span class="line"><span class="built_in">print</span> (<span class="string">&quot;张量X的形状:&quot;</span>, X.shape)</span><br><span class="line"><span class="built_in">print</span> (<span class="string">&quot;张量X的内容:&quot;</span>, X)</span><br></pre></td></tr></table></figure><p><img src="https://rozen-picture.oss-cn-beijing.aliyuncs.com/img/202412121935089.png" alt="image-20241212193538001"></p><p>目前X数组中只有一个特征，张量的阶为1，这个1D的特征张量不是机器学习算法能够接受的格式</p><p>对于回归问题的数值类型数据集，机器学习模型所读入的规范格式应该是2D张量，也就是矩阵，其形状为 （样本数，标签数）。</p><p>那么就现在的特征张量X而言，则是要把它的形状从（200，）变成（200，1），然后再进行机器学习。</p><p>因此需要用reshape方法给上面的张量变形：</p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">X = X.reshape((<span class="built_in">len</span>(X),<span class="number">1</span>)) <span class="comment">#通过reshape函数把向量转换为矩阵，len函数返回样本个数</span></span><br><span class="line">y = y.reshape((<span class="built_in">len</span>(y),<span class="number">1</span>)) <span class="comment">#通过reshape函数把向量转换为矩阵，len函数返回样本个数</span></span><br><span class="line"><span class="built_in">print</span> (<span class="string">&quot;张量X的阶:&quot;</span>,X.ndim)</span><br><span class="line"><span class="built_in">print</span> (<span class="string">&quot;张量X的形状:&quot;</span>, X.shape)</span><br><span class="line"><span class="built_in">print</span> (<span class="string">&quot;张量X的内容:&quot;</span>, X)</span><br></pre></td></tr></table></figure><p><img src="https://rozen-picture.oss-cn-beijing.aliyuncs.com/img/202412121940146.png" alt="image-20241212194037090"></p><p>&nbsp;</p><h3 id="拆分为训练集和测试集"><a href="#拆分为训练集和测试集" class="headerlink" title="拆分为训练集和测试集"></a>拆分为训练集和测试集</h3><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#将数据集进行80%(训练集)和20%(测试集)的分割</span></span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split</span><br><span class="line">X_train, X_test, y_train, y_test = train_test_split(X, y, </span><br><span class="line">                                   test_size=<span class="number">0.2</span>, random_state=<span class="number">0</span>)</span><br></pre></td></tr></table></figure><p>Sklearn中的<code>train_test_split</code>函数，是机器学习中拆分数据集的常用工具。</p><p>test_size=0.2，表示拆分出来的测试集占总样本量的20%。</p><p>如果用print语句输出拆分之后的新数据集（如X_train、X_test）的内容，会发现这个工具已经为数据集进行了乱序（重新随机排序）的工作，因为其中的shuffle参数默认值为True。</p><p>而其中的<code>random_state</code>参数，则用于数据集拆分过程的随机化设</p><p>定。如果指定了一个整数，那么这个数叫作随机化种子，每次设定固定</p><p>的种子能够保证得到同样的训练集和测试集，否则进行随机分割。</p><p>&nbsp;</p><h3 id="数据归一化"><a href="#数据归一化" class="headerlink" title="数据归一化"></a>数据归一化</h3><p>归一化是按比例的线性缩放。==数据分布不变==，但是都落入一个小的特定区间，比如0～1或者-1～+1。</p><p><img src="https://rozen-picture.oss-cn-beijing.aliyuncs.com/img/202502161608751.png" alt="image-20250216160806652"></p><script type="math/tex; mode=display">x^{\prime}=\frac{x-\min(x)}{\max(x)-\min(x)}</script><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">scaler</span>(<span class="params">train, test</span>): <span class="comment">#定义归一化函数，进行数据压缩    </span></span><br><span class="line">    <span class="built_in">min</span> = train.<span class="built_in">min</span>(axis=<span class="number">0</span>) <span class="comment">#训练集最小值</span></span><br><span class="line">    <span class="built_in">max</span> = train.<span class="built_in">max</span>(axis=<span class="number">0</span>) <span class="comment">#训练集最大值</span></span><br><span class="line">    gap = <span class="built_in">max</span> - <span class="built_in">min</span> <span class="comment">#最大值和最小值的差</span></span><br><span class="line">    train -= <span class="built_in">min</span> <span class="comment">#所有数据减最小值</span></span><br><span class="line">    train /= gap <span class="comment">#所有数据除以大小值差</span></span><br><span class="line">    test -= <span class="built_in">min</span> <span class="comment">#把训练集最小值应用于测试集</span></span><br><span class="line">    test /= gap <span class="comment">#把训练集大小值差应用于测试集</span></span><br><span class="line">    <span class="keyword">return</span> train, test <span class="comment">#返回压缩后的数据</span></span><br></pre></td></tr></table></figure><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">X_train,X_test = scaler(X_train,X_test) <span class="comment">#对特征归一化</span></span><br><span class="line">y_train,y_test = scaler(y_train,y_test) <span class="comment">#对标签也归一化</span></span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#用之前已经导入的matplotlib.pyplot中的plot方法显示散点图</span></span><br><span class="line">plt.plot(X_train,y_train,<span class="string">&#x27;r.&#x27;</span>, label=<span class="string">&#x27;Training data&#x27;</span>) </span><br><span class="line">plt.xlabel(<span class="string">&#x27;Wechat Ads&#x27;</span>) <span class="comment"># x轴Label</span></span><br><span class="line">plt.ylabel(<span class="string">&#x27;Sales&#x27;</span>) <span class="comment"># y轴Label</span></span><br><span class="line">plt.legend() <span class="comment"># 显示图例</span></span><br><span class="line">plt.show() <span class="comment"># 显示绘图结果</span></span><br></pre></td></tr></table></figure><p><img src="https://rozen-picture.oss-cn-beijing.aliyuncs.com/img/202502161620655.png" alt="image-20250216162005545"></p><h2 id="选择机器学习模型"><a href="#选择机器学习模型" class="headerlink" title="选择机器学习模型"></a>选择机器学习模型</h2><p>图中的一元线性特征很明显，所以用一元线性函数就可以进行拟合。</p><p>如果模型的预测完全准确，则损失（loss）为0；如果不准确，就有损失。</p><p>根据loss创建的函数也叫损失函数。</p><p><img src="https://rozen-picture.oss-cn-beijing.aliyuncs.com/img/202502161624200.png" alt="image-20250216162433097"></p><p>在回归中，损失函数经常是：均方误差，平均绝对误差，平均偏差误差等</p><p>在分类中，损失函数经常是：交叉熵损失，多分类SVM损失，等</p><p>在这个工程中，我们用MES函数，即<code>最小二乘法</code></p><script type="math/tex; mode=display">L(w,b)=MSE=\frac{1}{2N}\sum_{(x,y)\in D}(y-h(x))^2</script><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">loss_function</span>(<span class="params">X, y, weight, bias</span>): <span class="comment"># 手工定义一个MSE均方误差函数</span></span><br><span class="line">    y_hat = weight*X + bias <span class="comment"># 这是假设函数,其中已经应用了Python的广播功能</span></span><br><span class="line">    loss = y_hat-y  <span class="comment"># 求出每一个y’和训练集中真实的y之间的差异 </span></span><br><span class="line">    cost = np.<span class="built_in">sum</span>(loss**<span class="number">2</span>)/(<span class="number">2</span>*<span class="built_in">len</span>(X)) <span class="comment"># 这是均方误差函数的代码实现</span></span><br><span class="line">    <span class="keyword">return</span> cost <span class="comment"># 返回当前模型的均方误差值</span></span><br></pre></td></tr></table></figure><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">print</span> (<span class="string">&quot;当权重5，偏置3时，损失为：&quot;</span>, </span><br><span class="line">loss_function(X_train, y_train, weight=<span class="number">5</span>, bias=<span class="number">3</span>))</span><br><span class="line"><span class="built_in">print</span> (<span class="string">&quot;当权重100，偏置1时，损失为：&quot;</span>, </span><br><span class="line">loss_function(X_train, y_train, weight=<span class="number">100</span>, bias=<span class="number">1</span>))</span><br></pre></td></tr></table></figure><figure class="highlight markdown"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">当权重5，偏置3时，损失为： 12.79639097078006</span><br><span class="line">当权重100，偏置1时，损失为： 1577.9592615030556</span><br></pre></td></tr></table></figure><p>&nbsp;</p><h2 id="通过梯度下降找最佳参数"><a href="#通过梯度下降找最佳参数" class="headerlink" title="通过梯度下降找最佳参数"></a>通过梯度下降找最佳参数</h2><script type="math/tex; mode=display">\text{梯度}=\frac{\partial}{\partial w}L(w)=\frac{\partial}{\partial w}\frac{1}{2N}\sum_{(x,y)\in D}(y-h(x))^2=\frac{1}{2N}\sum_{(x,y)\in D}(y-(w\bullet x))\bullet x</script><p>即对权重求偏导</p><p><img src="https://rozen-picture.oss-cn-beijing.aliyuncs.com/img/202502161638711.png" alt="image-20250216163842571"></p><h2 id="学习速率"><a href="#学习速率" class="headerlink" title="学习速率"></a>学习速率</h2><p>w随梯度下降的更新如下</p><script type="math/tex; mode=display">w=w-\alpha\cdot\frac{\partial}{\partial w}L(w)</script><p><img src="https://rozen-picture.oss-cn-beijing.aliyuncs.com/img/202502161651983.png" alt="image-20250216165114834"></p><p>在机器学习刚刚开始的时候，学习速率设置得大一些，当接近最佳权重时，减小学习速率。</p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">gradient_descent</span>(<span class="params">X, y, w, b, lr, <span class="built_in">iter</span></span>): <span class="comment"># 定义一个实现梯度下降的函数</span></span><br><span class="line">    l_history = np.zeros(<span class="built_in">iter</span>) <span class="comment"># 初始化记录梯度下降过程中损失的数组</span></span><br><span class="line">    w_history = np.zeros(<span class="built_in">iter</span>) <span class="comment"># 初始化记录梯度下降过程中权重的数组</span></span><br><span class="line">    b_history = np.zeros(<span class="built_in">iter</span>) <span class="comment"># 初始化记录梯度下降过程中偏置的数组</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">iter</span>): <span class="comment"># 进行梯度下降的迭代，就是下多少级台阶</span></span><br><span class="line">        y_hat  = w*X + b <span class="comment"># 这个是向量化运行实现的假设函数</span></span><br><span class="line">        loss = y_hat-y <span class="comment"># 这是中间过程,求得的是假设函数预测的y和真正的y值间的差值</span></span><br><span class="line">        derivative_w = X.T.dot(loss)/<span class="built_in">len</span>(X) <span class="comment"># 对权重求导, len(X)是样本总数</span></span><br><span class="line">        derivative_b = <span class="built_in">sum</span>(loss)*<span class="number">1</span>/<span class="built_in">len</span>(X) <span class="comment"># 对偏置求导</span></span><br><span class="line">        w = w - lr*derivative_w <span class="comment"># 结合下降速率alpha更新权重</span></span><br><span class="line">        b = b - lr*derivative_b <span class="comment"># 结合下降速率alpha更新偏置</span></span><br><span class="line">        l_history[i] = loss_function(X, y, w,b) <span class="comment"># 梯度下降过程中损失的历史</span></span><br><span class="line">        w_history[i] = w <span class="comment"># 梯度下降过程中权重的历史</span></span><br><span class="line">        b_history[i] = b <span class="comment"># 梯度下降过程中偏置的历史</span></span><br><span class="line">    <span class="keyword">return</span> l_history, w_history, b_history <span class="comment"># 返回梯度下降过程数据</span></span><br></pre></td></tr></table></figure><h2 id="实现一元线性回归模型并调试参数"><a href="#实现一元线性回归模型并调试参数" class="headerlink" title="实现一元线性回归模型并调试参数"></a>实现一元线性回归模型并调试参数</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 首先确定参数的初始值</span></span><br><span class="line">iterations = <span class="number">100</span>; <span class="comment"># 迭代100次</span></span><br><span class="line">alpha = <span class="number">0.5</span>; <span class="comment"># 此处初始学习速率设为0.5， 如果调整为1，你会看到不同的结果</span></span><br><span class="line">weight = -<span class="number">5</span> <span class="comment"># 权重</span></span><br><span class="line">bias = <span class="number">3</span> <span class="comment"># 偏置</span></span><br><span class="line"><span class="comment"># 计算一下初始权重和偏置值所带来的损失</span></span><br><span class="line"><span class="built_in">print</span> (<span class="string">&#x27;当前损失：&#x27;</span>,loss_function(X_train, y_train, weight, bias))</span><br></pre></td></tr></table></figure><figure class="highlight markdown"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">当前损失： 1.343795534906634</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 绘制当前的函数模型</span></span><br><span class="line">plt.plot(X_train, y_train,<span class="string">&#x27;r.&#x27;</span>, label=<span class="string">&#x27;Training data&#x27;</span>) <span class="comment"># 显示训练集散点图</span></span><br><span class="line">line_X = np.linspace(X_train.<span class="built_in">min</span>(), X_train.<span class="built_in">max</span>(), <span class="number">500</span>) <span class="comment"># X值域</span></span><br><span class="line">line_y = [weight*xx + bias <span class="keyword">for</span> xx <span class="keyword">in</span> line_X] <span class="comment"># 假设函数y_hat</span></span><br><span class="line">plt.plot(line_X,line_y,<span class="string">&#x27;b--&#x27;</span>, label=<span class="string">&#x27;Current hypothesis&#x27;</span> ) <span class="comment">#显示当前拟合</span></span><br><span class="line">plt.xlabel(<span class="string">&#x27;Wechat Ads&#x27;</span>) <span class="comment"># X轴Label</span></span><br><span class="line">plt.ylabel(<span class="string">&#x27;Sales&#x27;</span>) <span class="comment"># y轴Label</span></span><br><span class="line">plt.legend() <span class="comment"># 显示图例</span></span><br><span class="line">plt.show() <span class="comment"># 显示绘图</span></span><br></pre></td></tr></table></figure><p><img src="https://rozen-picture.oss-cn-beijing.aliyuncs.com/img/202502161655028.png" alt="image-20250216165535900"></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 根据初始参数值，进行梯度下降，也就是开始训练机器，拟合函数</span></span><br><span class="line">loss_history, weight_history, bias_history = gradient_descent(</span><br><span class="line">             X_train, y_train, weight, bias, alpha, iterations)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">plt.plot(loss_history,<span class="string">&#x27;g--&#x27;</span>,label=<span class="string">&#x27;Loss Curve&#x27;</span>)</span><br><span class="line">plt.xlabel(<span class="string">&#x27;Iterations&#x27;</span>) <span class="comment"># x轴Label</span></span><br><span class="line">plt.ylabel(<span class="string">&#x27;Loss&#x27;</span>) <span class="comment"># y轴Label</span></span><br><span class="line">plt.legend() <span class="comment"># 显示图例</span></span><br><span class="line">plt.show() <span class="comment"># 显示损失曲线</span></span><br></pre></td></tr></table></figure><p><img src="https://rozen-picture.oss-cn-beijing.aliyuncs.com/img/202502161708590.png" alt="image-20250216170850465"></p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 绘制当前的函数模型</span></span><br><span class="line">plt.plot(X_train, y_train,<span class="string">&#x27;r.&#x27;</span>, label=<span class="string">&#x27;Training data&#x27;</span>) <span class="comment"># 显示训练集散点图</span></span><br><span class="line">line_X = np.linspace(X_train.<span class="built_in">min</span>(), X_train.<span class="built_in">max</span>(), <span class="number">500</span>) <span class="comment"># X值域</span></span><br><span class="line"><span class="comment"># 关于weight_history[-1],这里的索引[-1]，就代表迭代500次后的最后一个W值</span></span><br><span class="line">line_y = [weight_history[-<span class="number">1</span>]*xx + bias_history[-<span class="number">1</span>] <span class="keyword">for</span> xx <span class="keyword">in</span> line_X] <span class="comment"># 假设函数</span></span><br><span class="line">plt.plot(line_X,line_y,<span class="string">&#x27;b--&#x27;</span>, label=<span class="string">&#x27;Current hypothesis&#x27;</span> ) <span class="comment"># 显示当前函数</span></span><br><span class="line">plt.xlabel(<span class="string">&#x27;Wechat Ads&#x27;</span>) <span class="comment"># x轴Label</span></span><br><span class="line">plt.ylabel(<span class="string">&#x27;Sales&#x27;</span>) <span class="comment"># y轴Label</span></span><br><span class="line">plt.legend() <span class="comment"># 显示图例</span></span><br><span class="line">plt.show() <span class="comment"># 显示函数图像</span></span><br></pre></td></tr></table></figure><p><img src="https://rozen-picture.oss-cn-beijing.aliyuncs.com/img/202502161709943.png" alt="image-20250216170912820"></p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">print</span> (<span class="string">&#x27;当前损失：&#x27;</span>,loss_function(X_train, y_train, </span><br><span class="line">                  weight_history[-<span class="number">1</span>], bias_history[-<span class="number">1</span>]))</span><br><span class="line"><span class="built_in">print</span> (<span class="string">&#x27;当前权重：&#x27;</span>,weight_history[-<span class="number">1</span>])</span><br><span class="line"><span class="built_in">print</span> (<span class="string">&#x27;当前偏置：&#x27;</span>,bias_history[-<span class="number">1</span>])</span><br></pre></td></tr></table></figure><figure class="highlight markdown"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">当前损失： 0.006157577974318896</span><br><span class="line">当前权重： 0.4721302015868674</span><br><span class="line">当前偏置： 0.2707186935933173</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">print</span> (<span class="string">&#x27;测试集损失：&#x27;</span>,loss_function(X_test, y_test, </span><br><span class="line">                    weight_history[-<span class="number">1</span>], bias_history[-<span class="number">1</span>]))</span><br></pre></td></tr></table></figure><figure class="highlight markdown"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">测试集损失： 0.007776406285658269</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 同时绘制训练集和测试集损失曲线</span></span><br><span class="line">loss_test ,a , b = gradient_descent(X_test, y_test, weight, bias, alpha, iterations)</span><br><span class="line">plt.plot(loss_history,<span class="string">&#x27;g--&#x27;</span>,label=<span class="string">&#x27;Traning Loss Curve&#x27;</span>)</span><br><span class="line">plt.plot(loss_test,<span class="string">&#x27;r&#x27;</span>,label=<span class="string">&#x27;Test Loss Curve&#x27;</span>)</span><br><span class="line">plt.xlabel(<span class="string">&#x27;Iterations&#x27;</span>) <span class="comment"># x轴Label</span></span><br><span class="line">plt.ylabel(<span class="string">&#x27;Loss&#x27;</span>) <span class="comment"># y轴Label</span></span><br><span class="line">plt.legend() <span class="comment"># 显示图例</span></span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p><img src="https://rozen-picture.oss-cn-beijing.aliyuncs.com/img/202502161729408.png" alt="image-20250216172954272"></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 设计Contour Plot动画</span></span><br><span class="line"><span class="keyword">import</span> matplotlib.animation <span class="keyword">as</span> animation</span><br><span class="line"></span><br><span class="line"><span class="comment"># 初始化参数网格</span></span><br><span class="line">theta0_vals = np.linspace(-<span class="number">2</span>, <span class="number">3</span>, <span class="number">100</span>)  <span class="comment"># bias 范围</span></span><br><span class="line">theta1_vals = np.linspace(-<span class="number">3</span>, <span class="number">3</span>, <span class="number">100</span>)  <span class="comment"># weight 范围</span></span><br><span class="line">J_vals = np.zeros((theta0_vals.size, theta1_vals.size))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 计算损失函数网格</span></span><br><span class="line"><span class="keyword">for</span> t1, bias <span class="keyword">in</span> <span class="built_in">enumerate</span>(theta0_vals):</span><br><span class="line">    <span class="keyword">for</span> t2, weight <span class="keyword">in</span> <span class="built_in">enumerate</span>(theta1_vals):</span><br><span class="line">        J_vals[t1, t2] = loss_function(X_train, y_train, weight, bias)</span><br><span class="line"></span><br><span class="line">J_vals = J_vals.T</span><br><span class="line">A, B = np.meshgrid(theta0_vals, theta1_vals)</span><br><span class="line">C = J_vals</span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建画布</span></span><br><span class="line">fig = plt.figure(figsize=(<span class="number">12</span>,<span class="number">5</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 左图：数据拟合过程</span></span><br><span class="line">ax1 = plt.subplot(<span class="number">121</span>)</span><br><span class="line">ax1.plot(X_train, y_train, <span class="string">&#x27;ro&#x27;</span>, label=<span class="string">&#x27;Training data&#x27;</span>)</span><br><span class="line">ax1.set_title(<span class="string">&#x27;Sales Prediction&#x27;</span>)</span><br><span class="line">ax1.set_xlim(X_train.<span class="built_in">min</span>()-X_train.std(), X_train.<span class="built_in">max</span>()+X_train.std())</span><br><span class="line">ax1.set_ylim(y_train.<span class="built_in">min</span>()-y_train.std(), y_train.<span class="built_in">max</span>()+y_train.std())</span><br><span class="line">ax1.grid(axis=<span class="string">&#x27;both&#x27;</span>)</span><br><span class="line">ax1.set_xlabel(<span class="string">&quot;WeChat Ads Volumn (X1)&quot;</span>)</span><br><span class="line">ax1.set_ylabel(<span class="string">&quot;Sales Volumn (Y)&quot;</span>)</span><br><span class="line">ax1.legend(loc=<span class="string">&#x27;lower right&#x27;</span>)</span><br><span class="line"></span><br><span class="line">line, = ax1.plot([], [], <span class="string">&#x27;b-&#x27;</span>, label=<span class="string">&#x27;Current Hypothesis&#x27;</span>)</span><br><span class="line">annotation = ax1.text(-<span class="number">2</span>, <span class="number">3</span>, <span class="string">&#x27;&#x27;</span>, fontsize=<span class="number">12</span>, color=<span class="string">&#x27;green&#x27;</span>)</span><br><span class="line">annotation.set_animated(<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 右图：损失函数等高线</span></span><br><span class="line">ax2 = plt.subplot(<span class="number">122</span>)</span><br><span class="line">cp = ax2.contourf(A, B, C, levels=<span class="number">20</span>, cmap=<span class="string">&#x27;viridis&#x27;</span>)</span><br><span class="line">plt.colorbar(cp, ax=ax2)</span><br><span class="line">ax2.set_title(<span class="string">&#x27;Loss Function Contour&#x27;</span>)</span><br><span class="line">ax2.set_xlabel(<span class="string">&#x27;Bias (θ₀)&#x27;</span>)</span><br><span class="line">ax2.set_ylabel(<span class="string">&#x27;Weight (θ₁)&#x27;</span>)</span><br><span class="line">track, = ax2.plot([], [], <span class="string">&#x27;r-&#x27;</span>, lw=<span class="number">1</span>, alpha=<span class="number">0.5</span>)</span><br><span class="line">point, = ax2.plot([], [], <span class="string">&#x27;ro&#x27;</span>, markersize=<span class="number">5</span>)</span><br><span class="line"></span><br><span class="line">plt.tight_layout()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 动画初始化函数</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">init</span>():</span><br><span class="line">    line.set_data([], [])</span><br><span class="line">    track.set_data([], [])</span><br><span class="line">    point.set_data([], [])</span><br><span class="line">    annotation.set_text(<span class="string">&#x27;&#x27;</span>)</span><br><span class="line">    <span class="keyword">return</span> line, track, point, annotation</span><br><span class="line"></span><br><span class="line"><span class="comment"># 动画更新函数</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">animate</span>(<span class="params">i</span>):</span><br><span class="line">    <span class="comment"># 更新拟合直线</span></span><br><span class="line">    fit1_X = np.linspace(X_train.<span class="built_in">min</span>()-X_train.std(), X_train.<span class="built_in">max</span>()+X_train.std(), <span class="number">1000</span>)</span><br><span class="line">    fit1_y = bias_history[i] + weight_history[i] * fit1_X</span><br><span class="line">    line.set_data(fit1_X, fit1_y)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 更新参数轨迹</span></span><br><span class="line">    track.set_data(bias_history[:i], weight_history[:i])</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 更新当前参数点</span></span><br><span class="line">    point.set_data([bias_history[i]], [weight_history[i]])</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 更新损失值显示</span></span><br><span class="line">    annotation.set_text(<span class="string">f&#x27;Iter: <span class="subst">&#123;i&#125;</span>\nCost = <span class="subst">&#123;loss_history[i]:<span class="number">.4</span>f&#125;</span>&#x27;</span>)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> line, track, point, annotation</span><br><span class="line"></span><br><span class="line"><span class="comment"># 生成动画</span></span><br><span class="line">anim = animation.FuncAnimation(</span><br><span class="line">    fig, animate, init_func=init,</span><br><span class="line">    frames=<span class="built_in">len</span>(weight_history),  <span class="comment"># 确保与训练步数一致</span></span><br><span class="line">    interval=<span class="number">50</span>, blit=<span class="literal">True</span></span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 保存动画</span></span><br><span class="line">anim.save(<span class="string">&#x27;linear_regression_training.gif&#x27;</span>, writer=<span class="string">&#x27;pillow&#x27;</span>, fps=<span class="number">15</span>)</span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure><p><img src="https://rozen-picture.oss-cn-beijing.aliyuncs.com/img/202502161754364.png" alt="image-20250216175424234"></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 显示Contour Plot动画</span></span><br><span class="line"><span class="keyword">import</span> io</span><br><span class="line"><span class="keyword">import</span> base64</span><br><span class="line"><span class="keyword">from</span> IPython.display <span class="keyword">import</span> HTML</span><br><span class="line"></span><br><span class="line">filename = <span class="string">&#x27;linear_regression_training.gif&#x27;</span></span><br><span class="line"></span><br><span class="line">video = io.<span class="built_in">open</span>(filename, <span class="string">&#x27;r+b&#x27;</span>).read()</span><br><span class="line">encoded = base64.b64encode(video)</span><br><span class="line">HTML(data=<span class="string">&#x27;&#x27;&#x27;&lt;img src=&quot;data:image/gif;base64,&#123;0&#125;&quot; type=&quot;gif&quot; /&gt;&#x27;&#x27;&#x27;</span>.<span class="built_in">format</span>(encoded.decode(<span class="string">&#x27;ascii&#x27;</span>)))</span><br></pre></td></tr></table></figure><p><img src="https://rozen-picture.oss-cn-beijing.aliyuncs.com/img/202502161815543.gif" alt></p>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;机器学习—线性回归&quot;&gt;&lt;a href=&quot;#机器学习—线性回归&quot; class=&quot;headerlink&quot; title=&quot;机器学习—线性回归&quot;&gt;&lt;/a&gt;机器学习—线性回归&lt;/h1&gt;&lt;p&gt;所谓回归分析（regression analysis），是确定两种或两种以上变量间相</summary>
      
    
    
    
    
  </entry>
  
  <entry>
    <title></title>
    <link href="https://rozen12123.github.io/posts/0.html"/>
    <id>https://rozen12123.github.io/posts/0.html</id>
    <published>2024-12-10T03:16:05.310Z</published>
    <updated>2024-12-10T03:54:32.713Z</updated>
    
    <content type="html"><![CDATA[<h1 id="机器学习—张量"><a href="#机器学习—张量" class="headerlink" title="机器学习—张量"></a>机器学习—张量</h1><h2 id="机器学习的数据结构—张量"><a href="#机器学习的数据结构—张量" class="headerlink" title="机器学习的数据结构—张量"></a>机器学习的数据结构—张量</h2><p>张量是机器学习程序中的数字容器，本质上就是各种不同维度的数组，如下图所示。</p><p>张量的维度称为轴（axis），轴的个数称为阶（rank）</p><p><img src="https://rozen-picture.oss-cn-beijing.aliyuncs.com/img/202412101117917.png" alt="image-20241210111724714"></p><h3 id="标量—0D张量"><a href="#标量—0D张量" class="headerlink" title="标量—0D张量"></a>标量—0D张量</h3><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np <span class="comment">#导入NumPy</span></span><br><span class="line">X = np.array(<span class="number">5</span>) <span class="comment"># 创建0D张量，也就是标量</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;X的值&quot;</span>,X)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;X的阶&quot;</span>,X.ndim) <span class="comment">#ndim属性显示张量轴的个数</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;X的数据类型&quot;</span>,X.dtype) <span class="comment"># dtype属性显示张量数据类型</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;X的形状&quot;</span>,X.shape) <span class="comment"># shape属性显示张量形状</span></span><br></pre></td></tr></table></figure><p><img src="https://rozen-picture.oss-cn-beijing.aliyuncs.com/img/202412101121690.png" alt="image-20241210112116640"></p><h3 id="向量—1D张量"><a href="#向量—1D张量" class="headerlink" title="向量—1D张量"></a>向量—1D张量</h3><p>由一组数字组成的数组叫作向量（vector），也就是一阶张量</p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">X = np.array([<span class="number">5</span>,<span class="number">6</span>,<span class="number">7</span>,<span class="number">8</span>,<span class="number">9</span>]) <span class="comment">#创建1D张量，也就是向量</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;X的值&quot;</span>,X)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;X的阶&quot;</span>,X.ndim) <span class="comment">#ndim属性显示张量轴的个数</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;X的形状&quot;</span>,X.shape) <span class="comment"># shape属性显示张量形状</span></span><br></pre></td></tr></table></figure><p><img src="https://rozen-picture.oss-cn-beijing.aliyuncs.com/img/202412101123728.png" alt="image-20241210112310680"></p><p>（5，）表示一个1D张量，元素数量是5，也就是5维向量。</p><h3 id="矩阵—2D张量"><a href="#矩阵—2D张量" class="headerlink" title="矩阵—2D张量"></a>矩阵—2D张量</h3><p>矩阵是2D张量，形状为 （==样本，特征==）。第一个轴是样本轴，第二个轴是特征轴。</p><p><img src="https://rozen-picture.oss-cn-beijing.aliyuncs.com/img/202412101125432.png" alt="image-20241210112508372"></p><h3 id="序列数据—3D张量"><a href="#序列数据—3D张量" class="headerlink" title="序列数据—3D张量"></a>序列数据—3D张量</h3><p>时序数据集的形状为3D张量：（样本，时戳，标签）</p><p><img src="https://rozen-picture.oss-cn-beijing.aliyuncs.com/img/202412101144350.png" alt="image-20241210114413291"></p><h3 id="图像数据—4D张量"><a href="#图像数据—4D张量" class="headerlink" title="图像数据—4D张量"></a>图像数据—4D张量</h3><p>图像数据集其形状为（样本，图像高，图像宽度，颜色深度），如MNIST特征数据集的形状为 （60000，28，28，1）。</p><p><img src="https://rozen-picture.oss-cn-beijing.aliyuncs.com/img/202412101146418.png" alt="image-20241210114633354"></p><p>比如指定批量大小为64。此时每批的100px×100px的彩色图像张量形状为（64， 100，100，3），如果是灰度图像，则为（64，100，100，1）</p><h3 id="视频数据—5D张量"><a href="#视频数据—5D张量" class="headerlink" title="视频数据—5D张量"></a>视频数据—5D张量</h3><p>其形状为（样本，帧，高度，宽度，颜色深度）</p><h2 id="张量的创建和访问"><a href="#张量的创建和访问" class="headerlink" title="张量的创建和访问"></a>张量的创建和访问</h2><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">array_04=np.arange(<span class="number">1</span>,<span class="number">5</span>,<span class="number">1</span>) <span class="comment"># 通过arange函数生成数组</span></span><br><span class="line">array_05=np.linspace(<span class="number">1</span>,<span class="number">5</span>,<span class="number">5</span>) <span class="comment"># 通过linspace函数生成数组</span></span><br><span class="line"><span class="built_in">print</span> (array_04)</span><br><span class="line"><span class="built_in">print</span> (array_05)</span><br></pre></td></tr></table></figure><p>arange（a，b，c）函数产生a～b（不包括b），间隔为c；</p><p>linspace（a，b， c）函数是把a～b（包括b），平均分成c份。</p><p>&nbsp;</p><p>索引（indexing）和切片（slicing）这两种方式访问张量</p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">array_06 = np.arange(<span class="number">10</span>)</span><br><span class="line"><span class="built_in">print</span> (array_06)</span><br><span class="line">index_01 = array_06[<span class="number">3</span>] <span class="comment"># 索引-第4个元素</span></span><br><span class="line"><span class="built_in">print</span> (<span class="string">&#x27;第4个元素&#x27;</span>, index_01)</span><br><span class="line">index_02 = array_06[-<span class="number">1</span>] <span class="comment"># 索引-最后一个元素</span></span><br><span class="line"><span class="built_in">print</span> (<span class="string">&#x27;第-1个元素&#x27;</span>, index_02)</span><br><span class="line">slice_01 = array_06[:<span class="number">4</span>] <span class="comment"># 从0到4切片</span></span><br><span class="line"><span class="built_in">print</span> (<span class="string">&#x27;从0到4切片&#x27;</span>, slice_01)</span><br><span class="line">slice_02 = array_06[<span class="number">0</span>:<span class="number">12</span>:<span class="number">4</span>] <span class="comment"># 从0到12切片，步长为2</span></span><br><span class="line"><span class="built_in">print</span> (<span class="string">&#x27;从0到12切片，步长为4&#x27;</span>, slice_02)</span><br></pre></td></tr></table></figure><p><img src="https://rozen-picture.oss-cn-beijing.aliyuncs.com/img/202412101154128.png" alt="image-20241210115431061"></p>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;机器学习—张量&quot;&gt;&lt;a href=&quot;#机器学习—张量&quot; class=&quot;headerlink&quot; title=&quot;机器学习—张量&quot;&gt;&lt;/a&gt;机器学习—张量&lt;/h1&gt;&lt;h2 id=&quot;机器学习的数据结构—张量&quot;&gt;&lt;a href=&quot;#机器学习的数据结构—张量&quot; class=&quot;</summary>
      
    
    
    
    
  </entry>
  
  <entry>
    <title></title>
    <link href="https://rozen12123.github.io/posts/0.html"/>
    <id>https://rozen12123.github.io/posts/0.html</id>
    <published>2024-12-08T14:48:02.452Z</published>
    <updated>2024-12-09T10:02:47.467Z</updated>
    
    <content type="html"><![CDATA[<h1 id="机器学习—Kaggle的使用"><a href="#机器学习—Kaggle的使用" class="headerlink" title="机器学习—Kaggle的使用"></a>机器学习—Kaggle的使用</h1><p>打开<a href="https://www.kaggle.com/">Kaggle: Your Machine Learning and Data Science Community</a>并点击<code>Sign In</code>登录账号</p><p><img src="https://rozen-picture.oss-cn-beijing.aliyuncs.com/img/202412091625587.png" alt="image-20241209162509517"></p><p>kaggle中自带了很多的数据集</p><p><img src="https://rozen-picture.oss-cn-beijing.aliyuncs.com/img/202412091629831.png" alt="image-20241209162947772"></p><p>在点击<code>Datasets</code>之后，单点<code>Notebook</code>，如果有适用的数据集可以单击<code>Copy and Edit</code>复制其<code>Notebook</code>，之后我们自己进行慢慢研习。</p><p><img src="https://rozen-picture.oss-cn-beijing.aliyuncs.com/img/202412091627358.png" alt="image-20241209162749300"></p><p>点击<code>File</code>，<code>Upload input</code>，<code>Upload dataset</code>后即可把我们现有的文档进行上传。</p><p><img src="https://rozen-picture.oss-cn-beijing.aliyuncs.com/img/202412091716295.png" alt="image-20241209171618199"></p><p>来举一个手写数字识别的栗子：</p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np <span class="comment"># 导入NumPy数学工具箱</span></span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd <span class="comment"># 导入Pandas数据处理工具箱</span></span><br><span class="line"><span class="keyword">from</span> keras.datasets <span class="keyword">import</span> mnist <span class="comment">#从Keras中导入mnist数据集</span></span><br><span class="line"><span class="comment">#读入训练集和测试集</span></span><br><span class="line">(X_train_image, y_train_lable), (X_test_image, y_test_lable) =  mnist.load_data() </span><br></pre></td></tr></table></figure><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">print</span> (<span class="string">&quot;特征集张量形状：&quot;</span>, X_train_image.shape) <span class="comment">#用shape方法显示张量的形状</span></span><br><span class="line"><span class="built_in">print</span> (<span class="string">&quot;第一个数据样本：\n&quot;</span>, X_train_image[<span class="number">0</span>]) <span class="comment">#注意Python的索引是从0开始的</span></span><br></pre></td></tr></table></figure><p>结果如下<img src="https://rozen-picture.oss-cn-beijing.aliyuncs.com/img/202412091746769.png" alt="image-20241209174647681"></p><p>shape方法显示X_train_image张量的形状。灰度图像数据集是3D张量，第一个维度是样本维（也就是一张一张的图片，共60 000张），后面两个是特征维（也就是图片的28px×28px的矩阵）</p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">print</span> (<span class="string">&quot;第一个数据样本的标签：&quot;</span>, y_train_lable[<span class="number">0</span>])</span><br></pre></td></tr></table></figure><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> tensorflow.keras.utils <span class="keyword">import</span> to_categorical <span class="comment"># 导入keras.utils工具箱的类别转换工具</span></span><br><span class="line">X_train = X_train_image.reshape(<span class="number">60000</span>,<span class="number">28</span>,<span class="number">28</span>,<span class="number">1</span>) <span class="comment"># 给标签增加一个维度</span></span><br><span class="line">X_test = X_test_image.reshape(<span class="number">10000</span>,<span class="number">28</span>,<span class="number">28</span>,<span class="number">1</span>) <span class="comment"># 给标签增加一个维度</span></span><br><span class="line">y_train = to_categorical(y_train_lable, <span class="number">10</span>) <span class="comment"># 特征转换为one-hot编码</span></span><br><span class="line">y_test = to_categorical(y_test_lable, <span class="number">10</span>) <span class="comment"># 特征转换为one-hot编码</span></span><br><span class="line"><span class="built_in">print</span> (<span class="string">&quot;数据集张量形状：&quot;</span>, X_train.shape) <span class="comment"># 特征集张量的形状</span></span><br><span class="line"><span class="built_in">print</span> (<span class="string">&quot;第一个数据标签：&quot;</span>,y_train[<span class="number">0</span>]) <span class="comment"># 显示标签集的第一个数据</span></span><br></pre></td></tr></table></figure><p><img src="https://rozen-picture.oss-cn-beijing.aliyuncs.com/img/202412091750508.png" alt="image-20241209175001439"></p><p>（1）Keras要求图像数据集导入卷积网络模型时为4阶张量，最后一阶代表颜色深度，灰度图像只有一个颜色通道，可以设置其值为1。</p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> keras <span class="keyword">import</span> models <span class="comment"># 导入Keras模型, 和各种神经网络的层</span></span><br><span class="line"><span class="keyword">from</span> keras.layers <span class="keyword">import</span> Dense, Dropout, Flatten, Conv2D, MaxPooling2D</span><br><span class="line">model = models.Sequential() <span class="comment"># 用序贯方式建立模型</span></span><br><span class="line">model.add(Conv2D(<span class="number">32</span>, (<span class="number">3</span>, <span class="number">3</span>), activation=<span class="string">&#x27;relu&#x27;</span>, <span class="comment"># 添加Conv2D层</span></span><br><span class="line">                 input_shape=(<span class="number">28</span>,<span class="number">28</span>,<span class="number">1</span>))) <span class="comment"># 指定输入数据样本张量的类型</span></span><br><span class="line">model.add(MaxPooling2D(pool_size=(<span class="number">2</span>, <span class="number">2</span>))) <span class="comment"># 添加MaxPooling2D层</span></span><br><span class="line">model.add(Conv2D(<span class="number">64</span>, (<span class="number">3</span>, <span class="number">3</span>), activation=<span class="string">&#x27;relu&#x27;</span>)) <span class="comment"># 添加Conv2D层</span></span><br><span class="line">model.add(MaxPooling2D(pool_size=(<span class="number">2</span>, <span class="number">2</span>))) <span class="comment"># 添加MaxPooling2D层</span></span><br><span class="line">model.add(Dropout(<span class="number">0.25</span>)) <span class="comment"># 添加Dropout层</span></span><br><span class="line">model.add(Flatten()) <span class="comment"># 展平</span></span><br><span class="line">model.add(Dense(<span class="number">128</span>, activation=<span class="string">&#x27;relu&#x27;</span>)) <span class="comment"># 添加全连接层</span></span><br><span class="line">model.add(Dropout(<span class="number">0.5</span>)) <span class="comment"># 添加Dropout层</span></span><br><span class="line">model.add(Dense(<span class="number">10</span>, activation=<span class="string">&#x27;softmax&#x27;</span>)) <span class="comment"># Softmax分类激活，输出10维分类码</span></span><br><span class="line"><span class="comment"># 编译模型</span></span><br><span class="line">model.<span class="built_in">compile</span>(optimizer=<span class="string">&#x27;rmsprop&#x27;</span>, <span class="comment"># 指定优化器</span></span><br><span class="line">              loss=<span class="string">&#x27;categorical_crossentropy&#x27;</span>, <span class="comment"># 指定损失函数</span></span><br><span class="line">              metrics=[<span class="string">&#x27;accuracy&#x27;</span>]) <span class="comment"># 指定验证过程中的评估指标</span></span><br></pre></td></tr></table></figure><p>这段代码把数据集放入<code>卷积神经网络</code>进行处理。这个网络中包括两个<code>Conv2D</code>（二维卷积）层，两个<code>MaxPooling2D</code>（最大池化）层，两个<code>Dropout</code>层用于防止过拟合，还有<code>Dense</code>（全连接）层，</p><p>最后通过<code>Softmax</code>分类器输出预测标签y’值，也就是所预测的分类值。这个y’值，是一个<code>one-hot</code>（即“一位有效编码”）格式的10维向量。我们可以将y’与标签真值y进行比较，以计算预测的准确率。</p><p><img src="https://rozen-picture.oss-cn-beijing.aliyuncs.com/img/202412091752647.png" alt="image-20241209175219511"></p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">model.fit(X_train, y_train, <span class="comment"># 指定训练特征集和训练标签集</span></span><br><span class="line">          validation_split = <span class="number">0.3</span>, <span class="comment"># 部分训练集数据拆分成验证集</span></span><br><span class="line">          epochs=<span class="number">5</span>, <span class="comment"># 训练轮次为5轮</span></span><br><span class="line">          batch_size=<span class="number">128</span>) <span class="comment"># 以128为批量进行训练</span></span><br></pre></td></tr></table></figure><p><img src="https://rozen-picture.oss-cn-beijing.aliyuncs.com/img/202412091758140.png" alt="image-20241209175816069"></p><p><code>accuracy</code>：代表训练集上的预测准确率。</p><p><code>val_accuracy</code>：代表验证集上的预测准确率。</p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">score = model.evaluate(X_test, y_test) <span class="comment"># 在测试集上进行模型评估</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;测试集预测准确率:&#x27;</span>, score[<span class="number">1</span>]) <span class="comment"># 打印测试集上的预测准确率</span></span><br></pre></td></tr></table></figure><p>K折验证:机器学习中有重用同一个数据集进行多次验证的方法</p><p><img src="https://rozen-picture.oss-cn-beijing.aliyuncs.com/img/202412091801429.png" alt="image-20241209180153328"></p><p>K折验证（K-fold validation）的思路是将数据划分为大小相同的K个分区，对于每个分区，都在剩余的K-1个分区上训练模型，然后在留</p><p>下的分区上评估模型。</p><p>最终分数等于K个分数的平均值。对于数据集的规模比较小或者模型性能很不稳定的情况，这是一种很有用的方法。</p><p>==注意K折验证仍需要预留独立的测试集再次进行模型的校正==</p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">pred = model.predict(X_test[<span class="number">0</span>].reshape(<span class="number">1</span>, <span class="number">28</span>, <span class="number">28</span>, <span class="number">1</span>)) <span class="comment"># 预测测试集第一个数据</span></span><br><span class="line"><span class="built_in">print</span>(pred[<span class="number">0</span>],<span class="string">&quot;转换一下格式得到：&quot;</span>,pred.argmax()) <span class="comment"># 把one-hot码转换为数字</span></span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt <span class="comment"># 导入绘图工具包</span></span><br><span class="line">plt.imshow(X_test[<span class="number">0</span>].reshape(<span class="number">28</span>, <span class="number">28</span>),cmap=<span class="string">&#x27;Greys&#x27;</span>) <span class="comment"># 输出这个图片</span></span><br></pre></td></tr></table></figure>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;机器学习—Kaggle的使用&quot;&gt;&lt;a href=&quot;#机器学习—Kaggle的使用&quot; class=&quot;headerlink&quot; title=&quot;机器学习—Kaggle的使用&quot;&gt;&lt;/a&gt;机器学习—Kaggle的使用&lt;/h1&gt;&lt;p&gt;打开&lt;a href=&quot;https://www</summary>
      
    
    
    
    
  </entry>
  
  <entry>
    <title></title>
    <link href="https://rozen12123.github.io/posts/0.html"/>
    <id>https://rozen12123.github.io/posts/0.html</id>
    <published>2024-12-08T12:55:28.970Z</published>
    <updated>2024-12-08T13:40:21.635Z</updated>
    
    <content type="html"><![CDATA[<h1 id="colab使用说明"><a href="#colab使用说明" class="headerlink" title="colab使用说明"></a>colab使用说明</h1><h2 id="1-上传文件"><a href="#1-上传文件" class="headerlink" title="1.上传文件"></a>1.上传文件</h2><p>点击+New按钮可以添加本地的文件和程序（在colab中要读取的数据需要实现上床，这点不如Kaggle有很多可以直接用的数据）</p><p><img src="https://cdn.jsdelivr.net/gh/Rozen12123/picture_1/Picgo/202412082054270.png" alt="image-20241208205426233" style="zoom:67%;"></p><p><img src="https://cdn.jsdelivr.net/gh/Rozen12123/picture_1/Picgo/202412082110066.png" alt="image-20241208211028029" style="zoom:80%;"></p><h2 id="2-选用高性能计算单元"><a href="#2-选用高性能计算单元" class="headerlink" title="2.选用高性能计算单元"></a>2.选用高性能计算单元</h2><p><img src="https://cdn.jsdelivr.net/gh/Rozen12123/picture_1/Picgo/202412082124135.png" alt="image-20241208212414077"></p><p><img src="https://cdn.jsdelivr.net/gh/Rozen12123/picture_1/Picgo/202412082125578.png" alt="image-20241208212513538"></p><p>点击右上角可以显示高性能的GPU</p><p><img src="https://cdn.jsdelivr.net/gh/Rozen12123/picture_1/Picgo/202412082126274.png" alt="image-20241208212627228"></p><p>当然，更高性能的计算单元也意味着我们购买的计算单元消耗的速度越快</p><h2 id="3-举一个栗子"><a href="#3-举一个栗子" class="headerlink" title="3.举一个栗子"></a>3.举一个栗子</h2><p>打开<code>ipynb</code>文件后即可运行代码（以下直接读取github中的开源数据）</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd <span class="comment">#导入Pandas，用于数据读取和处理</span></span><br><span class="line"><span class="comment"># 读入房价数据，示例代码中的文件地址为internet链接，读者也可以下载该文件到本机进行读取</span></span><br><span class="line"><span class="comment"># 如，当数据集和代码文件位于相同本地目录，路径名应为&quot;./house.csv&quot;，或直接放&quot;house.csv&quot;亦可</span></span><br><span class="line">df_housing = pd.read_csv(<span class="string">&quot;https://raw.githubusercontent.com/huangjia2019/house/master/house.csv&quot;</span>)</span><br><span class="line">df_housing.head <span class="comment">#显示加州房价数据</span></span><br></pre></td></tr></table></figure><p>把一个网上共享的数据集（csv文件）读入<code>DataFrame</code>数据结构<code>df_housing</code>中</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">X = df_housing.drop(<span class="string">&quot;median_house_value&quot;</span>,axis = <span class="number">1</span>) <span class="comment">#构建特征集X</span></span><br><span class="line">y = df_housing.median_house_value <span class="comment">#构建标签集y</span></span><br></pre></td></tr></table></figure><p><code>drop</code>:把最后一列<code>median_house_value</code>字段去掉，其他所有的字段保存为特征集<code>X</code></p><p>把整个<code>median_house_value</code>字段单独赋值给标签值<code>Y</code></p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split <span class="comment">#导入数据集拆分工具</span></span><br><span class="line">X_train, X_test, y_train, y_test = train_test_split(X, y,</span><br><span class="line">         test_size=<span class="number">0.2</span>, random_state=<span class="number">0</span>) <span class="comment">#以80%/20%的比例进行数据集的拆分</span></span><br></pre></td></tr></table></figure><p>把数据集一分为二，80%用于训练，20%的数据用于测试。</p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.linear_model <span class="keyword">import</span> LinearRegression <span class="comment">#导入线性回归算法模型</span></span><br><span class="line">model = LinearRegression() <span class="comment">#使用线性回归算法</span></span><br><span class="line">model.fit(X_train, y_train) <span class="comment">#用训练集数据，训练机器，拟合函数，确定参数</span></span><br></pre></td></tr></table></figure><p>选定模型，也就是算法，通过其中的<code>fit</code>方法来训练机器，进行函数拟合</p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">y_pred = model.predict(X_test) <span class="comment">#预测测试集的Y值</span></span><br><span class="line"><span class="built_in">print</span> (<span class="string">&#x27;房价的真值(测试集)&#x27;</span>,y_test)</span><br><span class="line"><span class="built_in">print</span> (<span class="string">&#x27;预测的房价(测试集)&#x27;</span>,y_pred)</span><br></pre></td></tr></table></figure><p>当成功运行完<code>fit</code>方法后，学习到的函数也已经保存在机器中了，可以用<code>model</code>的<code>predict</code>方法对测试集的房价进行预测。</p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">print</span>(<span class="string">&quot;给预测评分：&quot;</span>, model.score(X_test, y_test)) <span class="comment">#评估预测结果</span></span><br></pre></td></tr></table></figure><p>还可以显示一下预测的大致得分（Sklearn线性回归模型score徐行给出的是R2分数，即预测值的方差何总体方差之间的差异）</p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt <span class="comment">#导入matplotlib画图库</span></span><br><span class="line"><span class="comment">#用散点图显示家庭收入中位数和房价中位数的分布</span></span><br><span class="line">plt.scatter(X_test.median_income, y_test,  color=<span class="string">&#x27;brown&#x27;</span>)</span><br><span class="line"><span class="comment">#画出回归函数(从特征到预测标签)</span></span><br><span class="line">plt.plot(X_test.median_income, y_pred, color=<span class="string">&#x27;green&#x27;</span>, linewidth=<span class="number">1</span>)</span><br><span class="line">plt.xlabel(<span class="string">&#x27;Median Income&#x27;</span>) <span class="comment">#X轴-家庭收入中位数</span></span><br><span class="line">plt.ylabel(<span class="string">&#x27;Median House Value&#x27;</span>) <span class="comment">#Y轴-房价中位数</span></span><br><span class="line">plt.show() <span class="comment">#显示房价分布和机器习得的函数图形</span></span><br></pre></td></tr></table></figure><p>当然也可也用代码绘制出机器学习的函数，由于x的特征太多，我们将与房价关系最大的<code>median_income</code>作为代表特征来显示散点图</p>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;colab使用说明&quot;&gt;&lt;a href=&quot;#colab使用说明&quot; class=&quot;headerlink&quot; title=&quot;colab使用说明&quot;&gt;&lt;/a&gt;colab使用说明&lt;/h1&gt;&lt;h2 id=&quot;1-上传文件&quot;&gt;&lt;a href=&quot;#1-上传文件&quot; class=&quot;head</summary>
      
    
    
    
    
  </entry>
  
</feed>
